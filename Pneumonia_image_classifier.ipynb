{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia image classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pneumonia identification from X-Ray images**\n",
        "\n",
        "#### Teams: Carlos Relaño Rupérez, Javier Rubio Serrano and Javier López Rodríguez"
      ],
      "metadata": {
        "id": "vB7Ver5ikM8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**"
      ],
      "metadata": {
        "id": "4PGw38JH28zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pneumonia is an infection that inflames the air sacs in one or both lungs, and a major cause of morbidity and death in susceptible populations such as infants and elder people. Properly diagnosing this disease can make a difference in its treatment and outcome.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Taking into acount the importance of reliable diagnostics, we will develop an image classifier that is able to distinguish between X-ray images from healthy and pneumonia patients.\n",
        "\n",
        "Our aim is to perform **Transfer Learning** using a pre-trained Keras deep learning model, transfering the knowledge of that pre-trained model to our new model in order to adapt it to our problem.\n",
        "\n",
        "We are choosing this option because the Kaggle Xray dataset has too little data to train a full-scale neural network model and get good results with it.\n",
        "\n",
        "The pre-trained model we're using is **InceptionV3** (https://arxiv.org/abs/1512.00567, https://keras.io/api/applications/inceptionv3/). It is a Neural Network architecture that was an important milestone in the field of Computer Vision. InceptionV3 can be used to classify images, and that is why we're using it for our problem.\n",
        "\n",
        "There are other Neural Networks that were created after InceptionV3 that would probably give better results (InceptionV4, Inception-ResNet, and many other examples). However, we think that InceptionV3 is a good enough model for our task.\n",
        "\n",
        "The model will be created using the **Keras Functional API**, a high level API. In the preprocessing steps of the Dataset, **Keras Sequential API** and some basic **TensorFlow** transformations will be used.\n",
        "\n",
        "All of these steps will be performed using **Orca from Analytics Zoo**, so that, if we were on a cluster (and not on Google Colab), the different steps of the data preprocessing, model training, etc. could be distributed."
      ],
      "metadata": {
        "id": "SnnqSyB23CL3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMBntim9bMf"
      },
      "source": [
        "## **Environment Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_OS4HKJMNpv"
      },
      "source": [
        "**Install Analytics Zoo**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install jdk8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "# Set environment variable JAVA_HOME.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ],
      "metadata": {
        "id": "_Tdx3WP42Heu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9348d420-3e2c-4f12-fdaf-12f0ec78aea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "openjdk version \"1.8.0_312\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qfT8CaC51hI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20ec610-07ee-4156-d9bf-b7d198f3e2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting analytics-zoo[ray]\n",
            "  Downloading analytics_zoo-0.12.0b2022020801-py2.py3-none-manylinux1_x86_64.whl (194.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 194.7 MB 34 kB/s \n",
            "\u001b[?25hCollecting bigdl==0.13.1.dev0\n",
            "  Downloading BigDL-0.13.1.dev0-py2.py3-none-manylinux1_x86_64.whl (114.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 114.0 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from analytics-zoo[ray]) (3.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from analytics-zoo[ray]) (21.3)\n",
            "Collecting pyspark==2.4.6\n",
            "  Downloading pyspark-2.4.6.tar.gz (218.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 218.4 MB 49 kB/s \n",
            "\u001b[?25hCollecting conda-pack==0.3.1\n",
            "  Downloading conda_pack-0.3.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting aioredis==1.1.0\n",
            "  Downloading aioredis-1.1.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp==3.7.0\n",
            "  Downloading aiohttp-3.7.0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.5 MB/s \n",
            "\u001b[?25hCollecting ray==1.2.0\n",
            "  Downloading ray-1.2.0-cp37-cp37m-manylinux2014_x86_64.whl (47.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.5 MB 93 kB/s \n",
            "\u001b[?25hCollecting hiredis==1.1.0\n",
            "  Downloading hiredis-1.1.0-cp37-cp37m-manylinux2010_x86_64.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 920 kB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from analytics-zoo[ray]) (5.4.8)\n",
            "Collecting async-timeout==3.0.1\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.0->analytics-zoo[ray]) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.0->analytics-zoo[ray]) (21.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from bigdl==0.13.1.dev0->analytics-zoo[ray]) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bigdl==0.13.1.dev0->analytics-zoo[ray]) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from conda-pack==0.3.1->analytics-zoo[ray]) (57.4.0)\n",
            "Collecting py4j==0.10.7\n",
            "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (3.17.3)\n",
            "Collecting gpustat\n",
            "  Downloading gpustat-1.0.0b1.tar.gz (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 263 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (2.23.0)\n",
            "Collecting colorful\n",
            "  Downloading colorful-0.6.0a1-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 65.6 MB/s \n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.4.0.dev1-py2.py3-none-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (1.0.3)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.1.3-py3-none-any.whl (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (0.13.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (3.13)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (1.43.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray==1.2.0->analytics-zoo[ray]) (4.3.3)\n",
            "Collecting opencensus\n",
            "  Downloading opencensus-0.8.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 65.0 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.3\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray==1.2.0->analytics-zoo[ray]) (4.10.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray==1.2.0->analytics-zoo[ray]) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray==1.2.0->analytics-zoo[ray]) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray==1.2.0->analytics-zoo[ray]) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->analytics-zoo[ray]) (3.0.7)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.7.0->analytics-zoo[ray]) (2.10)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray==1.2.0->analytics-zoo[ray]) (7.352.0)\n",
            "Collecting blessed>=1.17.1\n",
            "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat->ray==1.2.0->analytics-zoo[ray]) (0.2.5)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==1.2.0->analytics-zoo[ray]) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==1.2.0->analytics-zoo[ray]) (0.18.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray==1.2.0->analytics-zoo[ray]) (1.26.3)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (1.54.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (1.35.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (2018.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0->analytics-zoo[ray]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.2.0->analytics-zoo[ray]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.2.0->analytics-zoo[ray]) (2021.10.8)\n",
            "Building wheels for collected packages: pyspark, gpustat\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.6-py2.py3-none-any.whl size=218814407 sha256=7f086500dfec0c936af37a2673419dd4cfad68af951b3c59b68de7efcf4cb681\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/42/b0/ba397759613f4feb1611021a2503e60e344e546671b2ae04f8\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.0.0b1-py3-none-any.whl size=15979 sha256=e74415d181727d3cc778ec698abf1b59df2b9e4371e339246f461d760f58427e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/16/e2/3e2437fba4c4b6a97a97bd96fce5d14e66cff5c4966fb1cc8c\n",
            "Successfully built pyspark gpustat\n",
            "Installing collected packages: multidict, yarl, py4j, async-timeout, pyspark, opencensus-context, hiredis, deprecated, blessed, aiohttp, redis, py-spy, opencensus, gpustat, conda-pack, colorful, colorama, bigdl, aioredis, aiohttp-cors, setproctitle, ray, analytics-zoo\n",
            "Successfully installed aiohttp-3.7.0 aiohttp-cors-0.7.0 aioredis-1.1.0 analytics-zoo-0.12.0b2022020801 async-timeout-3.0.1 bigdl-0.13.1.dev0 blessed-1.19.1 colorama-0.4.4 colorful-0.6.0a1 conda-pack-0.3.1 deprecated-1.2.13 gpustat-1.0.0b1 hiredis-1.1.0 multidict-6.0.2 opencensus-0.8.0 opencensus-context-0.1.2 py-spy-0.4.0.dev1 py4j-0.10.7 pyspark-2.4.6 ray-1.2.0 redis-4.1.3 setproctitle-1.2.2 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "# Install latest pre-release version of Analytics Zoo with RayOnSpark\n",
        "# Installing Analytics Zoo from pip will automatically install pyspark, bigdl, and their dependencies.\n",
        "!pip install --pre --upgrade analytics-zoo[ray]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "naOgyqI2qoDO",
        "outputId": "3d0d2fab-2e2a-494b-dc90-1049ec2c0d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kOBgiqFWqvPn",
        "outputId": "97242ff4-53f7-4554-bc78-e36ebeb32274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBdeoZzLWWlY"
      },
      "source": [
        "## **Orca Context** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Orca library from Analytics Zoo allows the scaling and distribution of a Python notebook across large clusters, in order to process distributed Big Data (https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/orca.html). \n",
        "\n",
        "To use it, we need to initialize the **Orca Context**. While inside of the Orca Context, Orca allows the distribution of data-parallel processing pipelines, model training, inference, and hyperparameter tuning.\n"
      ],
      "metadata": {
        "id": "XC0SWGmZ1B_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDPt2XTsT-Gv"
      },
      "outputs": [],
      "source": [
        "# import necesary libraries and modules\n",
        "from zoo.orca import init_orca_context, stop_orca_context\n",
        "from zoo.orca import OrcaContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're using Google Colab, we will use cluster_mode = \"local\" with 2 cores (the number of cores provided by Google)"
      ],
      "metadata": {
        "id": "bnV8tN-Z2NUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAQp0FcUWaH3",
        "outputId": "9c8dbe69-25bb-4b45-fae6-96a69186bd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing orca context\n",
            "Current pyspark location is : /usr/local/lib/python3.7/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.7/dist-packages/zoo/share/lib/analytics-zoo-bigdl_0.13.1-SNAPSHOT-spark_2.4.6-0.12.0-SNAPSHOT-jar-with-dependencies.jar:/usr/local/lib/python3.7/dist-packages/bigdl/share/lib/bigdl-0.13.1-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
            "2022-02-08 13:15:08 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/local/lib/python3.7/dist-packages/zoo/share/lib/analytics-zoo-bigdl_0.13.1-SNAPSHOT-spark_2.4.6-0.12.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
            "BigDLBasePickler registering: bigdl.util.common  Sample\n",
            "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
            "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
            "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
            "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
            "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
            "BigDLBasePickler registering: bigdl.util.common  JActivity\n",
            "Successfully got a SparkContext\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "User settings:\n",
            "\n",
            "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
            "   KMP_BLOCKTIME=0\n",
            "   KMP_SETTINGS=1\n",
            "   OMP_NUM_THREADS=1\n",
            "\n",
            "Effective settings:\n",
            "\n",
            "   KMP_ABORT_DELAY=0\n",
            "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
            "   KMP_ALIGN_ALLOC=64\n",
            "   KMP_ALL_THREADPRIVATE=128\n",
            "   KMP_ATOMIC_MODE=2\n",
            "   KMP_BLOCKTIME=0\n",
            "   KMP_CPUINFO_FILE: value is not defined\n",
            "   KMP_DETERMINISTIC_REDUCTION=false\n",
            "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
            "   KMP_DISP_HAND_THREAD=false\n",
            "   KMP_DISP_NUM_BUFFERS=7\n",
            "   KMP_DUPLICATE_LIB_OK=false\n",
            "   KMP_FORCE_REDUCTION: value is not defined\n",
            "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
            "   KMP_FORKJOIN_BARRIER='2,2'\n",
            "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
            "   KMP_FORKJOIN_FRAMES=true\n",
            "   KMP_FORKJOIN_FRAMES_MODE=3\n",
            "   KMP_GTID_MODE=3\n",
            "   KMP_HANDLE_SIGNALS=false\n",
            "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
            "   KMP_HOT_TEAMS_MODE=0\n",
            "   KMP_INIT_AT_FORK=true\n",
            "   KMP_ITT_PREPARE_DELAY=0\n",
            "   KMP_LIBRARY=throughput\n",
            "   KMP_LOCK_KIND=queuing\n",
            "   KMP_MALLOC_POOL_INCR=1M\n",
            "   KMP_MWAIT_HINTS=0\n",
            "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
            "   KMP_PLAIN_BARRIER='2,2'\n",
            "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
            "   KMP_REDUCTION_BARRIER='1,1'\n",
            "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
            "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
            "   KMP_SETTINGS=true\n",
            "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
            "   KMP_STACKOFFSET=64\n",
            "   KMP_STACKPAD=0\n",
            "   KMP_STACKSIZE=8M\n",
            "   KMP_STORAGE_MAP=false\n",
            "   KMP_TASKING=2\n",
            "   KMP_TASKLOOP_MIN_TASKS=0\n",
            "   KMP_TASK_STEALING_CONSTRAINT=1\n",
            "   KMP_TEAMS_THREAD_LIMIT=2\n",
            "   KMP_TOPOLOGY_METHOD=all\n",
            "   KMP_USER_LEVEL_MWAIT=false\n",
            "   KMP_USE_YIELD=1\n",
            "   KMP_VERSION=false\n",
            "   KMP_WARNINGS=true\n",
            "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
            "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
            "   OMP_CANCELLATION=false\n",
            "   OMP_DEBUG=disabled\n",
            "   OMP_DEFAULT_DEVICE=0\n",
            "   OMP_DISPLAY_AFFINITY=false\n",
            "   OMP_DISPLAY_ENV=false\n",
            "   OMP_DYNAMIC=false\n",
            "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
            "   OMP_MAX_TASK_PRIORITY=0\n",
            "   OMP_NESTED=false\n",
            "   OMP_NUM_THREADS='1'\n",
            "   OMP_PLACES: value is not defined\n",
            "   OMP_PROC_BIND='intel'\n",
            "   OMP_SCHEDULE='static'\n",
            "   OMP_STACKSIZE=8M\n",
            "   OMP_TARGET_OFFLOAD=DEFAULT\n",
            "   OMP_THREAD_LIMIT=2147483647\n",
            "   OMP_TOOL=enabled\n",
            "   OMP_TOOL_LIBRARIES: value is not defined\n",
            "   OMP_WAIT_POLICY=PASSIVE\n",
            "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c6da07730e98:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.6</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[2] appName=pyspark-shell>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# recommended to set it to True when running Analytics Zoo in Jupyter notebook \n",
        "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
        "\n",
        "init_orca_context(cluster_mode=\"local\", cores=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the spark session from the Orca Context\n",
        "spark = OrcaContext.get_spark_session()"
      ],
      "metadata": {
        "id": "IEvpdpMx_DrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Getting the dataset from Kaggle:**"
      ],
      "metadata": {
        "id": "RYl8c7srPHUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We manually downloaded the Kaggle dataset and stored it in Google Drive\n",
        "# (https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia)\n",
        "import os\n",
        "base_path = \"/content/drive/MyDrive/BigData/chest_xray\"\n",
        "train_path = os.path.join(base_path, \"train\")\n",
        "val_path = os.path.join(base_path, \"val\")\n",
        "test_path = os.path.join(base_path, \"test\")\n",
        "train_path, val_path, test_path\n",
        "# each of those folders contain two additional folders,\n",
        "#PNEUMONIA for pneumonia images and NORMAL for normal images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y19cptLgPcL6",
        "outputId": "82d806c3-e700-4828-e739-60fe90ad7e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/BigData/chest_xray/train',\n",
              " '/content/drive/MyDrive/BigData/chest_xray/val',\n",
              " '/content/drive/MyDrive/BigData/chest_xray/test')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation dataset from Kaggle contains only 16 images (8 with pneumonia and 8 without pneumonia). This is a very small validation dataset, so we will ignore it, and we will instead split the training dataset into 80% training and 20% validation."
      ],
      "metadata": {
        "id": "L73iEnb50vif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the Datasets:**"
      ],
      "metadata": {
        "id": "gJBoBXNH0i7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're going to perform Transfer Learning on a Keras model, we will use TensorFlow Datasets to load our data. We're inside of the **Orca Context**, and it allows us to use and modify TensorFlow Datasets in a **distributed** way (https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html).\n",
        "\n",
        "Orca will automatically replicate the TensorFlow Dataset pipeline on each node in the cluster, executing the data pipelines using Apache Spark and/or Ray distributedly."
      ],
      "metadata": {
        "id": "2UDPZT412Yj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to use them inside of the Orca Estimator, we will create **data_creator functions**. These functions contain every step that occurs from the creation of the Datasets until the dataset is ready to be used by the Estimator. Orca allows any transformation or pipeline that occurs to the dataset to be distributed. Inside of these functions there are the following steps:"
      ],
      "metadata": {
        "id": "oezzF5XR1J-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the TensorFlow Datasets**\n",
        "\n",
        "We will use Keras function **image_dataset_from_directory** to create the TF Datasets, resizing them to (299, 299), the size of the InceptionV3 input; and setting the number of channels to 3 (rgb), also needed by InceptionV3.\n",
        "\n",
        "In the case of the training and validation sets, they both come from the same directory (train_path), but are in different functions, so the datasets won't be shuffled until the 80-20% split has been done. This way, no repeated images will be in the training and validation sets."
      ],
      "metadata": {
        "id": "Fb_FxiKJ6-B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data augmentation (training set only)**"
      ],
      "metadata": {
        "id": "MhZowC6zSpAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because of the low amount of data that we have, we will use **data augmentation** to try and artificially increase the size of our training dataset. To do that, we will randomly flip horizontally some images, rotate them slightly, and zoom out randomly. We create a data_augmentation pipeline using Keras Sequential API, and we duplicate the training set and apply the augmentation pipeline.\n",
        "\n",
        "The Orca context allows this and other transformations to happen in a distributed way."
      ],
      "metadata": {
        "id": "WnGCnY7-7jWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional preprocessing (every dataset)**"
      ],
      "metadata": {
        "id": "jA1VFMpByAwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply a transformation to every dataset, **preprocess_input**, which is required in order to use the InceptionV3 model. This function is provided by Keras, and among other things, scales the pixels values between -1 and 1, which is what InceptionV3 accepts as input (https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input). \n",
        "\n",
        "In the end, we will cast it to tf.float32 to specify the type of the tensor and try and avoid issues with conversions."
      ],
      "metadata": {
        "id": "TYlnq_RQ8yBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# importing the necessary libraries"
      ],
      "metadata": {
        "id": "gn1KQU0ieA8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# As explained in the documentation (https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/QuickStart/orca-tf2keras-quickstart.html)\n",
        "# We create data_creator functions, that will load the datasets and perform all of the \n",
        "# previous steps inside of the Orca Estimator\n",
        "\n",
        "# Training set:\n",
        "def train_data_creator(config, batch_size):\n",
        "    target_size = (299, 299)\n",
        "    # Creating a dataset from the directory train_path\n",
        "    dataset = image_dataset_from_directory(train_path, \n",
        "                                            image_size=target_size, \n",
        "                                            batch_size = 1,\n",
        "                                            shuffle=False,\n",
        "                                            color_mode='rgb',\n",
        "                                            label_mode='binary',\n",
        "                                            class_names = ['NORMAL', 'PNEUMONIA'])\n",
        "    # Every image needs to be the same shape for the neural network \n",
        "    # In the case of InceptionV3 we will use 299 x 299\n",
        "    # The function image_dataset_from_directory automatically resizes it\n",
        "    # color_mode = \"rgb\" automatically sets the channel number to 3\n",
        "    # (even though they are grayscale, we need (x, x, 3) shape to use InceptionV3)\n",
        "\n",
        "    # This dataset will be split into train and val, but we need to do it in different functions,\n",
        "    # so I have included shuffle = False and batch_size = 1 \n",
        "    # (so the images are obtained one by one, and in the same order in both cases)\n",
        "\n",
        "    # For the training set:\n",
        "    # Obtaining the full size of the dataset\n",
        "    ds_size = dataset.cardinality().numpy()\n",
        "    # 80% goes to train, 20% will go to validation\n",
        "    train_size = int(0.8 * ds_size)\n",
        "    # Taking the first 80% of the dataset\n",
        "    dataset = dataset.take(train_size)\n",
        "    # Now we can shuffle the dataset and divide it in batches\n",
        "    dataset = dataset.shuffle(train_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # In the training set we will perform data augmentation:\n",
        "    # Randomly flipping horizontally\n",
        "    # randomly rotating between 0.1 * 2pi radians clockwise and 0.1 * 2pi radians counterclockwise\n",
        "    # randomly zooming out between 5 and 15% of the image size\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            layers.RandomRotation((-0.1, 0.1)),\n",
        "            layers.RandomZoom((0.05, 0.15)),\n",
        "        ]\n",
        "    )\n",
        "    # Duplicating the dataset\n",
        "    dataset = dataset.repeat(2)\n",
        "    # Reshaping it to the expected shape \n",
        "    # (because we apply batch twice, in image_dataset_from_directory and in dataset.batch(),\n",
        "    # the shape was (None, None, 299, 299, 3) instead of (None, 299, 299, 3), so we reshape it)\n",
        "    dataset = dataset.map(lambda x, y: (tf.reshape(x, (-1, 299, 299, 3)), y))\n",
        "    # Applying the data augmentation pipeline\n",
        "    dataset = dataset.map(lambda x, y: (data_augmentation(x, training = True), y))\n",
        "\n",
        "    # Finally, preprocessing steps common to every dataset:\n",
        "    dataset = dataset.map(lambda x, y: (keras.applications.inception_v3.preprocess_input(x), y))\n",
        "    # Casting it to tf.float32 to make conversions easier (because it also gave problems without it)\n",
        "    dataset = dataset.map(lambda x, y: (tf.cast(x, dtype = tf.float32), y))\n",
        "\n",
        "    # Returning the processed dataset, ready to be used in training\n",
        "    return dataset\n",
        "\n",
        "# Validation set:\n",
        "def val_data_creator(config, batch_size):\n",
        "    target_size = (299, 299)\n",
        "    # Creating a dataset from the directory train_path\n",
        "    # (train_path again, we're using the last 20% as validation)\n",
        "    dataset = image_dataset_from_directory(train_path, \n",
        "                                            image_size=target_size, \n",
        "                                            batch_size = 1,\n",
        "                                            shuffle=False,\n",
        "                                            color_mode='rgb',\n",
        "                                            label_mode='binary',\n",
        "                                            class_names = ['NORMAL', 'PNEUMONIA'])\n",
        "    # This dataset will be split into train and val, but we need to do it in different functions,\n",
        "    # so I include shuffle = False and batch_size = 1 \n",
        "    # (so the images are obtained one by one, and in the same order in both cases)\n",
        "\n",
        "    # For the validation set:\n",
        "    # Obtaining the full size of the dataset\n",
        "    ds_size = dataset.cardinality().numpy()\n",
        "    # 80% went to train, 20% goes to validation\n",
        "    train_size = int(0.8 * ds_size)\n",
        "    val_size = ds_size - train_size\n",
        "    # Skipping the first 80% and taking the remaining 20%\n",
        "    dataset = dataset.skip(train_size).take(val_size)\n",
        "    # Now we can shuffle the dataset and divide it in batches\n",
        "    dataset = dataset.shuffle(val_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # The validation and test sets don't go through data augmentation\n",
        "\n",
        "    # Reshaping it to the expected shape\n",
        "    dataset = dataset.map(lambda x, y: (tf.reshape(x, (-1, 299, 299, 3)), y))\n",
        "\n",
        "    # Finally, preprocessing steps common to every dataset:\n",
        "    dataset = dataset.map(lambda x, y: (keras.applications.inception_v3.preprocess_input(x), y))\n",
        "    # Casting it to tf.float32 to make conversions easier (because it also gave problems without it)\n",
        "    dataset = dataset.map(lambda x, y: (tf.cast(x, dtype = tf.float32), y))\n",
        "    \n",
        "    # Returning the processed dataset\n",
        "    return dataset\n",
        "\n",
        "# Test set:\n",
        "def test_data_creator(config, batch_size):\n",
        "    target_size = (299, 299)\n",
        "    # Creating a dataset from the directory test_path\n",
        "    dataset = image_dataset_from_directory(test_path, \n",
        "                                            image_size=target_size, \n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle=True,\n",
        "                                            color_mode='rgb',\n",
        "                                            label_mode='binary',\n",
        "                                            class_names = ['NORMAL', 'PNEUMONIA'])\n",
        "    # In this case, there is no splitting the dataset or anything, so we directly\n",
        "    # establish shuffle = True and the batch size.\n",
        "    \n",
        "    # The validation and test sets don't go through data augmentation\n",
        "\n",
        "    # Reshaping it to the expected shape\n",
        "    dataset = dataset.map(lambda x, y: (tf.reshape(x, (-1, 299, 299, 3)), y))\n",
        "\n",
        "    # Finally, preprocessing steps common to every dataset:\n",
        "    dataset = dataset.map(lambda x, y: (keras.applications.inception_v3.preprocess_input(x), y))\n",
        "    # Casting it to tf.float32 to make conversions easier (because it also gave problems without it)\n",
        "    dataset = dataset.map(lambda x, y: (tf.cast(x, dtype = tf.float32), y))\n",
        "    \n",
        "    # Returning the processed dataset\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "s3tnKA7weYMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building the model:**"
      ],
      "metadata": {
        "id": "I61w-dAofIUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're loading the **InceptionV3 pre-trained model** from Keras without the top (last) layer, freezing all of its layers (so they don't change during training), and adding at the end some additional layers so that it can be applied to our problem:\n",
        "\n",
        "*   x = MaxPooling2D((2, 2))(pre_trained_model.output)\n",
        "*   x = Conv2D(256, (1, 1), activation=LeakyReLU())(x)\n",
        "*   x = BatchNormalization()(x)\n",
        "\n",
        "The previous block reduces the output of the pre-trained model (with MaxPooling2D), maintaining only the maximum value of each 2x2 window; then passes it through a convolutional layer; and finally it normalizes it. We're using Leaky ReLU as the activation function of Conv2D to avoid having a gradient equal to 0 (which would happen with ReLU when the input < 0), but normal ReLU could be used either way probably.\n",
        "    \n",
        "*   x = Flatten()(x)\n",
        "*   x = Dropout(0.75)(x)\n",
        "\n",
        "The previous block flattens the input (from 2D to 1D), and then applies dropout regularization (inactivating randomly 75% of the cells) to avoid overfitting. The dropout percentage can be tuned up (if there is overfitting, to increase even more the regularization) or down (if there is underfitting, to reduce the regularization).\n",
        "   \n",
        "*   outputs = Dense(1, activation='sigmoid')(x)  \n",
        "\n",
        "We include the final Dense layer for the binary classification between 0 (normal) and 1 (pneumonia).\n",
        "\n",
        "To train the model, we're using the **RMSprop optimizer**, which according to the documentation is the best for InceptionV3 (https://cloud.google.com/tpu/docs/inception-v3-advanced#optimizer). The metric is binary crossentropy for binary classification, and the metrics that we're considering are accuracy, precision, recall and the area under precision-recall curve."
      ],
      "metadata": {
        "id": "FPwaUoh7_6Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Flatten, Dense, Dropout, Normalization, BatchNormalization, LeakyReLU, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, Rescaling\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "# Custom model joining every layer and the pretrained InceptionV3 model:\n",
        "def model_creator(config):\n",
        "    import tensorflow as tf\n",
        "    input_shape = (299, 299, 3)\n",
        "\n",
        "  ### Pretrained model:\n",
        "    # the pretrained InceptionV3 model:\n",
        "    pre_trained_model = InceptionV3(input_shape = input_shape, # Shape of our images, (299, 299, 3)\n",
        "                                    include_top = False, # Leave out the last fully connected layer\n",
        "                                    weights = 'imagenet')\n",
        "    pre_trained_model.trainable = False # freezing the pretrained model so it doesn't change during training\n",
        "\n",
        "  ### After the pretrained model:\n",
        "    x = MaxPooling2D((2, 2))(pre_trained_model.output)\n",
        "    x = Conv2D(256, (1, 1), activation=LeakyReLU())(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.75)(x)\n",
        "   \n",
        "    # Add a final sigmoid layer for binary classification\n",
        "    outputs = Dense(1, activation='sigmoid')(x)  \n",
        "\n",
        "    model = keras.Model(pre_trained_model.inputs, outputs)\n",
        "\n",
        "    ### Compiling the model:\n",
        "    # loss function: binary cross entropy (for binary classification)\n",
        "    # optimizer: RMSProp is the optimal for InceptionV3 according to:\n",
        "    # https://cloud.google.com/tpu/docs/inception-v3-advanced#optimizer\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = 0.0001),\n",
        "                 loss=keras.losses.BinaryCrossentropy(), \n",
        "                 metrics=['accuracy', \n",
        "           keras.metrics.Precision(name='precision'), \n",
        "           keras.metrics.Recall(name='recall'), \n",
        "           keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "          ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "eQ2YlOHkn3Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model training:**"
      ],
      "metadata": {
        "id": "RCstVTaRCxFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **Estimator class** provided by Orca. Creating this Estimator from the Keras model allows to distribute the training of the model across each node of the cluster (https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/distributed-training-inference.html)."
      ],
      "metadata": {
        "id": "Af1B-x1-C1i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zoo.orca.learn.tf2.estimator import Estimator\n",
        "# using Orca to create an Estimator from the custom model created by the model_creator function\n",
        "est = Estimator.from_keras(model_creator = model_creator)"
      ],
      "metadata": {
        "id": "utcsAd8PFWmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15ab9ac-5eac-4f69-91df-90388161de3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-08 13:31:29,701\tINFO services.py:1174 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://172.28.0.2:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'node_ip_address': '172.28.0.2', 'raylet_ip_address': '172.28.0.2', 'redis_address': '172.28.0.2:6379', 'object_store_address': '/tmp/ray/session_2022-02-08_13-31-28_739036_70/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-02-08_13-31-28_739036_70/sockets/raylet', 'webui_url': '172.28.0.2:8265', 'session_dir': '/tmp/ray/session_2022-02-08_13-31-28_739036_70', 'metrics_export_port': 58833, 'node_id': '3312e38635c8da7f3ff12e42b4df0dc63f0643c3f28d2a32fa9cb560'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/zoo/orca/learn/tf2/tf_runner.py:317: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 13:31:39.044088: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            " 3891200/87910968 [>.............................] - ETA: 1s\n",
            " 4202496/87910968 [>.............................] - ETA: 3s\n",
            " 8871936/87910968 [==>...........................] - ETA: 1s\n",
            "14680064/87910968 [====>.........................] - ETA: 1s\n",
            "20152320/87910968 [=====>........................] - ETA: 1s\n",
            "26836992/87910968 [========>.....................] - ETA: 0s\n",
            "33112064/87910968 [==========>...................] - ETA: 0s\n",
            "39690240/87910968 [============>.................] - ETA: 0s\n",
            "45604864/87910968 [==============>...............] - ETA: 0s\n",
            "52166656/87910968 [================>.............] - ETA: 0s\n",
            "58933248/87910968 [===================>..........] - ETA: 0s\n",
            "64479232/87910968 [=====================>........] - ETA: 0s\n",
            "69844992/87910968 [======================>.......] - ETA: 0s\n",
            "76292096/87910968 [=========================>....] - ETA: 0s\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a Keras callback to use early stopping inside of the Orca estimator\n",
        "my_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=2,\n",
        "    verbose=2,\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "MWlPMLy5_uQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = est.fit(train_data_creator,\n",
        "                epochs=5,\n",
        "                batch_size = 32,\n",
        "                validation_data=val_data_creator,\n",
        "                callbacks = my_callback,)\n",
        "                \n",
        "#est.save(\"/tmp/Xray_InceptionV3_keras.ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WqFzMdpMDem7",
        "outputId": "a254104e-2d31-4cea-f7f2-f09d3e077bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Found 5216 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function train_data_creator.<locals>.<lambda> at 0x7f0966747830> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function train_data_creator.<locals>.<lambda> at 0x7f0966747830>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function train_data_creator.<locals>.<lambda> at 0x7f0966747e60> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function train_data_creator.<locals>.<lambda> at 0x7f0966747e60>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function train_data_creator.<locals>.<lambda> at 0x7f0966747ef0> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function train_data_creator.<locals>.<lambda> at 0x7f0966747ef0>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function train_data_creator.<locals>.<lambda> at 0x7f0966747dd0> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function train_data_creator.<locals>.<lambda> at 0x7f0966747dd0>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Found 5216 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function val_data_creator.<locals>.<lambda> at 0x7f09675d8a70> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function val_data_creator.<locals>.<lambda> at 0x7f09675d8a70>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function val_data_creator.<locals>.<lambda> at 0x7f09676dc5f0> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function val_data_creator.<locals>.<lambda> at 0x7f09676dc5f0>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function val_data_creator.<locals>.<lambda> at 0x7f09676dc9e0> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function val_data_creator.<locals>.<lambda> at 0x7f09676dc9e0>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 13:38:49.374860: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m op: \"TensorSliceDataset\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m input: \"Placeholder/_0\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"Toutput_types\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       type: DT_STRING\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"_cardinality\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     i: 5216\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"is_files\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     b: false\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"metadata\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     s: \"\\n\\025TensorSliceDataset:11\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"output_shapes\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       shape {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 13:38:49.755114: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Epoch 1/5\n",
            "  1/262 [..............................] - ETA: 49:50:04 - loss: 1.5063 - accuracy: 0.5000 - precision: 0.6667 - recall: 0.4762 - prc: 0.6958\n",
            "  2/262 [..............................] - ETA: 29:41 - loss: 1.1709 - accuracy: 0.5938 - precision: 0.6944 - recall: 0.6250 - prc: 0.7173   \n",
            "  3/262 [..............................] - ETA: 29:53 - loss: 1.1598 - accuracy: 0.5625 - precision: 0.7083 - recall: 0.5484 - prc: 0.7367\n",
            "  4/262 [..............................] - ETA: 30:10 - loss: 1.2212 - accuracy: 0.5547 - precision: 0.7015 - recall: 0.5595 - prc: 0.7074\n",
            "  5/262 [..............................] - ETA: 29:53 - loss: 1.1154 - accuracy: 0.5813 - precision: 0.7000 - recall: 0.6117 - prc: 0.7212\n",
            "  6/262 [..............................] - ETA: 29:47 - loss: 1.0695 - accuracy: 0.5833 - precision: 0.7027 - recall: 0.6240 - prc: 0.7279\n",
            "  7/262 [..............................] - ETA: 29:58 - loss: 1.0416 - accuracy: 0.5759 - precision: 0.7015 - recall: 0.6309 - prc: 0.7395\n",
            "  8/262 [..............................] - ETA: 30:01 - loss: 0.9973 - accuracy: 0.5820 - precision: 0.7051 - recall: 0.6433 - prc: 0.7539\n",
            "  9/262 [>.............................] - ETA: 30:02 - loss: 1.0012 - accuracy: 0.5694 - precision: 0.7018 - recall: 0.6218 - prc: 0.7544\n",
            " 10/262 [>.............................] - ETA: 29:54 - loss: 0.9887 - accuracy: 0.5875 - precision: 0.7225 - recall: 0.6359 - prc: 0.7738\n",
            " 11/262 [>.............................] - ETA: 29:48 - loss: 0.9853 - accuracy: 0.5966 - precision: 0.7286 - recall: 0.6429 - prc: 0.7761\n",
            " 12/262 [>.............................] - ETA: 29:40 - loss: 0.9674 - accuracy: 0.6068 - precision: 0.7308 - recall: 0.6602 - prc: 0.7792\n",
            " 13/262 [>.............................] - ETA: 29:25 - loss: 0.9490 - accuracy: 0.6130 - precision: 0.7364 - recall: 0.6714 - prc: 0.7852\n",
            " 14/262 [>.............................] - ETA: 29:17 - loss: 0.9286 - accuracy: 0.6205 - precision: 0.7401 - recall: 0.6766 - prc: 0.7880\n",
            " 15/262 [>.............................] - ETA: 29:08 - loss: 0.9025 - accuracy: 0.6271 - precision: 0.7517 - recall: 0.6809 - prc: 0.7969\n",
            " 16/262 [>.............................] - ETA: 28:55 - loss: 0.8695 - accuracy: 0.6406 - precision: 0.7647 - recall: 0.6958 - prc: 0.8137\n",
            " 17/262 [>.............................] - ETA: 28:44 - loss: 0.8485 - accuracy: 0.6452 - precision: 0.7708 - recall: 0.7042 - prc: 0.8240\n",
            " 18/262 [=>............................] - ETA: 28:38 - loss: 0.8349 - accuracy: 0.6493 - precision: 0.7692 - recall: 0.7160 - prc: 0.8285\n",
            " 19/262 [=>............................] - ETA: 28:28 - loss: 0.8203 - accuracy: 0.6530 - precision: 0.7708 - recall: 0.7183 - prc: 0.8336\n",
            " 20/262 [=>............................] - ETA: 28:22 - loss: 0.8218 - accuracy: 0.6562 - precision: 0.7718 - recall: 0.7162 - prc: 0.8299\n",
            " 21/262 [=>............................] - ETA: 28:12 - loss: 0.8201 - accuracy: 0.6548 - precision: 0.7673 - recall: 0.7177 - prc: 0.8287\n",
            " 22/262 [=>............................] - ETA: 28:06 - loss: 0.8171 - accuracy: 0.6577 - precision: 0.7659 - recall: 0.7231 - prc: 0.8259\n",
            " 23/262 [=>............................] - ETA: 28:04 - loss: 0.7988 - accuracy: 0.6658 - precision: 0.7704 - recall: 0.7307 - prc: 0.8321\n",
            " 24/262 [=>............................] - ETA: 27:57 - loss: 0.7850 - accuracy: 0.6732 - precision: 0.7762 - recall: 0.7333 - prc: 0.8347\n",
            " 25/262 [=>............................] - ETA: 27:50 - loss: 0.7808 - accuracy: 0.6750 - precision: 0.7774 - recall: 0.7377 - prc: 0.8393\n",
            " 26/262 [=>............................] - ETA: 27:40 - loss: 0.7692 - accuracy: 0.6791 - precision: 0.7804 - recall: 0.7408 - prc: 0.8436\n",
            " 27/262 [==>...........................] - ETA: 27:30 - loss: 0.7582 - accuracy: 0.6829 - precision: 0.7829 - recall: 0.7432 - prc: 0.8470\n",
            " 28/262 [==>...........................] - ETA: 27:21 - loss: 0.7465 - accuracy: 0.6864 - precision: 0.7828 - recall: 0.7455 - prc: 0.8488\n",
            " 29/262 [==>...........................] - ETA: 27:12 - loss: 0.7360 - accuracy: 0.6918 - precision: 0.7824 - recall: 0.7524 - prc: 0.8478\n",
            " 30/262 [==>...........................] - ETA: 27:02 - loss: 0.7338 - accuracy: 0.6885 - precision: 0.7816 - recall: 0.7465 - prc: 0.8489\n",
            " 31/262 [==>...........................] - ETA: 26:53 - loss: 0.7284 - accuracy: 0.6905 - precision: 0.7853 - recall: 0.7466 - prc: 0.8518\n",
            " 32/262 [==>...........................] - ETA: 26:45 - loss: 0.7192 - accuracy: 0.6943 - precision: 0.7891 - recall: 0.7493 - prc: 0.8571\n",
            " 33/262 [==>...........................] - ETA: 26:36 - loss: 0.7106 - accuracy: 0.6970 - precision: 0.7915 - recall: 0.7517 - prc: 0.8611\n",
            " 34/262 [==>...........................] - ETA: 26:28 - loss: 0.7173 - accuracy: 0.6976 - precision: 0.7920 - recall: 0.7524 - prc: 0.8586\n",
            " 35/262 [===>..........................] - ETA: 26:19 - loss: 0.7120 - accuracy: 0.7036 - precision: 0.7956 - recall: 0.7579 - prc: 0.8585\n",
            " 36/262 [===>..........................] - ETA: 26:12 - loss: 0.7010 - accuracy: 0.7092 - precision: 0.7970 - recall: 0.7632 - prc: 0.8613\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  37/262 [===>..........................] - ETA: 26:04 - loss: 0.6984 - accuracy: 0.7111 - precision: 0.7997 - recall: 0.7638 - prc: 0.8620\n",
            " 38/262 [===>..........................] - ETA: 25:57 - loss: 0.6897 - accuracy: 0.7130 - precision: 0.8003 - recall: 0.7661 - prc: 0.8645\n",
            " 39/262 [===>..........................] - ETA: 25:51 - loss: 0.6829 - accuracy: 0.7171 - precision: 0.8025 - recall: 0.7711 - prc: 0.8673\n",
            " 40/262 [===>..........................] - ETA: 25:46 - loss: 0.6827 - accuracy: 0.7164 - precision: 0.8024 - recall: 0.7699 - prc: 0.8676\n",
            " 41/262 [===>..........................] - ETA: 25:39 - loss: 0.6743 - accuracy: 0.7188 - precision: 0.8035 - recall: 0.7718 - prc: 0.8698\n",
            " 42/262 [===>..........................] - ETA: 25:31 - loss: 0.6719 - accuracy: 0.7202 - precision: 0.8076 - recall: 0.7703 - prc: 0.8726\n",
            " 43/262 [===>..........................] - ETA: 25:23 - loss: 0.6736 - accuracy: 0.7188 - precision: 0.8049 - recall: 0.7712 - prc: 0.8719\n",
            " 44/262 [====>.........................] - ETA: 25:16 - loss: 0.6719 - accuracy: 0.7202 - precision: 0.8055 - recall: 0.7733 - prc: 0.8730\n",
            " 45/262 [====>.........................] - ETA: 25:08 - loss: 0.6702 - accuracy: 0.7194 - precision: 0.8038 - recall: 0.7741 - prc: 0.8736\n",
            " 46/262 [====>.........................] - ETA: 25:00 - loss: 0.6739 - accuracy: 0.7194 - precision: 0.8033 - recall: 0.7751 - prc: 0.8736\n",
            " 47/262 [====>.........................] - ETA: 24:52 - loss: 0.6754 - accuracy: 0.7201 - precision: 0.8022 - recall: 0.7761 - prc: 0.8731\n",
            " 48/262 [====>.........................] - ETA: 24:44 - loss: 0.6667 - accuracy: 0.7240 - precision: 0.8050 - recall: 0.7801 - prc: 0.8757\n",
            " 49/262 [====>.........................] - ETA: 24:37 - loss: 0.6631 - accuracy: 0.7264 - precision: 0.8070 - recall: 0.7819 - prc: 0.8763\n",
            " 50/262 [====>.........................] - ETA: 24:31 - loss: 0.6657 - accuracy: 0.7256 - precision: 0.8067 - recall: 0.7806 - prc: 0.8759\n",
            " 51/262 [====>.........................] - ETA: 24:25 - loss: 0.6630 - accuracy: 0.7286 - precision: 0.8079 - recall: 0.7836 - prc: 0.8760\n",
            " 52/262 [====>.........................] - ETA: 24:18 - loss: 0.6571 - accuracy: 0.7314 - precision: 0.8097 - recall: 0.7859 - prc: 0.8784\n",
            " 53/262 [=====>........................] - ETA: 24:10 - loss: 0.6501 - accuracy: 0.7341 - precision: 0.8112 - recall: 0.7877 - prc: 0.8804\n",
            " 54/262 [=====>........................] - ETA: 24:03 - loss: 0.6478 - accuracy: 0.7355 - precision: 0.8122 - recall: 0.7892 - prc: 0.8818\n",
            " 55/262 [=====>........................] - ETA: 23:57 - loss: 0.6425 - accuracy: 0.7369 - precision: 0.8135 - recall: 0.7909 - prc: 0.8838\n",
            " 56/262 [=====>........................] - ETA: 23:51 - loss: 0.6377 - accuracy: 0.7383 - precision: 0.8150 - recall: 0.7914 - prc: 0.8854\n",
            " 57/262 [=====>........................] - ETA: 23:45 - loss: 0.6416 - accuracy: 0.7390 - precision: 0.8154 - recall: 0.7915 - prc: 0.8836\n",
            " 58/262 [=====>........................] - ETA: 23:38 - loss: 0.6368 - accuracy: 0.7419 - precision: 0.8184 - recall: 0.7943 - prc: 0.8861\n",
            " 59/262 [=====>........................] - ETA: 23:32 - loss: 0.6323 - accuracy: 0.7436 - precision: 0.8191 - recall: 0.7959 - prc: 0.8871\n",
            " 60/262 [=====>........................] - ETA: 23:24 - loss: 0.6275 - accuracy: 0.7458 - precision: 0.8205 - recall: 0.7977 - prc: 0.8888\n",
            " 61/262 [=====>........................] - ETA: 23:17 - loss: 0.6251 - accuracy: 0.7464 - precision: 0.8217 - recall: 0.7986 - prc: 0.8899\n",
            " 62/262 [======>.......................] - ETA: 23:11 - loss: 0.6243 - accuracy: 0.7465 - precision: 0.8203 - recall: 0.8001 - prc: 0.8895\n",
            " 63/262 [======>.......................] - ETA: 23:03 - loss: 0.6209 - accuracy: 0.7485 - precision: 0.8203 - recall: 0.8028 - prc: 0.8898\n",
            " 64/262 [======>.......................] - ETA: 22:55 - loss: 0.6209 - accuracy: 0.7495 - precision: 0.8207 - recall: 0.8046 - prc: 0.8891\n",
            " 65/262 [======>.......................] - ETA: 22:47 - loss: 0.6173 - accuracy: 0.7505 - precision: 0.8208 - recall: 0.8061 - prc: 0.8911\n",
            " 66/262 [======>.......................] - ETA: 22:40 - loss: 0.6174 - accuracy: 0.7514 - precision: 0.8197 - recall: 0.8076 - prc: 0.8903\n",
            " 67/262 [======>.......................] - ETA: 22:32 - loss: 0.6130 - accuracy: 0.7542 - precision: 0.8229 - recall: 0.8098 - prc: 0.8925\n",
            " 68/262 [======>.......................] - ETA: 22:25 - loss: 0.6085 - accuracy: 0.7555 - precision: 0.8235 - recall: 0.8112 - prc: 0.8936\n",
            " 69/262 [======>.......................] - ETA: 22:17 - loss: 0.6057 - accuracy: 0.7563 - precision: 0.8253 - recall: 0.8109 - prc: 0.8951\n",
            " 70/262 [=======>......................] - ETA: 22:09 - loss: 0.6026 - accuracy: 0.7571 - precision: 0.8263 - recall: 0.8110 - prc: 0.8965\n",
            " 71/262 [=======>......................] - ETA: 22:01 - loss: 0.6004 - accuracy: 0.7584 - precision: 0.8277 - recall: 0.8121 - prc: 0.8978\n",
            " 72/262 [=======>......................] - ETA: 21:53 - loss: 0.5958 - accuracy: 0.7600 - precision: 0.8289 - recall: 0.8135 - prc: 0.8994\n",
            " 73/262 [=======>......................] - ETA: 21:46 - loss: 0.5916 - accuracy: 0.7620 - precision: 0.8315 - recall: 0.8147 - prc: 0.9015\n",
            " 74/262 [=======>......................] - ETA: 21:38 - loss: 0.5874 - accuracy: 0.7635 - precision: 0.8331 - recall: 0.8166 - prc: 0.9033\n",
            " 75/262 [=======>......................] - ETA: 21:31 - loss: 0.5880 - accuracy: 0.7646 - precision: 0.8333 - recall: 0.8175 - prc: 0.9028\n",
            " 76/262 [=======>......................] - ETA: 21:23 - loss: 0.5876 - accuracy: 0.7652 - precision: 0.8336 - recall: 0.8180 - prc: 0.9036\n",
            " 77/262 [=======>......................] - ETA: 21:15 - loss: 0.5819 - accuracy: 0.7670 - precision: 0.8355 - recall: 0.8195 - prc: 0.9056\n",
            " 78/262 [=======>......................] - ETA: 21:08 - loss: 0.5798 - accuracy: 0.7676 - precision: 0.8348 - recall: 0.8201 - prc: 0.9059\n",
            " 79/262 [========>.....................] - ETA: 21:00 - loss: 0.5764 - accuracy: 0.7678 - precision: 0.8363 - recall: 0.8193 - prc: 0.9072\n",
            " 80/262 [========>.....................] - ETA: 20:53 - loss: 0.5731 - accuracy: 0.7691 - precision: 0.8370 - recall: 0.8201 - prc: 0.9077\n",
            " 81/262 [========>.....................] - ETA: 20:45 - loss: 0.5687 - accuracy: 0.7708 - precision: 0.8384 - recall: 0.8212 - prc: 0.9089\n",
            " 82/262 [========>.....................] - ETA: 20:37 - loss: 0.5665 - accuracy: 0.7713 - precision: 0.8388 - recall: 0.8209 - prc: 0.9097\n",
            " 83/262 [========>.....................] - ETA: 20:30 - loss: 0.5630 - accuracy: 0.7733 - precision: 0.8400 - recall: 0.8233 - prc: 0.9102\n",
            " 84/262 [========>.....................] - ETA: 20:22 - loss: 0.5605 - accuracy: 0.7738 - precision: 0.8413 - recall: 0.8230 - prc: 0.9114\n",
            " 85/262 [========>.....................] - ETA: 20:15 - loss: 0.5569 - accuracy: 0.7754 - precision: 0.8426 - recall: 0.8240 - prc: 0.9125\n",
            " 86/262 [========>.....................] - ETA: 20:07 - loss: 0.5526 - accuracy: 0.7769 - precision: 0.8441 - recall: 0.8253 - prc: 0.9137\n",
            " 87/262 [========>.....................] - ETA: 19:59 - loss: 0.5486 - accuracy: 0.7784 - precision: 0.8455 - recall: 0.8264 - prc: 0.9149\n",
            " 88/262 [=========>....................] - ETA: 19:52 - loss: 0.5470 - accuracy: 0.7788 - precision: 0.8456 - recall: 0.8267 - prc: 0.9152\n",
            " 89/262 [=========>....................] - ETA: 19:44 - loss: 0.5423 - accuracy: 0.7809 - precision: 0.8467 - recall: 0.8284 - prc: 0.9162\n",
            " 90/262 [=========>....................] - ETA: 19:37 - loss: 0.5387 - accuracy: 0.7823 - precision: 0.8474 - recall: 0.8297 - prc: 0.9171\n",
            " 91/262 [=========>....................] - ETA: 19:29 - loss: 0.5351 - accuracy: 0.7837 - precision: 0.8483 - recall: 0.8311 - prc: 0.9183\n",
            " 92/262 [=========>....................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 19:22 - loss: 0.5315 - accuracy: 0.7850 - precision: 0.8487 - recall: 0.8322 - prc: 0.9191\n",
            " 93/262 [=========>....................] - ETA: 19:15 - loss: 0.5312 - accuracy: 0.7853 - precision: 0.8487 - recall: 0.8323 - prc: 0.9193\n",
            " 94/262 [=========>....................] - ETA: 19:08 - loss: 0.5278 - accuracy: 0.7869 - precision: 0.8500 - recall: 0.8338 - prc: 0.9204\n",
            " 95/262 [=========>....................] - ETA: 19:07 - loss: 0.5254 - accuracy: 0.7875 - precision: 0.8501 - recall: 0.8345 - prc: 0.9211\n",
            " 96/262 [=========>....................] - ETA: 19:03 - loss: 0.5233 - accuracy: 0.7887 - precision: 0.8513 - recall: 0.8354 - prc: 0.9221\n",
            " 97/262 [==========>...................] - ETA: 18:56 - loss: 0.5205 - accuracy: 0.7896 - precision: 0.8521 - recall: 0.8355 - prc: 0.9229\n",
            " 98/262 [==========>...................] - ETA: 18:49 - loss: 0.5172 - accuracy: 0.7908 - precision: 0.8532 - recall: 0.8364 - prc: 0.9238\n",
            " 99/262 [==========>...................] - ETA: 18:42 - loss: 0.5139 - accuracy: 0.7926 - precision: 0.8542 - recall: 0.8379 - prc: 0.9245\n",
            "100/262 [==========>...................] - ETA: 18:35 - loss: 0.5147 - accuracy: 0.7931 - precision: 0.8539 - recall: 0.8390 - prc: 0.9243\n",
            "101/262 [==========>...................] - ETA: 18:28 - loss: 0.5117 - accuracy: 0.7942 - precision: 0.8545 - recall: 0.8401 - prc: 0.9251\n",
            "102/262 [==========>...................] - ETA: 18:21 - loss: 0.5102 - accuracy: 0.7950 - precision: 0.8560 - recall: 0.8402 - prc: 0.9260\n",
            "103/262 [==========>...................] - ETA: 18:13 - loss: 0.5070 - accuracy: 0.7958 - precision: 0.8572 - recall: 0.8400 - prc: 0.9268\n",
            "104/262 [==========>...................] - ETA: 18:06 - loss: 0.5070 - accuracy: 0.7960 - precision: 0.8575 - recall: 0.8397 - prc: 0.9265\n",
            "105/262 [===========>..................] - ETA: 18:06 - loss: 0.5083 - accuracy: 0.7955 - precision: 0.8567 - recall: 0.8398 - prc: 0.9258\n",
            "106/262 [===========>..................] - ETA: 18:01 - loss: 0.5090 - accuracy: 0.7954 - precision: 0.8559 - recall: 0.8403 - prc: 0.9253\n",
            "107/262 [===========>..................] - ETA: 17:54 - loss: 0.5081 - accuracy: 0.7967 - precision: 0.8564 - recall: 0.8417 - prc: 0.9254\n",
            "108/262 [===========>..................] - ETA: 17:47 - loss: 0.5051 - accuracy: 0.7980 - precision: 0.8573 - recall: 0.8427 - prc: 0.9262\n",
            "109/262 [===========>..................] - ETA: 17:39 - loss: 0.5036 - accuracy: 0.7990 - precision: 0.8582 - recall: 0.8433 - prc: 0.9267\n",
            "110/262 [===========>..................] - ETA: 17:32 - loss: 0.5007 - accuracy: 0.8003 - precision: 0.8591 - recall: 0.8444 - prc: 0.9274\n",
            "111/262 [===========>..................] - ETA: 17:26 - loss: 0.4987 - accuracy: 0.8018 - precision: 0.8598 - recall: 0.8455 - prc: 0.9273\n",
            "112/262 [===========>..................] - ETA: 17:18 - loss: 0.4976 - accuracy: 0.8025 - precision: 0.8608 - recall: 0.8459 - prc: 0.9279\n",
            "113/262 [===========>..................] - ETA: 17:11 - loss: 0.4983 - accuracy: 0.8025 - precision: 0.8608 - recall: 0.8461 - prc: 0.9283\n",
            "114/262 [============>.................] - ETA: 17:04 - loss: 0.4964 - accuracy: 0.8032 - precision: 0.8612 - recall: 0.8466 - prc: 0.9290\n",
            "115/262 [============>.................] - ETA: 16:56 - loss: 0.4971 - accuracy: 0.8035 - precision: 0.8609 - recall: 0.8474 - prc: 0.9286\n",
            "116/262 [============>.................] - ETA: 16:49 - loss: 0.4941 - accuracy: 0.8047 - precision: 0.8619 - recall: 0.8478 - prc: 0.9291\n",
            "117/262 [============>.................] - ETA: 16:42 - loss: 0.4911 - accuracy: 0.8058 - precision: 0.8626 - recall: 0.8486 - prc: 0.9297\n",
            "118/262 [============>.................] - ETA: 16:35 - loss: 0.4886 - accuracy: 0.8064 - precision: 0.8630 - recall: 0.8492 - prc: 0.9303\n",
            "119/262 [============>.................] - ETA: 16:27 - loss: 0.4877 - accuracy: 0.8072 - precision: 0.8640 - recall: 0.8492 - prc: 0.9308\n",
            "120/262 [============>.................] - ETA: 16:20 - loss: 0.4860 - accuracy: 0.8076 - precision: 0.8641 - recall: 0.8498 - prc: 0.9313\n",
            "121/262 [============>.................] - ETA: 16:12 - loss: 0.4856 - accuracy: 0.8076 - precision: 0.8637 - recall: 0.8502 - prc: 0.9315\n",
            "122/262 [============>.................] - ETA: 16:05 - loss: 0.4855 - accuracy: 0.8074 - precision: 0.8634 - recall: 0.8504 - prc: 0.9316\n",
            "123/262 [=============>................] - ETA: 15:58 - loss: 0.4863 - accuracy: 0.8077 - precision: 0.8642 - recall: 0.8502 - prc: 0.9320\n",
            "124/262 [=============>................] - ETA: 15:50 - loss: 0.4858 - accuracy: 0.8075 - precision: 0.8638 - recall: 0.8504 - prc: 0.9323\n",
            "125/262 [=============>................] - ETA: 15:43 - loss: 0.4846 - accuracy: 0.8083 - precision: 0.8641 - recall: 0.8511 - prc: 0.9325\n",
            "126/262 [=============>................] - ETA: 15:42 - loss: 0.4832 - accuracy: 0.8085 - precision: 0.8649 - recall: 0.8510 - prc: 0.9330\n",
            "127/262 [=============>................] - ETA: 15:41 - loss: 0.4830 - accuracy: 0.8086 - precision: 0.8652 - recall: 0.8508 - prc: 0.9335\n",
            "128/262 [=============>................] - ETA: 15:40 - loss: 0.4815 - accuracy: 0.8091 - precision: 0.8654 - recall: 0.8511 - prc: 0.9337\n",
            "129/262 [=============>................] - ETA: 15:39 - loss: 0.4808 - accuracy: 0.8098 - precision: 0.8662 - recall: 0.8517 - prc: 0.9342\n",
            "130/262 [=============>................] - ETA: 15:39 - loss: 0.4804 - accuracy: 0.8101 - precision: 0.8663 - recall: 0.8516 - prc: 0.9339\n",
            "131/262 [==============>...............] - ETA: 15:29 - loss: 0.4797 - accuracy: 0.8104 - precision: 0.8666 - recall: 0.8516 - prc: 0.9340\n",
            "132/262 [==============>...............] - ETA: 15:23 - loss: 0.4770 - accuracy: 0.8116 - precision: 0.8677 - recall: 0.8525 - prc: 0.9347\n",
            "133/262 [==============>...............] - ETA: 15:15 - loss: 0.4758 - accuracy: 0.8116 - precision: 0.8669 - recall: 0.8530 - prc: 0.9349\n",
            "134/262 [==============>...............] - ETA: 15:08 - loss: 0.4748 - accuracy: 0.8121 - precision: 0.8672 - recall: 0.8534 - prc: 0.9353\n",
            "135/262 [==============>...............] - ETA: 15:00 - loss: 0.4737 - accuracy: 0.8128 - precision: 0.8679 - recall: 0.8539 - prc: 0.9357\n",
            "136/262 [==============>...............] - ETA: 14:53 - loss: 0.4719 - accuracy: 0.8130 - precision: 0.8679 - recall: 0.8543 - prc: 0.9362\n",
            "137/262 [==============>...............] - ETA: 14:45 - loss: 0.4691 - accuracy: 0.8144 - precision: 0.8690 - recall: 0.8555 - prc: 0.9368\n",
            "138/262 [==============>...............] - ETA: 14:38 - loss: 0.4687 - accuracy: 0.8146 - precision: 0.8696 - recall: 0.8553 - prc: 0.9372\n",
            "139/262 [==============>...............] - ETA: 14:31 - loss: 0.4681 - accuracy: 0.8150 - precision: 0.8700 - recall: 0.8558 - prc: 0.9374\n",
            "140/262 [===============>..............]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 14:24 - loss: 0.4659 - accuracy: 0.8159 - precision: 0.8703 - recall: 0.8568 - prc: 0.9379\n",
            "141/262 [===============>..............] - ETA: 14:16 - loss: 0.4665 - accuracy: 0.8163 - precision: 0.8707 - recall: 0.8567 - prc: 0.9380\n",
            "142/262 [===============>..............] - ETA: 14:09 - loss: 0.4658 - accuracy: 0.8163 - precision: 0.8706 - recall: 0.8567 - prc: 0.9382\n",
            "143/262 [===============>..............] - ETA: 14:02 - loss: 0.4633 - accuracy: 0.8174 - precision: 0.8716 - recall: 0.8575 - prc: 0.9387\n",
            "144/262 [===============>..............] - ETA: 13:54 - loss: 0.4619 - accuracy: 0.8182 - precision: 0.8722 - recall: 0.8582 - prc: 0.9388\n",
            "145/262 [===============>..............] - ETA: 13:47 - loss: 0.4606 - accuracy: 0.8186 - precision: 0.8729 - recall: 0.8579 - prc: 0.9392\n",
            "146/262 [===============>..............] - ETA: 13:40 - loss: 0.4592 - accuracy: 0.8190 - precision: 0.8734 - recall: 0.8579 - prc: 0.9396\n",
            "147/262 [===============>..............] - ETA: 13:32 - loss: 0.4571 - accuracy: 0.8200 - precision: 0.8740 - recall: 0.8588 - prc: 0.9400\n",
            "148/262 [===============>..............] - ETA: 13:25 - loss: 0.4557 - accuracy: 0.8202 - precision: 0.8736 - recall: 0.8594 - prc: 0.9403\n",
            "149/262 [================>.............] - ETA: 13:18 - loss: 0.4534 - accuracy: 0.8208 - precision: 0.8742 - recall: 0.8598 - prc: 0.9409\n",
            "150/262 [================>.............]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 13:11 - loss: 0.4536 - accuracy: 0.8209 - precision: 0.8741 - recall: 0.8601 - prc: 0.9409\n",
            "151/262 [================>.............] - ETA: 13:03 - loss: 0.4547 - accuracy: 0.8207 - precision: 0.8734 - recall: 0.8603 - prc: 0.9405\n",
            "152/262 [================>.............] - ETA: 12:56 - loss: 0.4562 - accuracy: 0.8210 - precision: 0.8736 - recall: 0.8606 - prc: 0.9403\n",
            "153/262 [================>.............] - ETA: 12:49 - loss: 0.4541 - accuracy: 0.8216 - precision: 0.8736 - recall: 0.8615 - prc: 0.9407\n",
            "154/262 [================>.............] - ETA: 12:42 - loss: 0.4543 - accuracy: 0.8215 - precision: 0.8742 - recall: 0.8611 - prc: 0.9410\n",
            "155/262 [================>.............] - ETA: 12:34 - loss: 0.4520 - accuracy: 0.8223 - precision: 0.8745 - recall: 0.8620 - prc: 0.9415\n",
            "156/262 [================>.............] - ETA: 12:27 - loss: 0.4508 - accuracy: 0.8226 - precision: 0.8747 - recall: 0.8623 - prc: 0.9418\n",
            "157/262 [================>.............] - ETA: 12:20 - loss: 0.4492 - accuracy: 0.8227 - precision: 0.8751 - recall: 0.8620 - prc: 0.9421\n",
            "158/262 [=================>............] - ETA: 12:13 - loss: 0.4492 - accuracy: 0.8231 - precision: 0.8754 - recall: 0.8623 - prc: 0.9424\n",
            "159/262 [=================>............] - ETA: 12:06 - loss: 0.4475 - accuracy: 0.8236 - precision: 0.8759 - recall: 0.8627 - prc: 0.9428\n",
            "160/262 [=================>............] - ETA: 11:58 - loss: 0.4469 - accuracy: 0.8241 - precision: 0.8764 - recall: 0.8630 - prc: 0.9430\n",
            "161/262 [=================>............] - ETA: 11:51 - loss: 0.4463 - accuracy: 0.8244 - precision: 0.8765 - recall: 0.8632 - prc: 0.9431\n",
            "162/262 [=================>............] - ETA: 11:44 - loss: 0.4446 - accuracy: 0.8247 - precision: 0.8768 - recall: 0.8635 - prc: 0.9436\n",
            "163/262 [=================>............] - ETA: 11:37 - loss: 0.4428 - accuracy: 0.8254 - precision: 0.8776 - recall: 0.8639 - prc: 0.9441\n",
            "164/262 [=================>............] - ETA: 11:29 - loss: 0.4430 - accuracy: 0.8259 - precision: 0.8780 - recall: 0.8641 - prc: 0.9442\n",
            "165/262 [=================>............] - ETA: 11:23 - loss: 0.4422 - accuracy: 0.8262 - precision: 0.8789 - recall: 0.8642 - prc: 0.9447\n",
            "166/262 [==================>...........] - ETA: 11:16 - loss: 0.4398 - accuracy: 0.8271 - precision: 0.8794 - recall: 0.8650 - prc: 0.9452\n",
            "167/262 [==================>...........] - ETA: 11:12 - loss: 0.4390 - accuracy: 0.8274 - precision: 0.8795 - recall: 0.8652 - prc: 0.9455\n",
            "168/262 [==================>...........] - ETA: 11:06 - loss: 0.4401 - accuracy: 0.8273 - precision: 0.8795 - recall: 0.8652 - prc: 0.9457\n",
            "169/262 [==================>...........] - ETA: 11:01 - loss: 0.4408 - accuracy: 0.8276 - precision: 0.8793 - recall: 0.8657 - prc: 0.9455\n",
            "170/262 [==================>...........] - ETA: 10:53 - loss: 0.4385 - accuracy: 0.8284 - precision: 0.8802 - recall: 0.8664 - prc: 0.9460\n",
            "171/262 [==================>...........] - ETA: 10:46 - loss: 0.4372 - accuracy: 0.8289 - precision: 0.8810 - recall: 0.8665 - prc: 0.9464\n",
            "172/262 [==================>...........] - ETA: 10:38 - loss: 0.4369 - accuracy: 0.8290 - precision: 0.8808 - recall: 0.8667 - prc: 0.9465\n",
            "173/262 [==================>...........] - ETA: 10:31 - loss: 0.4369 - accuracy: 0.8287 - precision: 0.8803 - recall: 0.8669 - prc: 0.9463\n",
            "174/262 [==================>...........] - ETA: 10:24 - loss: 0.4366 - accuracy: 0.8291 - precision: 0.8807 - recall: 0.8672 - prc: 0.9466\n",
            "175/262 [===================>..........] - ETA: 10:17 - loss: 0.4358 - accuracy: 0.8292 - precision: 0.8812 - recall: 0.8671 - prc: 0.9469\n",
            "176/262 [===================>..........] - ETA: 10:09 - loss: 0.4363 - accuracy: 0.8289 - precision: 0.8808 - recall: 0.8670 - prc: 0.9469\n",
            "177/262 [===================>..........] - ETA: 10:02 - loss: 0.4344 - accuracy: 0.8297 - precision: 0.8812 - recall: 0.8677 - prc: 0.9472\n",
            "178/262 [===================>..........] - ETA: 9:55 - loss: 0.4351 - accuracy: 0.8298 - precision: 0.8808 - recall: 0.8681 - prc: 0.9469 \n",
            "179/262 [===================>..........] - ETA: 9:47 - loss: 0.4342 - accuracy: 0.8295 - precision: 0.8801 - recall: 0.8685 - prc: 0.9471\n",
            "180/262 [===================>..........] - ETA: 9:40 - loss: 0.4326 - accuracy: 0.8301 - precision: 0.8805 - recall: 0.8691 - prc: 0.9474\n",
            "181/262 [===================>..........] - ETA: 9:33 - loss: 0.4342 - accuracy: 0.8300 - precision: 0.8805 - recall: 0.8691 - prc: 0.9468\n",
            "182/262 [===================>..........] - ETA: 9:26 - loss: 0.4333 - accuracy: 0.8306 - precision: 0.8807 - recall: 0.8698 - prc: 0.9467\n",
            "183/262 [===================>..........] - ETA: 9:18 - loss: 0.4335 - accuracy: 0.8309 - precision: 0.8808 - recall: 0.8699 - prc: 0.9468\n",
            "184/262 [====================>.........] - ETA: 9:11 - loss: 0.4332 - accuracy: 0.8311 - precision: 0.8812 - recall: 0.8699 - prc: 0.9469\n",
            "185/262 [====================>.........] - ETA: 9:04 - loss: 0.4355 - accuracy: 0.8308 - precision: 0.8813 - recall: 0.8695 - prc: 0.9470\n",
            "186/262 [====================>.........] - ETA: 8:56 - loss: 0.4350 - accuracy: 0.8309 - precision: 0.8814 - recall: 0.8694 - prc: 0.9470\n",
            "187/262 [====================>.........] - ETA: 8:49 - loss: 0.4338 - accuracy: 0.8313 - precision: 0.8817 - recall: 0.8695 - prc: 0.9472\n",
            "188/262 [====================>.........] - ETA: 8:42 - loss: 0.4319 - accuracy: 0.8319 - precision: 0.8820 - recall: 0.8700 - prc: 0.9476\n",
            "189/262 [====================>.........] - ETA: 8:35 - loss: 0.4298 - accuracy: 0.8328 - precision: 0.8827 - recall: 0.8707 - prc: 0.9481\n",
            "190/262 [====================>.........] - ETA: 8:28 - loss: 0.4302 - accuracy: 0.8327 - precision: 0.8823 - recall: 0.8708 - prc: 0.9477\n",
            "191/262 [====================>.........] - ETA: 8:20 - loss: 0.4289 - accuracy: 0.8332 - precision: 0.8829 - recall: 0.8710 - prc: 0.9480\n",
            "192/262 [====================>.........] - ETA: 8:13 - loss: 0.4270 - accuracy: 0.8339 - precision: 0.8835 - recall: 0.8714 - prc: 0.9484\n",
            "193/262 [=====================>........] - ETA: 8:06 - loss: 0.4259 - accuracy: 0.8345 - precision: 0.8841 - recall: 0.8716 - prc: 0.9487\n",
            "194/262 [=====================>........] - ETA: 7:59 - loss: 0.4248 - accuracy: 0.8348 - precision: 0.8844 - recall: 0.8718 - prc: 0.9489\n",
            "195/262 [=====================>........] - ETA: 7:51 - loss: 0.4250 - accuracy: 0.8350 - precision: 0.8841 - recall: 0.8720 - prc: 0.9486\n",
            "196/262 [=====================>........] - ETA: 7:44 - loss: 0.4245 - accuracy: 0.8354 - precision: 0.8840 - recall: 0.8726 - prc: 0.9486\n",
            "197/262 [=====================>........] - ETA: 7:37 - loss: 0.4236 - accuracy: 0.8359 - precision: 0.8846 - recall: 0.8728 - prc: 0.9489\n",
            "198/262 [=====================>........] - ETA: 7:30 - loss: 0.4220 - accuracy: 0.8364 - precision: 0.8850 - recall: 0.8733 - prc: 0.9492\n",
            "199/262 [=====================>........] - ETA: 7:23 - loss: 0.4214 - accuracy: 0.8368 - precision: 0.8852 - recall: 0.8737 - prc: 0.9494\n",
            "200/262 [=====================>........] - ETA: 7:16 - loss: 0.4202 - accuracy: 0.8373 - precision: 0.8855 - recall: 0.8741 - prc: 0.9497\n",
            "201/262 [======================>.......] - ETA: 7:08 - loss: 0.4187 - accuracy: 0.8378 - precision: 0.8860 - recall: 0.8746 - prc: 0.9500\n",
            "202/262 [======================>.......] - ETA: 7:01 - loss: 0.4170 - accuracy: 0.8385 - precision: 0.8863 - recall: 0.8752 - prc: 0.9503\n",
            "203/262 [======================>.......] - ETA: 6:54 - loss: 0.4160 - accuracy: 0.8388 - precision: 0.8864 - recall: 0.8756 - prc: 0.9505\n",
            "204/262 [======================>.......] - ETA: 6:47 - loss: 0.4156 - accuracy: 0.8388 - precision: 0.8859 - recall: 0.8762 - prc: 0.9506\n",
            "205/262 [======================>.......] - ETA: 6:40 - loss: 0.4140 - accuracy: 0.8393 - precision: 0.8865 - recall: 0.8763 - prc: 0.9509\n",
            "206/262 [======================>.......] - ETA: 6:33 - loss: 0.4143 - accuracy: 0.8396 - precision: 0.8867 - recall: 0.8764 - prc: 0.9510\n",
            "207/262 [======================>.......] - ETA: 6:26 - loss: 0.4150 - accuracy: 0.8399 - precision: 0.8873 - recall: 0.8765 - prc: 0.9513\n",
            "208/262 [======================>.......] - ETA: 6:18 - loss: 0.4132 - accuracy: 0.8407 - precision: 0.8880 - recall: 0.8772 - prc: 0.9517\n",
            "209/262 [======================>.......] - ETA: 6:11 - loss: 0.4133 - accuracy: 0.8407 - precision: 0.8879 - recall: 0.8773 - prc: 0.9516\n",
            "210/262 [=======================>......] - ETA: 6:04 - loss: 0.4130 - accuracy: 0.8410 - precision: 0.8881 - recall: 0.8774 - prc: 0.9518\n",
            "211/262 [=======================>......] - ETA: 5:57 - loss: 0.4128 - accuracy: 0.8414 - precision: 0.8880 - recall: 0.8780 - prc: 0.9517\n",
            "212/262 [=======================>......] - ETA: 5:50 - loss: 0.4126 - accuracy: 0.8415 - precision: 0.8881 - recall: 0.8781 - prc: 0.9517\n",
            "213/262 [=======================>......] - ETA: 5:43 - loss: 0.4120 - accuracy: 0.8417 - precision: 0.8884 - recall: 0.8780 - prc: 0.9518\n",
            "214/262 [=======================>......] - ETA: 5:36 - loss: 0.4114 - accuracy: 0.8418 - precision: 0.8885 - recall: 0.8782 - prc: 0.9520\n",
            "215/262 [=======================>......] - ETA: 5:29 - loss: 0.4098 - accuracy: 0.8426 - precision: 0.8891 - recall: 0.8788 - prc: 0.9523\n",
            "216/262 [=======================>......] - ETA: 5:22 - loss: 0.4100 - accuracy: 0.8424 - precision: 0.8884 - recall: 0.8790 - prc: 0.9522\n",
            "217/262 [=======================>......] - ETA: 5:15 - loss: 0.4103 - accuracy: 0.8424 - precision: 0.8882 - recall: 0.8790 - prc: 0.9519\n",
            "218/262 [=======================>......] - ETA: 5:07 - loss: 0.4100 - accuracy: 0.8426 - precision: 0.8881 - recall: 0.8792 - prc: 0.9519\n",
            "219/262 [========================>.....] - ETA: 5:00 - loss: 0.4084 - accuracy: 0.8433 - precision: 0.8886 - recall: 0.8798 - prc: 0.9522\n",
            "220/262 [========================>.....] - ETA: 4:53 - loss: 0.4070 - accuracy: 0.8439 - precision: 0.8889 - recall: 0.8803 - prc: 0.9525\n",
            "221/262 [========================>.....] - ETA: 4:46 - loss: 0.4065 - accuracy: 0.8439 - precision: 0.8889 - recall: 0.8805 - prc: 0.9527\n",
            "222/262 [========================>.....] - ETA: 4:39 - loss: 0.4059 - accuracy: 0.8443 - precision: 0.8894 - recall: 0.8807 - prc: 0.9529\n",
            "223/262 [========================>.....] - ETA: 4:32 - loss: 0.4044 - accuracy: 0.8449 - precision: 0.8897 - recall: 0.8813 - prc: 0.9532\n",
            "224/262 [========================>.....] - ETA: 4:25 - loss: 0.4039 - accuracy: 0.8449 - precision: 0.8902 - recall: 0.8809 - prc: 0.9534\n",
            "225/262 [========================>.....] - ETA: 4:18 - loss: 0.4031 - accuracy: 0.8451 - precision: 0.8905 - recall: 0.8810 - prc: 0.9536\n",
            "226/262 [========================>.....] - ETA: 4:11 - loss: 0.4020 - accuracy: 0.8455 - precision: 0.8907 - recall: 0.8812 - prc: 0.9537\n",
            "227/262 [========================>.....] - ETA: 4:04 - loss: 0.4015 - accuracy: 0.8457 - precision: 0.8910 - recall: 0.8812 - prc: 0.9540\n",
            "228/262 [=========================>....] - ETA: 3:57 - loss: 0.4007 - accuracy: 0.8459 - precision: 0.8910 - recall: 0.8815 - prc: 0.9541\n",
            "229/262 [=========================>....] - ETA: 3:50 - loss: 0.4017 - accuracy: 0.8461 - precision: 0.8912 - recall: 0.8814 - prc: 0.9540\n",
            "230/262 [=========================>....] - ETA: 3:43 - loss: 0.4011 - accuracy: 0.8465 - precision: 0.8915 - recall: 0.8816 - prc: 0.9540\n",
            "231/262 [=========================>....] - ETA: 3:36 - loss: 0.4006 - accuracy: 0.8464 - precision: 0.8909 - recall: 0.8821 - prc: 0.9541\n",
            "232/262 [=========================>....] - ETA: 3:29 - loss: 0.4001 - accuracy: 0.8466 - precision: 0.8912 - recall: 0.8820 - prc: 0.9542\n",
            "233/262 [=========================>....] - ETA: 3:22 - loss: 0.4005 - accuracy: 0.8463 - precision: 0.8913 - recall: 0.8816 - prc: 0.9543\n",
            "234/262 [=========================>....] - ETA: 3:15 - loss: 0.3998 - accuracy: 0.8465 - precision: 0.8915 - recall: 0.8817 - prc: 0.9544\n",
            "235/262 [=========================>....] - ETA: 3:08 - loss: 0.3988 - accuracy: 0.8468 - precision: 0.8918 - recall: 0.8818 - prc: 0.9546\n",
            "236/262 [==========================>...] - ETA: 3:01 - loss: 0.3979 - accuracy: 0.8472 - precision: 0.8920 - recall: 0.8824 - prc: 0.9548\n",
            "237/262 [==========================>...] - ETA: 2:55 - loss: 0.3966 - accuracy: 0.8477 - precision: 0.8922 - recall: 0.8829 - prc: 0.9551\n",
            "238/262 [==========================>...] - ETA: 2:48 - loss: 0.3969 - accuracy: 0.8477 - precision: 0.8918 - recall: 0.8830 - prc: 0.9548\n",
            "239/262 [==========================>...] - ETA: 2:41 - loss: 0.3966 - accuracy: 0.8478 - precision: 0.8917 - recall: 0.8832 - prc: 0.9548\n",
            "240/262 [==========================>...] - ETA: 2:34 - loss: 0.3963 - accuracy: 0.8479 - precision: 0.8919 - recall: 0.8832 - prc: 0.9549\n",
            "241/262 [==========================>...] - ETA: 2:27 - loss: 0.3952 - accuracy: 0.8484 - precision: 0.8921 - recall: 0.8836 - prc: 0.9551\n",
            "242/262 [==========================>...] - ETA: 2:20 - loss: 0.3945 - accuracy: 0.8489 - precision: 0.8924 - recall: 0.8841 - prc: 0.9550\n",
            "243/262 [==========================>...] - ETA: 2:13 - loss: 0.3932 - accuracy: 0.8494 - precision: 0.8929 - recall: 0.8844 - prc: 0.9553\n",
            "244/262 [==========================>...] - ETA: 2:06 - loss: 0.3927 - accuracy: 0.8495 - precision: 0.8928 - recall: 0.8847 - prc: 0.9554\n",
            "245/262 [===========================>..] - ETA: 1:59 - loss: 0.3918 - accuracy: 0.8499 - precision: 0.8930 - recall: 0.8849 - prc: 0.9555\n",
            "246/262 [===========================>..] - ETA: 1:52 - loss: 0.3904 - accuracy: 0.8505 - precision: 0.8935 - recall: 0.8854 - prc: 0.9558\n",
            "247/262 [===========================>..] - ETA: 1:45 - loss: 0.3897 - accuracy: 0.8506 - precision: 0.8937 - recall: 0.8854 - prc: 0.9560\n",
            "248/262 [===========================>..] - ETA: 1:38 - loss: 0.3893 - accuracy: 0.8509 - precision: 0.8938 - recall: 0.8859 - prc: 0.9559\n",
            "249/262 [===========================>..] - ETA: 1:31 - loss: 0.3903 - accuracy: 0.8505 - precision: 0.8936 - recall: 0.8853 - prc: 0.9557\n",
            "250/262 [===========================>..] - ETA: 1:24 - loss: 0.3908 - accuracy: 0.8504 - precision: 0.8931 - recall: 0.8855 - prc: 0.9555\n",
            "251/262 [===========================>..] - ETA: 1:17 - loss: 0.3895 - accuracy: 0.8508 - precision: 0.8934 - recall: 0.8860 - prc: 0.9557\n",
            "252/262 [===========================>..] - ETA: 1:10 - loss: 0.3903 - accuracy: 0.8509 - precision: 0.8932 - recall: 0.8861 - prc: 0.9556\n",
            "253/262 [===========================>..] - ETA: 1:03 - loss: 0.3893 - accuracy: 0.8512 - precision: 0.8934 - recall: 0.8863 - prc: 0.9558\n",
            "254/262 [============================>.] - ETA: 56s - loss: 0.3882 - accuracy: 0.8515 - precision: 0.8936 - recall: 0.8865 - prc: 0.9560 \n",
            "255/262 [============================>.] - ETA: 49s - loss: 0.3881 - accuracy: 0.8514 - precision: 0.8935 - recall: 0.8864 - prc: 0.9560\n",
            "256/262 [============================>.] - ETA: 42s - loss: 0.3870 - accuracy: 0.8518 - precision: 0.8937 - recall: 0.8868 - prc: 0.9562\n",
            "257/262 [============================>.] - ETA: 35s - loss: 0.3858 - accuracy: 0.8523 - precision: 0.8939 - recall: 0.8871 - prc: 0.9564\n",
            "258/262 [============================>.] - ETA: 28s - loss: 0.3852 - accuracy: 0.8526 - precision: 0.8944 - recall: 0.8873 - prc: 0.9566\n",
            "259/262 [============================>.] - ETA: 21s - loss: 0.3860 - accuracy: 0.8526 - precision: 0.8943 - recall: 0.8874 - prc: 0.9566\n",
            "260/262 [============================>.] - ETA: 14s - loss: 0.3847 - accuracy: 0.8531 - precision: 0.8947 - recall: 0.8879 - prc: 0.9569\n",
            "261/262 [============================>.] - ETA: 7s - loss: 0.3839 - accuracy: 0.8535 - precision: 0.8950 - recall: 0.8882 - prc: 0.9570 \n",
            "262/262 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.8535 - precision: 0.8952 - recall: 0.8882 - prc: 0.9571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 14:20:48.469267: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m op: \"TensorSliceDataset\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m input: \"Placeholder/_0\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"Toutput_types\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       type: DT_STRING\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"_cardinality\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     i: 5216\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"is_files\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     b: false\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"metadata\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     s: \"\\n\\025TensorSliceDataset:26\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"output_shapes\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       shape {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 14:20:48.725848: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r262/262 [==============================] - 3071s 9s/step - loss: 0.3840 - accuracy: 0.8535 - precision: 0.8952 - recall: 0.8882 - prc: 0.9571 - val_loss: 0.1262 - val_accuracy: 0.9540 - val_precision: 1.0000 - val_recall: 0.9540 - val_prc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 14:30:01.007864: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Epoch 2/5\n",
            "  1/262 [..............................] - ETA: 5:13:04 - loss: 0.7699 - accuracy: 0.8125 - precision: 0.7368 - recall: 0.9333 - prc: 0.7849\n",
            "  2/262 [..............................] - ETA: 39:16 - loss: 0.5588 - accuracy: 0.8594 - precision: 0.8750 - recall: 0.8974 - prc: 0.8953  \n",
            "  3/262 [..............................] - ETA: 40:51 - loss: 0.4489 - accuracy: 0.8958 - precision: 0.9000 - recall: 0.9310 - prc: 0.9111\n",
            "  4/262 [..............................] - ETA: 44:22 - loss: 0.3925 - accuracy: 0.8828 - precision: 0.9000 - recall: 0.9114 - prc: 0.9296\n",
            "  5/262 [..............................] - ETA: 42:42 - loss: 0.3849 - accuracy: 0.8875 - precision: 0.9184 - recall: 0.9000 - prc: 0.9398\n",
            "  6/262 [..............................] - ETA: 40:39 - loss: 0.3809 - accuracy: 0.8958 - precision: 0.9250 - recall: 0.9098 - prc: 0.9469\n",
            "  7/262 [..............................] - ETA: 39:25 - loss: 0.3491 - accuracy: 0.9018 - precision: 0.9348 - recall: 0.9085 - prc: 0.9535\n",
            "  8/262 [..............................] - ETA: 38:12 - loss: 0.3356 - accuracy: 0.8984 - precision: 0.9317 - recall: 0.9091 - prc: 0.9577\n",
            "  9/262 [>.............................] - ETA: 38:19 - loss: 0.3294 - accuracy: 0.8958 - precision: 0.9344 - recall: 0.9048 - prc: 0.9616\n",
            " 10/262 [>.............................] - ETA: 38:46 - loss: 0.3109 - accuracy: 0.8969 - precision: 0.9275 - recall: 0.9143 - prc: 0.9649\n",
            " 11/262 [>.............................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 38:14 - loss: 0.3198 - accuracy: 0.8977 - precision: 0.9336 - recall: 0.9095 - prc: 0.9661\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            " 12/262 [>.............................] - ETA: 37:55 - loss: 0.3306 - accuracy: 0.8984 - precision: 0.9390 - recall: 0.9059 - prc: 0.9670\n",
            " 13/262 [>.............................] - ETA: 38:10 - loss: 0.3431 - accuracy: 0.8942 - precision: 0.9242 - recall: 0.9104 - prc: 0.9649\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  14/262 [>.............................] - ETA: 38:29 - loss: 0.3478 - accuracy: 0.8906 - precision: 0.9291 - recall: 0.9003 - prc: 0.9655\n",
            " 15/262 [>.............................] - ETA: 38:50 - loss: 0.3321 - accuracy: 0.8958 - precision: 0.9346 - recall: 0.9051 - prc: 0.9680\n",
            " 16/262 [>.............................] - ETA: 39:47 - loss: 0.3247 - accuracy: 0.8984 - precision: 0.9350 - recall: 0.9069 - prc: 0.9687\n",
            " 17/262 [>.............................] - ETA: 39:31 - loss: 0.3196 - accuracy: 0.8971 - precision: 0.9364 - recall: 0.9050 - prc: 0.9703\n",
            " 18/262 [=>............................] - ETA: 38:40 - loss: 0.3118 - accuracy: 0.8993 - precision: 0.9375 - recall: 0.9079 - prc: 0.9716\n",
            " 19/262 [=>............................] - ETA: 37:52 - loss: 0.3140 - accuracy: 0.8980 - precision: 0.9309 - recall: 0.9123 - prc: 0.9714\n",
            " 20/262 [=>............................] - ETA: 37:11 - loss: 0.3070 - accuracy: 0.8984 - precision: 0.9296 - recall: 0.9141 - prc: 0.9720\n",
            " 21/262 [=>............................] - ETA: 36:31 - loss: 0.3057 - accuracy: 0.8973 - precision: 0.9291 - recall: 0.9144 - prc: 0.9724\n",
            " 22/262 [=>............................] - ETA: 35:54 - loss: 0.3048 - accuracy: 0.8977 - precision: 0.9298 - recall: 0.9138 - prc: 0.9728\n",
            " 23/262 [=>............................] - ETA: 35:21 - loss: 0.3044 - accuracy: 0.8981 - precision: 0.9311 - recall: 0.9139 - prc: 0.9733\n",
            " 24/262 [=>............................] - ETA: 34:51 - loss: 0.3005 - accuracy: 0.8971 - precision: 0.9303 - recall: 0.9139 - prc: 0.9741\n",
            " 25/262 [=>............................] - ETA: 34:41 - loss: 0.3025 - accuracy: 0.8963 - precision: 0.9294 - recall: 0.9137 - prc: 0.9741\n",
            " 26/262 [=>............................] - ETA: 34:40 - loss: 0.3040 - accuracy: 0.8954 - precision: 0.9286 - recall: 0.9135 - prc: 0.9738\n",
            " 27/262 [==>...........................] - ETA: 34:26 - loss: 0.2963 - accuracy: 0.8981 - precision: 0.9311 - recall: 0.9149 - prc: 0.9748\n",
            " 28/262 [==>...........................] - ETA: 34:01 - loss: 0.2893 - accuracy: 0.9007 - precision: 0.9323 - recall: 0.9183 - prc: 0.9758\n",
            " 29/262 [==>...........................] - ETA: 33:37 - loss: 0.2902 - accuracy: 0.9009 - precision: 0.9329 - recall: 0.9179 - prc: 0.9759\n",
            " 30/262 [==>...........................] - ETA: 33:13 - loss: 0.2925 - accuracy: 0.9000 - precision: 0.9300 - recall: 0.9184 - prc: 0.9757\n",
            " 31/262 [==>...........................] - ETA: 32:50 - loss: 0.2859 - accuracy: 0.9012 - precision: 0.9297 - recall: 0.9212 - prc: 0.9766\n",
            " 32/262 [==>...........................] - ETA: 32:30 - loss: 0.2851 - accuracy: 0.9014 - precision: 0.9288 - recall: 0.9219 - prc: 0.9768\n",
            " 33/262 [==>...........................] - ETA: 32:09 - loss: 0.2790 - accuracy: 0.9025 - precision: 0.9292 - recall: 0.9225 - prc: 0.9773\n",
            " 34/262 [==>...........................] - ETA: 31:50 - loss: 0.2729 - accuracy: 0.9044 - precision: 0.9311 - recall: 0.9233 - prc: 0.9778\n",
            " 35/262 [===>..........................] - ETA: 31:33 - loss: 0.2670 - accuracy: 0.9062 - precision: 0.9322 - recall: 0.9259 - prc: 0.9787\n",
            " 36/262 [===>..........................] - ETA: 31:16 - loss: 0.2633 - accuracy: 0.9071 - precision: 0.9315 - recall: 0.9278 - prc: 0.9791\n",
            " 37/262 [===>..........................] - ETA: 31:12 - loss: 0.2608 - accuracy: 0.9079 - precision: 0.9318 - recall: 0.9282 - prc: 0.9794\n",
            " 38/262 [===>..........................] - ETA: 31:12 - loss: 0.2615 - accuracy: 0.9071 - precision: 0.9323 - recall: 0.9265 - prc: 0.9795\n",
            " 39/262 [===>..........................] - ETA: 31:16 - loss: 0.2583 - accuracy: 0.9087 - precision: 0.9330 - recall: 0.9285 - prc: 0.9799\n",
            " 40/262 [===>..........................] - ETA: 31:13 - loss: 0.2557 - accuracy: 0.9086 - precision: 0.9348 - recall: 0.9271 - prc: 0.9803\n",
            " 41/262 [===>..........................] - ETA: 31:12 - loss: 0.2540 - accuracy: 0.9078 - precision: 0.9351 - recall: 0.9255 - prc: 0.9805\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            " 42/262 [===>..........................] - ETA: 31:11 - loss: 0.2520 - accuracy: 0.9092 - precision: 0.9367 - recall: 0.9263 - prc: 0.9809\n",
            " 43/262 [===>..........................] - ETA: 30:55 - loss: 0.2486 - accuracy: 0.9106 - precision: 0.9382 - recall: 0.9269 - prc: 0.9813\n",
            " 44/262 [====>.........................] - ETA: 30:40 - loss: 0.2469 - accuracy: 0.9112 - precision: 0.9376 - recall: 0.9286 - prc: 0.9815\n",
            " 45/262 [====>.........................] - ETA: 30:30 - loss: 0.2455 - accuracy: 0.9111 - precision: 0.9369 - recall: 0.9291 - prc: 0.9817\n",
            " 46/262 [====>.........................] - ETA: 30:31 - loss: 0.2433 - accuracy: 0.9124 - precision: 0.9374 - recall: 0.9307 - prc: 0.9819\n",
            " 47/262 [====>.........................] - ETA: 30:23 - loss: 0.2395 - accuracy: 0.9136 - precision: 0.9389 - recall: 0.9315 - prc: 0.9824\n",
            " 48/262 [====>.........................] - ETA: 30:18 - loss: 0.2363 - accuracy: 0.9147 - precision: 0.9394 - recall: 0.9330 - prc: 0.9828\n",
            " 49/262 [====>.........................] - ETA: 30:13 - loss: 0.2394 - accuracy: 0.9120 - precision: 0.9377 - recall: 0.9305 - prc: 0.9825\n",
            " 50/262 [====>.........................] - ETA: 30:03 - loss: 0.2367 - accuracy: 0.9125 - precision: 0.9371 - recall: 0.9318 - prc: 0.9828\n",
            " 51/262 [====>.........................] - ETA: 29:59 - loss: 0.2377 - accuracy: 0.9130 - precision: 0.9365 - recall: 0.9331 - prc: 0.9827\n",
            " 52/262 [====>.........................] - ETA: 29:55 - loss: 0.2404 - accuracy: 0.9111 - precision: 0.9332 - recall: 0.9332 - prc: 0.9825\n",
            " 53/262 [=====>........................] - ETA: 29:48 - loss: 0.2379 - accuracy: 0.9110 - precision: 0.9345 - recall: 0.9320 - prc: 0.9828\n",
            " 54/262 [=====>........................] - ETA: 29:45 - loss: 0.2364 - accuracy: 0.9115 - precision: 0.9349 - recall: 0.9325 - prc: 0.9830\n",
            " 55/262 [=====>........................] - ETA: 29:38 - loss: 0.2410 - accuracy: 0.9102 - precision: 0.9329 - recall: 0.9329 - prc: 0.9826\n",
            " 56/262 [=====>........................] - ETA: 29:34 - loss: 0.2371 - accuracy: 0.9118 - precision: 0.9341 - recall: 0.9341 - prc: 0.9830\n",
            " 57/262 [=====>........................] - ETA: 29:39 - loss: 0.2362 - accuracy: 0.9123 - precision: 0.9345 - recall: 0.9345 - prc: 0.9831\n",
            " 58/262 [=====>........................] - ETA: 29:37 - loss: 0.2415 - accuracy: 0.9116 - precision: 0.9340 - recall: 0.9340 - prc: 0.9829\n",
            " 59/262 [=====>........................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 29:21 - loss: 0.2386 - accuracy: 0.9126 - precision: 0.9352 - recall: 0.9344 - prc: 0.9832\n",
            " 60/262 [=====>........................] - ETA: 29:06 - loss: 0.2351 - accuracy: 0.9141 - precision: 0.9362 - recall: 0.9355 - prc: 0.9836\n",
            " 61/262 [=====>........................] - ETA: 28:52 - loss: 0.2346 - accuracy: 0.9144 - precision: 0.9365 - recall: 0.9358 - prc: 0.9837\n",
            " 62/262 [======>.......................] - ETA: 28:38 - loss: 0.2350 - accuracy: 0.9138 - precision: 0.9345 - recall: 0.9367 - prc: 0.9835\n",
            " 63/262 [======>.......................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 28:23 - loss: 0.2325 - accuracy: 0.9147 - precision: 0.9355 - recall: 0.9368 - prc: 0.9837\n",
            " 64/262 [======>.......................] - ETA: 28:10 - loss: 0.2311 - accuracy: 0.9150 - precision: 0.9364 - recall: 0.9364 - prc: 0.9839\n",
            " 65/262 [======>.......................] - ETA: 27:55 - loss: 0.2301 - accuracy: 0.9154 - precision: 0.9368 - recall: 0.9368 - prc: 0.9841\n",
            " 66/262 [======>.......................] - ETA: 27:42 - loss: 0.2271 - accuracy: 0.9167 - precision: 0.9377 - recall: 0.9377 - prc: 0.9844\n",
            " 67/262 [======>.......................] - ETA: 27:28 - loss: 0.2296 - accuracy: 0.9160 - precision: 0.9373 - recall: 0.9373 - prc: 0.9835\n",
            " 68/262 [======>.......................] - ETA: 27:14 - loss: 0.2297 - accuracy: 0.9168 - precision: 0.9383 - recall: 0.9376 - prc: 0.9836\n",
            " 69/262 [======>.......................] - ETA: 27:02 - loss: 0.2282 - accuracy: 0.9176 - precision: 0.9384 - recall: 0.9384 - prc: 0.9836\n",
            " 70/262 [=======>......................] - ETA: 26:49 - loss: 0.2271 - accuracy: 0.9174 - precision: 0.9393 - recall: 0.9374 - prc: 0.9838\n",
            " 71/262 [=======>......................] - ETA: 26:36 - loss: 0.2272 - accuracy: 0.9177 - precision: 0.9395 - recall: 0.9376 - prc: 0.9839\n",
            " 72/262 [=======>......................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 26:23 - loss: 0.2273 - accuracy: 0.9175 - precision: 0.9383 - recall: 0.9383 - prc: 0.9838\n",
            " 73/262 [=======>......................] - ETA: 26:11 - loss: 0.2281 - accuracy: 0.9170 - precision: 0.9385 - recall: 0.9373 - prc: 0.9838\n",
            " 74/262 [=======>......................] - ETA: 25:59 - loss: 0.2290 - accuracy: 0.9168 - precision: 0.9374 - recall: 0.9380 - prc: 0.9837\n",
            " 75/262 [=======>......................] - ETA: 25:47 - loss: 0.2306 - accuracy: 0.9154 - precision: 0.9364 - recall: 0.9370 - prc: 0.9836\n",
            " 76/262 [=======>......................] - ETA: 25:35 - loss: 0.2308 - accuracy: 0.9153 - precision: 0.9359 - recall: 0.9371 - prc: 0.9836\n",
            " 77/262 [=======>......................] - ETA: 25:23 - loss: 0.2283 - accuracy: 0.9164 - precision: 0.9367 - recall: 0.9378 - prc: 0.9838\n",
            " 78/262 [=======>......................] - ETA: 25:12 - loss: 0.2288 - accuracy: 0.9159 - precision: 0.9362 - recall: 0.9373 - prc: 0.9837\n",
            " 79/262 [========>.....................] - ETA: 25:00 - loss: 0.2285 - accuracy: 0.9157 - precision: 0.9364 - recall: 0.9369 - prc: 0.9837\n",
            " 80/262 [========>.....................] - ETA: 24:49 - loss: 0.2274 - accuracy: 0.9160 - precision: 0.9360 - recall: 0.9377 - prc: 0.9839\n",
            " 81/262 [========>.....................] - ETA: 24:38 - loss: 0.2249 - accuracy: 0.9171 - precision: 0.9368 - recall: 0.9385 - prc: 0.9842\n",
            " 82/262 [========>.....................] - ETA: 24:26 - loss: 0.2225 - accuracy: 0.9181 - precision: 0.9377 - recall: 0.9393 - prc: 0.9844\n",
            " 83/262 [========>.....................] - ETA: 24:15 - loss: 0.2227 - accuracy: 0.9179 - precision: 0.9385 - recall: 0.9385 - prc: 0.9846\n",
            " 84/262 [========>.....................] - ETA: 24:04 - loss: 0.2204 - accuracy: 0.9189 - precision: 0.9393 - recall: 0.9393 - prc: 0.9849\n",
            " 85/262 [========>.....................] - ETA: 23:53 - loss: 0.2221 - accuracy: 0.9184 - precision: 0.9389 - recall: 0.9389 - prc: 0.9848\n",
            " 86/262 [========>.....................] - ETA: 23:42 - loss: 0.2246 - accuracy: 0.9179 - precision: 0.9392 - recall: 0.9381 - prc: 0.9841\n",
            " 87/262 [========>.....................] - ETA: 23:31 - loss: 0.2227 - accuracy: 0.9185 - precision: 0.9394 - recall: 0.9389 - prc: 0.9843\n",
            " 88/262 [=========>....................] - ETA: 23:20 - loss: 0.2211 - accuracy: 0.9190 - precision: 0.9403 - recall: 0.9393 - prc: 0.9845\n",
            " 89/262 [=========>....................] - ETA: 23:10 - loss: 0.2205 - accuracy: 0.9185 - precision: 0.9395 - recall: 0.9395 - prc: 0.9846\n",
            " 90/262 [=========>....................] - ETA: 22:59 - loss: 0.2190 - accuracy: 0.9191 - precision: 0.9398 - recall: 0.9402 - prc: 0.9848\n",
            " 91/262 [=========>....................] - ETA: 22:48 - loss: 0.2215 - accuracy: 0.9186 - precision: 0.9395 - recall: 0.9399 - prc: 0.9847\n",
            " 92/262 [=========>....................] - ETA: 22:38 - loss: 0.2198 - accuracy: 0.9192 - precision: 0.9396 - recall: 0.9406 - prc: 0.9848\n",
            " 93/262 [=========>....................] - ETA: 22:28 - loss: 0.2178 - accuracy: 0.9200 - precision: 0.9403 - recall: 0.9412 - prc: 0.9850\n",
            " 94/262 [=========>....................] - ETA: 22:17 - loss: 0.2171 - accuracy: 0.9205 - precision: 0.9410 - recall: 0.9415 - prc: 0.9851\n",
            " 95/262 [=========>....................] - ETA: 22:07 - loss: 0.2164 - accuracy: 0.9204 - precision: 0.9415 - recall: 0.9406 - prc: 0.9852\n",
            " 96/262 [=========>....................] - ETA: 21:57 - loss: 0.2176 - accuracy: 0.9202 - precision: 0.9416 - recall: 0.9402 - prc: 0.9851\n",
            " 97/262 [==========>...................] - ETA: 21:46 - loss: 0.2175 - accuracy: 0.9201 - precision: 0.9411 - recall: 0.9402 - prc: 0.9852\n",
            " 98/262 [==========>...................] - ETA: 21:36 - loss: 0.2165 - accuracy: 0.9203 - precision: 0.9411 - recall: 0.9402 - prc: 0.9852\n",
            " 99/262 [==========>...................] - ETA: 21:26 - loss: 0.2158 - accuracy: 0.9201 - precision: 0.9413 - recall: 0.9399 - prc: 0.9853\n",
            "100/262 [==========>...................] - ETA: 21:16 - loss: 0.2153 - accuracy: 0.9203 - precision: 0.9413 - recall: 0.9400 - prc: 0.9854\n",
            "101/262 [==========>...................] - ETA: 21:06 - loss: 0.2157 - accuracy: 0.9199 - precision: 0.9401 - recall: 0.9405 - prc: 0.9853\n",
            "102/262 [==========>...................] - ETA: 20:56 - loss: 0.2139 - accuracy: 0.9206 - precision: 0.9406 - recall: 0.9411 - prc: 0.9855\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 103/262 [==========>...................] - ETA: 20:46 - loss: 0.2146 - accuracy: 0.9208 - precision: 0.9407 - recall: 0.9411 - prc: 0.9849\n",
            "104/262 [==========>...................] - ETA: 20:37 - loss: 0.2129 - accuracy: 0.9216 - precision: 0.9414 - recall: 0.9418 - prc: 0.9851\n",
            "105/262 [===========>..................] - ETA: 20:27 - loss: 0.2152 - accuracy: 0.9214 - precision: 0.9416 - recall: 0.9416 - prc: 0.9851\n",
            "106/262 [===========>..................] - ETA: 20:17 - loss: 0.2141 - accuracy: 0.9222 - precision: 0.9422 - recall: 0.9422 - prc: 0.9852\n",
            "107/262 [===========>..................] - ETA: 20:08 - loss: 0.2140 - accuracy: 0.9223 - precision: 0.9420 - recall: 0.9428 - prc: 0.9852\n",
            "108/262 [===========>..................] - ETA: 19:59 - loss: 0.2127 - accuracy: 0.9230 - precision: 0.9426 - recall: 0.9434 - prc: 0.9854\n",
            "109/262 [===========>..................] - ETA: 19:49 - loss: 0.2118 - accuracy: 0.9232 - precision: 0.9423 - recall: 0.9439 - prc: 0.9855\n",
            "110/262 [===========>..................] - ETA: 19:40 - loss: 0.2103 - accuracy: 0.9236 - precision: 0.9423 - recall: 0.9443 - prc: 0.9856\n",
            "111/262 [===========>..................] - ETA: 19:31 - loss: 0.2119 - accuracy: 0.9234 - precision: 0.9424 - recall: 0.9440 - prc: 0.9856\n",
            "112/262 [===========>..................] - ETA: 19:22 - loss: 0.2116 - accuracy: 0.9235 - precision: 0.9430 - recall: 0.9438 - prc: 0.9857\n",
            "113/262 [===========>..................] - ETA: 19:12 - loss: 0.2119 - accuracy: 0.9231 - precision: 0.9427 - recall: 0.9435 - prc: 0.9858\n",
            "114/262 [============>.................] - ETA: 19:03 - loss: 0.2109 - accuracy: 0.9235 - precision: 0.9433 - recall: 0.9437 - prc: 0.9859\n",
            "115/262 [============>.................] - ETA: 18:54 - loss: 0.2097 - accuracy: 0.9239 - precision: 0.9434 - recall: 0.9442 - prc: 0.9861\n",
            "116/262 [============>.................] - ETA: 18:45 - loss: 0.2093 - accuracy: 0.9243 - precision: 0.9439 - recall: 0.9443 - prc: 0.9861\n",
            "117/262 [============>.................] - ETA: 18:36 - loss: 0.2120 - accuracy: 0.9233 - precision: 0.9436 - recall: 0.9432 - prc: 0.9855\n",
            "118/262 [============>.................] - ETA: 18:27 - loss: 0.2104 - accuracy: 0.9240 - precision: 0.9441 - recall: 0.9438 - prc: 0.9857\n",
            "119/262 [============>.................] - ETA: 18:18 - loss: 0.2097 - accuracy: 0.9244 - precision: 0.9443 - recall: 0.9443 - prc: 0.9859\n",
            "120/262 [============>.................] - ETA: 18:09 - loss: 0.2094 - accuracy: 0.9240 - precision: 0.9443 - recall: 0.9436 - prc: 0.9859\n",
            "121/262 [============>.................] - ETA: 18:00 - loss: 0.2143 - accuracy: 0.9230 - precision: 0.9433 - recall: 0.9433 - prc: 0.9853\n",
            "122/262 [============>.................] - ETA: 17:51 - loss: 0.2141 - accuracy: 0.9234 - precision: 0.9434 - recall: 0.9438 - prc: 0.9853\n",
            "123/262 [=============>................] - ETA: 17:42 - loss: 0.2125 - accuracy: 0.9240 - precision: 0.9439 - recall: 0.9443 - prc: 0.9855\n",
            "124/262 [=============>................] - ETA: 17:34 - loss: 0.2128 - accuracy: 0.9239 - precision: 0.9437 - recall: 0.9444 - prc: 0.9855\n",
            "125/262 [=============>................] - ETA: 17:25 - loss: 0.2140 - accuracy: 0.9240 - precision: 0.9438 - recall: 0.9444 - prc: 0.9851\n",
            "126/262 [=============>................] - ETA: 17:16 - loss: 0.2139 - accuracy: 0.9236 - precision: 0.9431 - recall: 0.9445 - prc: 0.9851\n",
            "127/262 [=============>................] - ETA: 17:14 - loss: 0.2126 - accuracy: 0.9240 - precision: 0.9436 - recall: 0.9446 - prc: 0.9853\n",
            "128/262 [=============>................] - ETA: 17:11 - loss: 0.2145 - accuracy: 0.9241 - precision: 0.9436 - recall: 0.9446 - prc: 0.9848\n",
            "129/262 [=============>................] - ETA: 17:09 - loss: 0.2148 - accuracy: 0.9242 - precision: 0.9436 - recall: 0.9446 - prc: 0.9848\n",
            "130/262 [=============>................] - ETA: 17:06 - loss: 0.2134 - accuracy: 0.9248 - precision: 0.9441 - recall: 0.9451 - prc: 0.9850\n",
            "131/262 [==============>...............] - ETA: 16:56 - loss: 0.2143 - accuracy: 0.9245 - precision: 0.9436 - recall: 0.9452 - prc: 0.9849\n",
            "132/262 [==============>...............] - ETA: 16:54 - loss: 0.2150 - accuracy: 0.9246 - precision: 0.9436 - recall: 0.9452 - prc: 0.9848\n",
            "133/262 [==============>...............] - ETA: 16:45 - loss: 0.2165 - accuracy: 0.9247 - precision: 0.9437 - recall: 0.9454 - prc: 0.9847\n",
            "134/262 [==============>...............] - ETA: 16:37 - loss: 0.2151 - accuracy: 0.9253 - precision: 0.9441 - recall: 0.9458 - prc: 0.9849\n",
            "135/262 [==============>...............] - ETA: 16:28 - loss: 0.2137 - accuracy: 0.9258 - precision: 0.9446 - recall: 0.9462 - prc: 0.9850\n",
            "136/262 [==============>...............] - ETA: 16:19 - loss: 0.2128 - accuracy: 0.9261 - precision: 0.9446 - recall: 0.9465 - prc: 0.9851\n",
            "137/262 [==============>...............] - ETA: 16:10 - loss: 0.2136 - accuracy: 0.9262 - precision: 0.9449 - recall: 0.9462 - prc: 0.9851\n",
            "138/262 [==============>...............] - ETA: 16:02 - loss: 0.2137 - accuracy: 0.9263 - precision: 0.9452 - recall: 0.9458 - prc: 0.9851\n",
            "139/262 [==============>...............] - ETA: 15:53 - loss: 0.2157 - accuracy: 0.9257 - precision: 0.9453 - recall: 0.9450 - prc: 0.9850\n",
            "140/262 [===============>..............] - ETA: 15:45 - loss: 0.2142 - accuracy: 0.9262 - precision: 0.9457 - recall: 0.9454 - prc: 0.9852\n",
            "141/262 [===============>..............] - ETA: 15:36 - loss: 0.2135 - accuracy: 0.9265 - precision: 0.9457 - recall: 0.9457 - prc: 0.9852\n",
            "142/262 [===============>..............] - ETA: 15:27 - loss: 0.2133 - accuracy: 0.9264 - precision: 0.9455 - recall: 0.9458 - prc: 0.9853\n",
            "143/262 [===============>..............] - ETA: 15:19 - loss: 0.2145 - accuracy: 0.9265 - precision: 0.9455 - recall: 0.9458 - prc: 0.9852\n",
            "144/262 [===============>..............] - ETA: 15:10 - loss: 0.2148 - accuracy: 0.9263 - precision: 0.9455 - recall: 0.9455 - prc: 0.9853\n",
            "145/262 [===============>..............] - ETA: 15:02 - loss: 0.2141 - accuracy: 0.9262 - precision: 0.9449 - recall: 0.9458 - prc: 0.9853\n",
            "146/262 [===============>..............] - ETA: 14:53 - loss: 0.2138 - accuracy: 0.9258 - precision: 0.9443 - recall: 0.9458 - prc: 0.9853\n",
            "147/262 [===============>..............] - ETA: 14:47 - loss: 0.2129 - accuracy: 0.9261 - precision: 0.9444 - recall: 0.9462 - prc: 0.9854\n",
            "148/262 [===============>..............] - ETA: 14:41 - loss: 0.2157 - accuracy: 0.9254 - precision: 0.9444 - recall: 0.9450 - prc: 0.9853\n",
            "149/262 [================>.............] - ETA: 14:33 - loss: 0.2158 - accuracy: 0.9254 - precision: 0.9448 - recall: 0.9448 - prc: 0.9853\n",
            "150/262 [================>.............] - ETA: 14:24 - loss: 0.2151 - accuracy: 0.9257 - precision: 0.9449 - recall: 0.9452 - prc: 0.9854\n",
            "151/262 [================>.............] - ETA: 14:16 - loss: 0.2150 - accuracy: 0.9260 - precision: 0.9453 - recall: 0.9453 - prc: 0.9854\n",
            "152/262 [================>.............] - ETA: 14:07 - loss: 0.2155 - accuracy: 0.9259 - precision: 0.9447 - recall: 0.9455 - prc: 0.9854\n",
            "153/262 [================>.............] - ETA: 13:59 - loss: 0.2171 - accuracy: 0.9260 - precision: 0.9447 - recall: 0.9456 - prc: 0.9854\n",
            "154/262 [================>.............] - ETA: 13:51 - loss: 0.2193 - accuracy: 0.9254 - precision: 0.9445 - recall: 0.9451 - prc: 0.9849\n",
            "155/262 [================>.............] - ETA: 13:44 - loss: 0.2191 - accuracy: 0.9255 - precision: 0.9446 - recall: 0.9451 - prc: 0.9849\n",
            "156/262 [================>.............] - ETA: 13:38 - loss: 0.2189 - accuracy: 0.9256 - precision: 0.9446 - recall: 0.9452 - prc: 0.9850\n",
            "157/262 [================>.............] - ETA: 13:30 - loss: 0.2184 - accuracy: 0.9257 - precision: 0.9446 - recall: 0.9452 - prc: 0.9850\n",
            "158/262 [=================>............] - ETA: 13:22 - loss: 0.2170 - accuracy: 0.9261 - precision: 0.9450 - recall: 0.9456 - prc: 0.9852\n",
            "159/262 [=================>............] - ETA: 13:14 - loss: 0.2162 - accuracy: 0.9264 - precision: 0.9453 - recall: 0.9456 - prc: 0.9852\n",
            "160/262 [=================>............] - ETA: 13:05 - loss: 0.2152 - accuracy: 0.9269 - precision: 0.9456 - recall: 0.9459 - prc: 0.9853\n",
            "161/262 [=================>............] - ETA: 12:57 - loss: 0.2150 - accuracy: 0.9267 - precision: 0.9454 - recall: 0.9459 - prc: 0.9853\n",
            "162/262 [=================>............] - ETA: 12:50 - loss: 0.2153 - accuracy: 0.9264 - precision: 0.9454 - recall: 0.9454 - prc: 0.9853\n",
            "163/262 [=================>............] - ETA: 12:44 - loss: 0.2182 - accuracy: 0.9257 - precision: 0.9443 - recall: 0.9454 - prc: 0.9848\n",
            "164/262 [=================>............] - ETA: 12:36 - loss: 0.2172 - accuracy: 0.9260 - precision: 0.9447 - recall: 0.9455 - prc: 0.9849\n",
            "165/262 [=================>............] - ETA: 12:28 - loss: 0.2173 - accuracy: 0.9259 - precision: 0.9447 - recall: 0.9453 - prc: 0.9849\n",
            "166/262 [==================>...........] - ETA: 12:19 - loss: 0.2179 - accuracy: 0.9257 - precision: 0.9451 - recall: 0.9448 - prc: 0.9850\n",
            "167/262 [==================>...........] - ETA: 12:11 - loss: 0.2183 - accuracy: 0.9256 - precision: 0.9449 - recall: 0.9449 - prc: 0.9850\n",
            "168/262 [==================>...........] - ETA: 12:03 - loss: 0.2185 - accuracy: 0.9253 - precision: 0.9447 - recall: 0.9447 - prc: 0.9850\n",
            "169/262 [==================>...........] - ETA: 11:55 - loss: 0.2189 - accuracy: 0.9252 - precision: 0.9451 - recall: 0.9443 - prc: 0.9850\n",
            "170/262 [==================>...........] - ETA: 11:46 - loss: 0.2188 - accuracy: 0.9251 - precision: 0.9451 - recall: 0.9441 - prc: 0.9851\n",
            "171/262 [==================>...........] - ETA: 11:38 - loss: 0.2186 - accuracy: 0.9252 - precision: 0.9455 - recall: 0.9440 - prc: 0.9851\n",
            "172/262 [==================>...........] - ETA: 11:30 - loss: 0.2201 - accuracy: 0.9251 - precision: 0.9453 - recall: 0.9441 - prc: 0.9850\n",
            "173/262 [==================>...........] - ETA: 11:22 - loss: 0.2208 - accuracy: 0.9248 - precision: 0.9456 - recall: 0.9433 - prc: 0.9850\n",
            "174/262 [==================>...........] - ETA: 11:14 - loss: 0.2208 - accuracy: 0.9248 - precision: 0.9454 - recall: 0.9436 - prc: 0.9850\n",
            "175/262 [===================>..........] - ETA: 11:06 - loss: 0.2203 - accuracy: 0.9247 - precision: 0.9457 - recall: 0.9432 - prc: 0.9851\n",
            "176/262 [===================>..........] - ETA: 10:58 - loss: 0.2219 - accuracy: 0.9243 - precision: 0.9450 - recall: 0.9432 - prc: 0.9850\n",
            "177/262 [===================>..........] - ETA: 10:50 - loss: 0.2209 - accuracy: 0.9247 - precision: 0.9453 - recall: 0.9436 - prc: 0.9851\n",
            "178/262 [===================>..........] - ETA: 10:44 - loss: 0.2206 - accuracy: 0.9248 - precision: 0.9451 - recall: 0.9438 - prc: 0.9851\n",
            "179/262 [===================>..........] - ETA: 10:36 - loss: 0.2203 - accuracy: 0.9247 - precision: 0.9452 - recall: 0.9437 - prc: 0.9852\n",
            "180/262 [===================>..........] - ETA: 10:28 - loss: 0.2200 - accuracy: 0.9246 - precision: 0.9450 - recall: 0.9438 - prc: 0.9852\n",
            "181/262 [===================>..........] - ETA: 10:20 - loss: 0.2199 - accuracy: 0.9248 - precision: 0.9453 - recall: 0.9439 - prc: 0.9852\n",
            "182/262 [===================>..........] - ETA: 10:12 - loss: 0.2190 - accuracy: 0.9252 - precision: 0.9456 - recall: 0.9442 - prc: 0.9853\n",
            "183/262 [===================>..........] - ETA: 10:04 - loss: 0.2183 - accuracy: 0.9255 - precision: 0.9459 - recall: 0.9442 - prc: 0.9854\n",
            "184/262 [====================>.........] - ETA: 9:56 - loss: 0.2198 - accuracy: 0.9248 - precision: 0.9455 - recall: 0.9438 - prc: 0.9850 \n",
            "185/262 [====================>.........] - ETA: 9:48 - loss: 0.2191 - accuracy: 0.9247 - precision: 0.9455 - recall: 0.9436 - prc: 0.9851\n",
            "186/262 [====================>.........] - ETA: 9:40 - loss: 0.2185 - accuracy: 0.9248 - precision: 0.9456 - recall: 0.9437 - prc: 0.9852\n",
            "187/262 [====================>.........] - ETA: 9:32 - loss: 0.2196 - accuracy: 0.9247 - precision: 0.9451 - recall: 0.9440 - prc: 0.9848\n",
            "188/262 [====================>.........] - ETA: 9:24 - loss: 0.2190 - accuracy: 0.9246 - precision: 0.9447 - recall: 0.9442 - prc: 0.9849\n",
            "189/262 [====================>.........] - ETA: 9:16 - loss: 0.2189 - accuracy: 0.9247 - precision: 0.9448 - recall: 0.9443 - prc: 0.9849\n",
            "190/262 [====================>.........] - ETA: 9:08 - loss: 0.2203 - accuracy: 0.9246 - precision: 0.9445 - recall: 0.9443 - prc: 0.9845\n",
            "191/262 [====================>.........] - ETA: 9:00 - loss: 0.2207 - accuracy: 0.9243 - precision: 0.9441 - recall: 0.9443 - prc: 0.9845\n",
            "192/262 [====================>.........] - ETA: 8:52 - loss: 0.2204 - accuracy: 0.9244 - precision: 0.9442 - recall: 0.9444 - prc: 0.9846\n",
            "193/262 [=====================>........] - ETA: 8:44 - loss: 0.2199 - accuracy: 0.9245 - precision: 0.9445 - recall: 0.9443 - prc: 0.9846\n",
            "194/262 [=====================>........] - ETA: 8:36 - loss: 0.2199 - accuracy: 0.9245 - precision: 0.9446 - recall: 0.9444 - prc: 0.9847\n",
            "195/262 [=====================>........] - ETA: 8:28 - loss: 0.2190 - accuracy: 0.9248 - precision: 0.9446 - recall: 0.9446 - prc: 0.9848\n",
            "196/262 [=====================>........] - ETA: 8:21 - loss: 0.2190 - accuracy: 0.9245 - precision: 0.9440 - recall: 0.9449 - prc: 0.9848\n",
            "197/262 [=====================>........] - ETA: 8:13 - loss: 0.2182 - accuracy: 0.9246 - precision: 0.9441 - recall: 0.9449 - prc: 0.9849\n",
            "198/262 [=====================>........] - ETA: 8:05 - loss: 0.2173 - accuracy: 0.9250 - precision: 0.9443 - recall: 0.9452 - prc: 0.9850\n",
            "199/262 [=====================>........] - ETA: 7:57 - loss: 0.2174 - accuracy: 0.9250 - precision: 0.9441 - recall: 0.9454 - prc: 0.9849\n",
            "200/262 [=====================>........] - ETA: 7:49 - loss: 0.2164 - accuracy: 0.9254 - precision: 0.9444 - recall: 0.9457 - prc: 0.9851\n",
            "201/262 [======================>.......]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 7:41 - loss: 0.2156 - accuracy: 0.9258 - precision: 0.9447 - recall: 0.9460 - prc: 0.9852\n",
            "202/262 [======================>.......] - ETA: 7:34 - loss: 0.2164 - accuracy: 0.9255 - precision: 0.9450 - recall: 0.9454 - prc: 0.9851\n",
            "203/262 [======================>.......] - ETA: 7:26 - loss: 0.2165 - accuracy: 0.9254 - precision: 0.9445 - recall: 0.9456 - prc: 0.9851\n",
            "204/262 [======================>.......] - ETA: 7:18 - loss: 0.2162 - accuracy: 0.9253 - precision: 0.9444 - recall: 0.9456 - prc: 0.9851\n",
            "205/262 [======================>.......] - ETA: 7:10 - loss: 0.2161 - accuracy: 0.9252 - precision: 0.9439 - recall: 0.9458 - prc: 0.9851\n",
            "206/262 [======================>.......] - ETA: 7:02 - loss: 0.2163 - accuracy: 0.9251 - precision: 0.9435 - recall: 0.9461 - prc: 0.9851\n",
            "207/262 [======================>.......] - ETA: 6:55 - loss: 0.2157 - accuracy: 0.9252 - precision: 0.9434 - recall: 0.9463 - prc: 0.9852\n",
            "208/262 [======================>.......] - ETA: 6:47 - loss: 0.2156 - accuracy: 0.9254 - precision: 0.9436 - recall: 0.9464 - prc: 0.9852\n",
            "209/262 [======================>.......] - ETA: 6:39 - loss: 0.2171 - accuracy: 0.9249 - precision: 0.9436 - recall: 0.9455 - prc: 0.9849\n",
            "210/262 [=======================>......] - ETA: 6:31 - loss: 0.2168 - accuracy: 0.9248 - precision: 0.9439 - recall: 0.9451 - prc: 0.9849\n",
            "211/262 [=======================>......] - ETA: 6:24 - loss: 0.2163 - accuracy: 0.9248 - precision: 0.9442 - recall: 0.9450 - prc: 0.9850\n",
            "212/262 [=======================>......] - ETA: 6:16 - loss: 0.2160 - accuracy: 0.9250 - precision: 0.9442 - recall: 0.9452 - prc: 0.9850\n",
            "213/262 [=======================>......] - ETA: 6:08 - loss: 0.2152 - accuracy: 0.9254 - precision: 0.9445 - recall: 0.9455 - prc: 0.9851\n",
            "214/262 [=======================>......] - ETA: 6:01 - loss: 0.2147 - accuracy: 0.9255 - precision: 0.9448 - recall: 0.9454 - prc: 0.9852\n",
            "215/262 [=======================>......] - ETA: 5:53 - loss: 0.2150 - accuracy: 0.9252 - precision: 0.9446 - recall: 0.9452 - prc: 0.9852\n",
            "216/262 [=======================>......] - ETA: 5:45 - loss: 0.2148 - accuracy: 0.9253 - precision: 0.9446 - recall: 0.9452 - prc: 0.9852\n",
            "217/262 [=======================>......] - ETA: 5:37 - loss: 0.2154 - accuracy: 0.9250 - precision: 0.9442 - recall: 0.9452 - prc: 0.9851\n",
            "218/262 [=======================>......] - ETA: 5:30 - loss: 0.2156 - accuracy: 0.9250 - precision: 0.9440 - recall: 0.9452 - prc: 0.9851\n",
            "219/262 [========================>.....] - ETA: 5:22 - loss: 0.2157 - accuracy: 0.9247 - precision: 0.9441 - recall: 0.9449 - prc: 0.9851\n",
            "220/262 [========================>.....] - ETA: 5:14 - loss: 0.2149 - accuracy: 0.9251 - precision: 0.9443 - recall: 0.9451 - prc: 0.9852\n",
            "221/262 [========================>.....] - ETA: 5:07 - loss: 0.2143 - accuracy: 0.9251 - precision: 0.9443 - recall: 0.9451 - prc: 0.9853\n",
            "222/262 [========================>.....] - ETA: 4:59 - loss: 0.2139 - accuracy: 0.9253 - precision: 0.9446 - recall: 0.9452 - prc: 0.9853\n",
            "223/262 [========================>.....] - ETA: 4:51 - loss: 0.2133 - accuracy: 0.9255 - precision: 0.9446 - recall: 0.9454 - prc: 0.9854\n",
            "224/262 [========================>.....] - ETA: 4:44 - loss: 0.2125 - accuracy: 0.9257 - precision: 0.9449 - recall: 0.9454 - prc: 0.9855\n",
            "225/262 [========================>.....] - ETA: 4:36 - loss: 0.2128 - accuracy: 0.9255 - precision: 0.9449 - recall: 0.9451 - prc: 0.9855\n",
            "226/262 [========================>.....] - ETA: 4:29 - loss: 0.2142 - accuracy: 0.9254 - precision: 0.9445 - recall: 0.9453 - prc: 0.9852\n",
            "227/262 [========================>.....] - ETA: 4:21 - loss: 0.2144 - accuracy: 0.9256 - precision: 0.9445 - recall: 0.9455 - prc: 0.9850\n",
            "228/262 [=========================>....] - ETA: 4:13 - loss: 0.2142 - accuracy: 0.9255 - precision: 0.9448 - recall: 0.9452 - prc: 0.9850\n",
            "229/262 [=========================>....] - ETA: 4:06 - loss: 0.2151 - accuracy: 0.9253 - precision: 0.9445 - recall: 0.9452 - prc: 0.9849\n",
            "230/262 [=========================>....] - ETA: 3:58 - loss: 0.2147 - accuracy: 0.9253 - precision: 0.9445 - recall: 0.9453 - prc: 0.9850\n",
            "231/262 [=========================>....] - ETA: 3:51 - loss: 0.2145 - accuracy: 0.9254 - precision: 0.9445 - recall: 0.9453 - prc: 0.9850\n",
            "232/262 [=========================>....] - ETA: 3:43 - loss: 0.2145 - accuracy: 0.9254 - precision: 0.9446 - recall: 0.9453 - prc: 0.9850\n",
            "233/262 [=========================>....] - ETA: 3:36 - loss: 0.2141 - accuracy: 0.9255 - precision: 0.9446 - recall: 0.9454 - prc: 0.9851\n",
            "234/262 [=========================>....] - ETA: 3:28 - loss: 0.2138 - accuracy: 0.9255 - precision: 0.9448 - recall: 0.9452 - prc: 0.9851\n",
            "235/262 [=========================>....] - ETA: 3:21 - loss: 0.2142 - accuracy: 0.9256 - precision: 0.9449 - recall: 0.9453 - prc: 0.9849\n",
            "236/262 [==========================>...] - ETA: 3:13 - loss: 0.2141 - accuracy: 0.9257 - precision: 0.9450 - recall: 0.9453 - prc: 0.9849\n",
            "237/262 [==========================>...] - ETA: 3:06 - loss: 0.2134 - accuracy: 0.9258 - precision: 0.9452 - recall: 0.9453 - prc: 0.9850\n",
            "238/262 [==========================>...] - ETA: 2:58 - loss: 0.2136 - accuracy: 0.9256 - precision: 0.9452 - recall: 0.9450 - prc: 0.9850\n",
            "239/262 [==========================>...] - ETA: 2:51 - loss: 0.2140 - accuracy: 0.9255 - precision: 0.9452 - recall: 0.9448 - prc: 0.9850\n",
            "240/262 [==========================>...] - ETA: 2:43 - loss: 0.2137 - accuracy: 0.9256 - precision: 0.9450 - recall: 0.9450 - prc: 0.9850\n",
            "241/262 [==========================>...] - ETA: 2:36 - loss: 0.2135 - accuracy: 0.9258 - precision: 0.9451 - recall: 0.9453 - prc: 0.9851\n",
            "242/262 [==========================>...] - ETA: 2:28 - loss: 0.2131 - accuracy: 0.9258 - precision: 0.9452 - recall: 0.9454 - prc: 0.9851\n",
            "243/262 [==========================>...] - ETA: 2:21 - loss: 0.2135 - accuracy: 0.9257 - precision: 0.9452 - recall: 0.9452 - prc: 0.9851\n",
            "244/262 [==========================>...] - ETA: 2:13 - loss: 0.2127 - accuracy: 0.9260 - precision: 0.9455 - recall: 0.9455 - prc: 0.9852\n",
            "245/262 [===========================>..] - ETA: 2:06 - loss: 0.2129 - accuracy: 0.9261 - precision: 0.9455 - recall: 0.9455 - prc: 0.9852\n",
            "246/262 [===========================>..] - ETA: 1:58 - loss: 0.2123 - accuracy: 0.9263 - precision: 0.9457 - recall: 0.9456 - prc: 0.9853\n",
            "247/262 [===========================>..] - ETA: 1:51 - loss: 0.2118 - accuracy: 0.9263 - precision: 0.9458 - recall: 0.9456 - prc: 0.9853\n",
            "248/262 [===========================>..] - ETA: 1:43 - loss: 0.2119 - accuracy: 0.9264 - precision: 0.9460 - recall: 0.9455 - prc: 0.9854\n",
            "249/262 [===========================>..] - ETA: 1:36 - loss: 0.2129 - accuracy: 0.9261 - precision: 0.9458 - recall: 0.9453 - prc: 0.9853\n",
            "250/262 [===========================>..] - ETA: 1:28 - loss: 0.2133 - accuracy: 0.9259 - precision: 0.9455 - recall: 0.9454 - prc: 0.9853\n",
            "251/262 [===========================>..] - ETA: 1:21 - loss: 0.2131 - accuracy: 0.9260 - precision: 0.9456 - recall: 0.9454 - prc: 0.9853\n",
            "252/262 [===========================>..] - ETA: 1:13 - loss: 0.2134 - accuracy: 0.9259 - precision: 0.9456 - recall: 0.9453 - prc: 0.9853\n",
            "253/262 [===========================>..] - ETA: 1:06 - loss: 0.2126 - accuracy: 0.9262 - precision: 0.9458 - recall: 0.9455 - prc: 0.9854\n",
            "254/262 [============================>.] - ETA: 59s - loss: 0.2129 - accuracy: 0.9261 - precision: 0.9457 - recall: 0.9455 - prc: 0.9854 \n",
            "255/262 [============================>.] - ETA: 51s - loss: 0.2124 - accuracy: 0.9262 - precision: 0.9457 - recall: 0.9455 - prc: 0.9854\n",
            "256/262 [============================>.] - ETA: 44s - loss: 0.2119 - accuracy: 0.9263 - precision: 0.9457 - recall: 0.9457 - prc: 0.9855\n",
            "257/262 [============================>.] - ETA: 36s - loss: 0.2116 - accuracy: 0.9265 - precision: 0.9458 - recall: 0.9459 - prc: 0.9855\n",
            "258/262 [============================>.] - ETA: 29s - loss: 0.2121 - accuracy: 0.9265 - precision: 0.9456 - recall: 0.9462 - prc: 0.9853\n",
            "259/262 [============================>.] - ETA: 22s - loss: 0.2120 - accuracy: 0.9265 - precision: 0.9455 - recall: 0.9462 - prc: 0.9853\n",
            "260/262 [============================>.] - ETA: 14s - loss: 0.2117 - accuracy: 0.9264 - precision: 0.9454 - recall: 0.9462 - prc: 0.9853\n",
            "261/262 [============================>.] - ETA: 7s - loss: 0.2113 - accuracy: 0.9265 - precision: 0.9454 - recall: 0.9464 - prc: 0.9854 \n",
            "262/262 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9267 - precision: 0.9455 - recall: 0.9465 - prc: 0.9854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 15:04:30.075927: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r262/262 [==============================] - 2239s 8s/step - loss: 0.2110 - accuracy: 0.9267 - precision: 0.9455 - recall: 0.9465 - prc: 0.9854 - val_loss: 0.1045 - val_accuracy: 0.9617 - val_precision: 1.0000 - val_recall: 0.9617 - val_prc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 15:08:41.536973: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Epoch 3/5\n",
            "  1/262 [..............................] - ETA: 3:06:50 - loss: 0.2542 - accuracy: 0.9375 - precision: 0.9524 - recall: 0.9524 - prc: 0.9353\n",
            "  2/262 [..............................] - ETA: 28:32 - loss: 0.2410 - accuracy: 0.9375 - precision: 0.9512 - recall: 0.9512 - prc: 0.9575  \n",
            "  3/262 [..............................] - ETA: 31:22 - loss: 0.1845 - accuracy: 0.9375 - precision: 0.9344 - recall: 0.9661 - prc: 0.9702\n",
            "  4/262 [..............................] - ETA: 35:14 - loss: 0.1881 - accuracy: 0.9297 - precision: 0.9375 - recall: 0.9494 - prc: 0.9756\n",
            "  5/262 [..............................] - ETA: 36:22 - loss: 0.2255 - accuracy: 0.9125 - precision: 0.9216 - recall: 0.9400 - prc: 0.9762\n",
            "  6/262 [..............................] - ETA: 34:40 - loss: 0.2036 - accuracy: 0.9167 - precision: 0.9256 - recall: 0.9412 - prc: 0.9792\n",
            "  7/262 [..............................] - ETA: 33:13 - loss: 0.2408 - accuracy: 0.9107 - precision: 0.9281 - recall: 0.9281 - prc: 0.9785\n",
            "  8/262 [..............................] - ETA: 32:32 - loss: 0.2206 - accuracy: 0.9180 - precision: 0.9317 - recall: 0.9375 - prc: 0.9816\n",
            "  9/262 [>.............................] - ETA: 31:55 - loss: 0.2017 - accuracy: 0.9271 - precision: 0.9399 - recall: 0.9451 - prc: 0.9841\n",
            " 10/262 [>.............................] - ETA: 31:29 - loss: 0.1963 - accuracy: 0.9250 - precision: 0.9314 - recall: 0.9500 - prc: 0.9850\n",
            " 11/262 [>.............................] - ETA: 31:04 - loss: 0.1967 - accuracy: 0.9290 - precision: 0.9375 - recall: 0.9502 - prc: 0.9855\n",
            " 12/262 [>.............................] - ETA: 30:40 - loss: 0.2037 - accuracy: 0.9245 - precision: 0.9385 - recall: 0.9424 - prc: 0.9852\n",
            " 13/262 [>.............................] - ETA: 30:21 - loss: 0.2015 - accuracy: 0.9255 - precision: 0.9432 - recall: 0.9396 - prc: 0.9857\n",
            " 14/262 [>.............................] - ETA: 30:04 - loss: 0.2310 - accuracy: 0.9219 - precision: 0.9443 - recall: 0.9345 - prc: 0.9846\n",
            " 15/262 [>.............................] - ETA: 30:47 - loss: 0.2228 - accuracy: 0.9229 - precision: 0.9448 - recall: 0.9357 - prc: 0.9856\n",
            " 16/262 [>.............................] - ETA: 30:30 - loss: 0.2130 - accuracy: 0.9277 - precision: 0.9483 - recall: 0.9398 - prc: 0.9863\n",
            " 17/262 [>.............................] - ETA: 30:13 - loss: 0.2060 - accuracy: 0.9283 - precision: 0.9517 - recall: 0.9384 - prc: 0.9872\n",
            " 18/262 [=>............................] - ETA: 30:00 - loss: 0.1959 - accuracy: 0.9323 - precision: 0.9548 - recall: 0.9423 - prc: 0.9882\n",
            " 19/262 [=>............................] - ETA: 29:45 - loss: 0.1971 - accuracy: 0.9293 - precision: 0.9544 - recall: 0.9378 - prc: 0.9881\n",
            " 20/262 [=>............................] - ETA: 29:29 - loss: 0.1937 - accuracy: 0.9297 - precision: 0.9569 - recall: 0.9368 - prc: 0.9886\n",
            " 21/262 [=>............................] - ETA: 29:14 - loss: 0.1994 - accuracy: 0.9286 - precision: 0.9567 - recall: 0.9354 - prc: 0.9882\n",
            " 22/262 [=>............................] - ETA: 29:01 - loss: 0.1936 - accuracy: 0.9304 - precision: 0.9563 - recall: 0.9379 - prc: 0.9886\n",
            " 23/262 [=>............................] - ETA: 28:48 - loss: 0.1889 - accuracy: 0.9321 - precision: 0.9582 - recall: 0.9387 - prc: 0.9889\n",
            " 24/262 [=>............................] - ETA: 28:35 - loss: 0.1878 - accuracy: 0.9310 - precision: 0.9559 - recall: 0.9390 - prc: 0.9891\n",
            " 25/262 [=>............................] - ETA: 28:22 - loss: 0.1949 - accuracy: 0.9300 - precision: 0.9559 - recall: 0.9379 - prc: 0.9887\n",
            " 26/262 [=>............................] - ETA: 28:11 - loss: 0.1958 - accuracy: 0.9303 - precision: 0.9555 - recall: 0.9381 - prc: 0.9886\n",
            " 27/262 [==>...........................] - ETA: 28:00 - loss: 0.1928 - accuracy: 0.9317 - precision: 0.9555 - recall: 0.9405 - prc: 0.9889\n",
            " 28/262 [==>...........................] - ETA: 27:49 - loss: 0.1955 - accuracy: 0.9286 - precision: 0.9538 - recall: 0.9377 - prc: 0.9888\n",
            " 29/262 [==>...........................] - ETA: 27:37 - loss: 0.1914 - accuracy: 0.9289 - precision: 0.9556 - recall: 0.9371 - prc: 0.9893\n",
            " 30/262 [==>...........................] - ETA: 27:27 - loss: 0.1944 - accuracy: 0.9271 - precision: 0.9539 - recall: 0.9360 - prc: 0.9891\n",
            " 31/262 [==>...........................] - ETA: 27:16 - loss: 0.1932 - accuracy: 0.9264 - precision: 0.9538 - recall: 0.9351 - prc: 0.9893\n",
            " 32/262 [==>...........................] - ETA: 27:05 - loss: 0.1934 - accuracy: 0.9268 - precision: 0.9553 - recall: 0.9344 - prc: 0.9894\n",
            " 33/262 [==>...........................] - ETA: 26:55 - loss: 0.1939 - accuracy: 0.9280 - precision: 0.9551 - recall: 0.9362 - prc: 0.9878\n",
            " 34/262 [==>...........................] - ETA: 26:46 - loss: 0.1904 - accuracy: 0.9283 - precision: 0.9552 - recall: 0.9368 - prc: 0.9882\n",
            " 35/262 [===>..........................] - ETA: 26:37 - loss: 0.1878 - accuracy: 0.9295 - precision: 0.9564 - recall: 0.9372 - prc: 0.9884\n",
            " 36/262 [===>..........................] - ETA: 26:29 - loss: 0.1846 - accuracy: 0.9306 - precision: 0.9578 - recall: 0.9380 - prc: 0.9888\n",
            " 37/262 [===>..........................] - ETA: 26:21 - loss: 0.1874 - accuracy: 0.9299 - precision: 0.9564 - recall: 0.9383 - prc: 0.9874\n",
            " 38/262 [===>..........................] - ETA: 26:13 - loss: 0.1833 - accuracy: 0.9317 - precision: 0.9574 - recall: 0.9398 - prc: 0.9877\n",
            " 39/262 [===>..........................] - ETA: 26:04 - loss: 0.1859 - accuracy: 0.9311 - precision: 0.9550 - recall: 0.9412 - prc: 0.9876\n",
            " 40/262 [===>..........................] - ETA: 25:56 - loss: 0.1842 - accuracy: 0.9320 - precision: 0.9549 - recall: 0.9426 - prc: 0.9878\n",
            " 41/262 [===>..........................] - ETA: 25:48 - loss: 0.1814 - accuracy: 0.9337 - precision: 0.9561 - recall: 0.9441 - prc: 0.9880\n",
            " 42/262 [===>..........................] - ETA: 25:40 - loss: 0.1848 - accuracy: 0.9338 - precision: 0.9572 - recall: 0.9433 - prc: 0.9879\n",
            " 43/262 [===>..........................] - ETA: 25:32 - loss: 0.1909 - accuracy: 0.9331 - precision: 0.9560 - recall: 0.9436 - prc: 0.9864\n",
            " 44/262 [====>.........................] - ETA: 25:25 - loss: 0.1932 - accuracy: 0.9325 - precision: 0.9549 - recall: 0.9437 - prc: 0.9863\n",
            " 45/262 [====>.........................] - ETA: 25:16 - loss: 0.2021 - accuracy: 0.9299 - precision: 0.9515 - recall: 0.9426 - prc: 0.9846\n",
            " 46/262 [====>.........................] - ETA: 25:08 - loss: 0.2021 - accuracy: 0.9300 - precision: 0.9507 - recall: 0.9439 - prc: 0.9848\n",
            " 47/262 [====>.........................] - ETA: 25:00 - loss: 0.2078 - accuracy: 0.9282 - precision: 0.9497 - recall: 0.9422 - prc: 0.9835\n",
            " 48/262 [====>.........................] - ETA: 24:53 - loss: 0.2061 - accuracy: 0.9284 - precision: 0.9498 - recall: 0.9424 - prc: 0.9837\n",
            " 49/262 [====>.........................] - ETA: 24:45 - loss: 0.2064 - accuracy: 0.9286 - precision: 0.9508 - recall: 0.9417 - prc: 0.9839\n",
            " 50/262 [====>.........................] - ETA: 24:38 - loss: 0.2042 - accuracy: 0.9294 - precision: 0.9508 - recall: 0.9427 - prc: 0.9840\n",
            " 51/262 [====>.........................] - ETA: 24:30 - loss: 0.2095 - accuracy: 0.9289 - precision: 0.9498 - recall: 0.9428 - prc: 0.9818\n",
            " 52/262 [====>.........................] - ETA: 24:23 - loss: 0.2057 - accuracy: 0.9303 - precision: 0.9509 - recall: 0.9440 - prc: 0.9823\n",
            " 53/262 [=====>........................] - ETA: 24:15 - loss: 0.2047 - accuracy: 0.9310 - precision: 0.9519 - recall: 0.9443 - prc: 0.9826\n",
            " 54/262 [=====>........................] - ETA: 24:07 - loss: 0.2065 - accuracy: 0.9306 - precision: 0.9500 - recall: 0.9451 - prc: 0.9825\n",
            " 55/262 [=====>........................] - ETA: 24:00 - loss: 0.2031 - accuracy: 0.9318 - precision: 0.9511 - recall: 0.9462 - prc: 0.9829\n",
            " 56/262 [=====>........................] - ETA: 23:52 - loss: 0.2002 - accuracy: 0.9330 - precision: 0.9522 - recall: 0.9474 - prc: 0.9832\n",
            " 57/262 [=====>........................] - ETA: 23:45 - loss: 0.2001 - accuracy: 0.9326 - precision: 0.9521 - recall: 0.9467 - prc: 0.9834\n",
            " 58/262 [=====>........................] - ETA: 23:37 - loss: 0.1970 - accuracy: 0.9337 - precision: 0.9530 - recall: 0.9476 - prc: 0.9838\n",
            " 59/262 [=====>........................] - ETA: 23:30 - loss: 0.1990 - accuracy: 0.9327 - precision: 0.9531 - recall: 0.9463 - prc: 0.9838\n",
            " 60/262 [=====>........................] - ETA: 23:23 - loss: 0.1970 - accuracy: 0.9328 - precision: 0.9531 - recall: 0.9464 - prc: 0.9840\n",
            " 61/262 [=====>........................] - ETA: 23:16 - loss: 0.1948 - accuracy: 0.9334 - precision: 0.9540 - recall: 0.9467 - prc: 0.9843\n",
            " 62/262 [======>.......................] - ETA: 23:09 - loss: 0.1950 - accuracy: 0.9335 - precision: 0.9532 - recall: 0.9475 - prc: 0.9844\n",
            " 63/262 [======>.......................] - ETA: 23:02 - loss: 0.1944 - accuracy: 0.9335 - precision: 0.9526 - recall: 0.9483 - prc: 0.9845\n",
            " 64/262 [======>.......................] - ETA: 22:55 - loss: 0.1938 - accuracy: 0.9336 - precision: 0.9533 - recall: 0.9478 - prc: 0.9846\n",
            " 65/262 [======>.......................] - ETA: 22:47 - loss: 0.1920 - accuracy: 0.9337 - precision: 0.9541 - recall: 0.9473 - prc: 0.9849\n",
            " 66/262 [======>.......................] - ETA: 22:40 - loss: 0.1948 - accuracy: 0.9332 - precision: 0.9528 - recall: 0.9481 - prc: 0.9848\n",
            " 67/262 [======>.......................] - ETA: 22:32 - loss: 0.1928 - accuracy: 0.9338 - precision: 0.9536 - recall: 0.9483 - prc: 0.9851\n",
            " 68/262 [======>.......................] - ETA: 22:25 - loss: 0.1923 - accuracy: 0.9334 - precision: 0.9530 - recall: 0.9484 - prc: 0.9852\n",
            " 69/262 [======>.......................] - ETA: 22:18 - loss: 0.1926 - accuracy: 0.9330 - precision: 0.9537 - recall: 0.9474 - prc: 0.9853\n",
            " 70/262 [=======>......................] - ETA: 22:11 - loss: 0.1923 - accuracy: 0.9335 - precision: 0.9543 - recall: 0.9474 - prc: 0.9854\n",
            " 71/262 [=======>......................] - ETA: 22:03 - loss: 0.1919 - accuracy: 0.9331 - precision: 0.9537 - recall: 0.9475 - prc: 0.9855\n",
            " 72/262 [=======>......................] - ETA: 21:56 - loss: 0.1899 - accuracy: 0.9336 - precision: 0.9538 - recall: 0.9483 - prc: 0.9857\n",
            " 73/262 [=======>......................] - ETA: 21:49 - loss: 0.1902 - accuracy: 0.9332 - precision: 0.9537 - recall: 0.9477 - prc: 0.9858\n",
            " 74/262 [=======>......................] - ETA: 21:42 - loss: 0.1902 - accuracy: 0.9337 - precision: 0.9543 - recall: 0.9478 - prc: 0.9859\n",
            " 75/262 [=======>......................] - ETA: 21:35 - loss: 0.1910 - accuracy: 0.9333 - precision: 0.9531 - recall: 0.9484 - prc: 0.9858\n",
            " 76/262 [=======>......................] - ETA: 21:27 - loss: 0.1910 - accuracy: 0.9338 - precision: 0.9530 - recall: 0.9490 - prc: 0.9852\n",
            " 77/262 [=======>......................] - ETA: 21:20 - loss: 0.1903 - accuracy: 0.9338 - precision: 0.9531 - recall: 0.9491 - prc: 0.9853\n",
            " 78/262 [=======>......................] - ETA: 21:12 - loss: 0.1913 - accuracy: 0.9339 - precision: 0.9531 - recall: 0.9492 - prc: 0.9853\n",
            " 79/262 [========>.....................] - ETA: 21:05 - loss: 0.1903 - accuracy: 0.9343 - precision: 0.9537 - recall: 0.9492 - prc: 0.9854\n",
            " 80/262 [========>.....................] - ETA: 20:58 - loss: 0.1882 - accuracy: 0.9352 - precision: 0.9542 - recall: 0.9498 - prc: 0.9856\n",
            " 81/262 [========>.....................] - ETA: 20:50 - loss: 0.1882 - accuracy: 0.9352 - precision: 0.9536 - recall: 0.9503 - prc: 0.9856\n",
            " 82/262 [========>.....................] - ETA: 20:43 - loss: 0.1898 - accuracy: 0.9348 - precision: 0.9542 - recall: 0.9493 - prc: 0.9856\n",
            " 83/262 [========>.....................] - ETA: 20:35 - loss: 0.1886 - accuracy: 0.9352 - precision: 0.9542 - recall: 0.9500 - prc: 0.9858\n",
            " 84/262 [========>.....................] - ETA: 20:28 - loss: 0.1904 - accuracy: 0.9342 - precision: 0.9542 - recall: 0.9484 - prc: 0.9857\n",
            " 85/262 [========>.....................] - ETA: 20:21 - loss: 0.1891 - accuracy: 0.9346 - precision: 0.9548 - recall: 0.9486 - prc: 0.9859\n",
            " 86/262 [========>.....................] - ETA: 20:14 - loss: 0.1875 - accuracy: 0.9346 - precision: 0.9542 - recall: 0.9491 - prc: 0.9861\n",
            " 87/262 [========>.....................] - ETA: 20:06 - loss: 0.1874 - accuracy: 0.9346 - precision: 0.9538 - recall: 0.9497 - prc: 0.9862\n",
            " 88/262 [=========>....................] - ETA: 19:59 - loss: 0.1866 - accuracy: 0.9350 - precision: 0.9538 - recall: 0.9503 - prc: 0.9863\n",
            " 89/262 [=========>....................] - ETA: 19:52 - loss: 0.1868 - accuracy: 0.9350 - precision: 0.9543 - recall: 0.9498 - prc: 0.9864\n",
            " 90/262 [=========>....................] - ETA: 19:45 - loss: 0.1862 - accuracy: 0.9354 - precision: 0.9543 - recall: 0.9504 - prc: 0.9864\n",
            " 91/262 [=========>....................] - ETA: 19:37 - loss: 0.1853 - accuracy: 0.9351 - precision: 0.9542 - recall: 0.9498 - prc: 0.9865\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            " 92/262 [=========>....................] - ETA: 19:30 - loss: 0.1880 - accuracy: 0.9344 - precision: 0.9537 - recall: 0.9494 - prc: 0.9864\n",
            " 93/262 [=========>....................] - ETA: 19:23 - loss: 0.1877 - accuracy: 0.9345 - precision: 0.9532 - recall: 0.9499 - prc: 0.9865\n",
            " 94/262 [=========>....................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 19:16 - loss: 0.1868 - accuracy: 0.9345 - precision: 0.9532 - recall: 0.9499 - prc: 0.9866\n",
            " 95/262 [=========>....................] - ETA: 19:09 - loss: 0.1854 - accuracy: 0.9349 - precision: 0.9531 - recall: 0.9503 - prc: 0.9867\n",
            " 96/262 [=========>....................] - ETA: 19:01 - loss: 0.1844 - accuracy: 0.9352 - precision: 0.9536 - recall: 0.9504 - prc: 0.9868\n",
            " 97/262 [==========>...................] - ETA: 18:54 - loss: 0.1833 - accuracy: 0.9356 - precision: 0.9537 - recall: 0.9510 - prc: 0.9869\n",
            " 98/262 [==========>...................] - ETA: 18:47 - loss: 0.1835 - accuracy: 0.9353 - precision: 0.9533 - recall: 0.9510 - prc: 0.9869\n",
            " 99/262 [==========>...................] - ETA: 18:39 - loss: 0.1834 - accuracy: 0.9353 - precision: 0.9537 - recall: 0.9505 - prc: 0.9870\n",
            "100/262 [==========>...................] - ETA: 18:32 - loss: 0.1833 - accuracy: 0.9356 - precision: 0.9541 - recall: 0.9506 - prc: 0.9871\n",
            "101/262 [==========>...................] - ETA: 18:25 - loss: 0.1845 - accuracy: 0.9356 - precision: 0.9537 - recall: 0.9511 - prc: 0.9866\n",
            "102/262 [==========>...................] - ETA: 18:18 - loss: 0.1835 - accuracy: 0.9357 - precision: 0.9542 - recall: 0.9507 - prc: 0.9867\n",
            "103/262 [==========>...................] - ETA: 18:11 - loss: 0.1844 - accuracy: 0.9351 - precision: 0.9529 - recall: 0.9511 - prc: 0.9866\n",
            "104/262 [==========>...................] - ETA: 18:03 - loss: 0.1840 - accuracy: 0.9348 - precision: 0.9534 - recall: 0.9504 - prc: 0.9867\n",
            "105/262 [===========>..................] - ETA: 17:56 - loss: 0.1837 - accuracy: 0.9348 - precision: 0.9534 - recall: 0.9505 - prc: 0.9868\n",
            "106/262 [===========>..................] - ETA: 17:49 - loss: 0.1831 - accuracy: 0.9351 - precision: 0.9539 - recall: 0.9506 - prc: 0.9869\n",
            "107/262 [===========>..................] - ETA: 17:42 - loss: 0.1824 - accuracy: 0.9355 - precision: 0.9539 - recall: 0.9510 - prc: 0.9870\n",
            "108/262 [===========>..................] - ETA: 17:35 - loss: 0.1822 - accuracy: 0.9355 - precision: 0.9539 - recall: 0.9511 - prc: 0.9871\n",
            "109/262 [===========>..................] - ETA: 17:28 - loss: 0.1818 - accuracy: 0.9355 - precision: 0.9544 - recall: 0.9507 - prc: 0.9872\n",
            "110/262 [===========>..................] - ETA: 17:21 - loss: 0.1826 - accuracy: 0.9358 - precision: 0.9543 - recall: 0.9511 - prc: 0.9868\n",
            "111/262 [===========>..................] - ETA: 17:13 - loss: 0.1838 - accuracy: 0.9358 - precision: 0.9548 - recall: 0.9508 - prc: 0.9868\n",
            "112/262 [===========>..................] - ETA: 17:06 - loss: 0.1837 - accuracy: 0.9358 - precision: 0.9543 - recall: 0.9512 - prc: 0.9868\n",
            "113/262 [===========>..................] - ETA: 16:59 - loss: 0.1844 - accuracy: 0.9358 - precision: 0.9543 - recall: 0.9512 - prc: 0.9868\n",
            "114/262 [============>.................] - ETA: 16:52 - loss: 0.1830 - accuracy: 0.9364 - precision: 0.9546 - recall: 0.9515 - prc: 0.9869\n",
            "115/262 [============>.................] - ETA: 16:45 - loss: 0.1839 - accuracy: 0.9359 - precision: 0.9541 - recall: 0.9510 - prc: 0.9869\n",
            "116/262 [============>.................] - ETA: 16:38 - loss: 0.1845 - accuracy: 0.9353 - precision: 0.9533 - recall: 0.9511 - prc: 0.9869\n",
            "117/262 [============>.................] - ETA: 16:31 - loss: 0.1831 - accuracy: 0.9359 - precision: 0.9538 - recall: 0.9515 - prc: 0.9870\n",
            "118/262 [============>.................] - ETA: 16:24 - loss: 0.1833 - accuracy: 0.9359 - precision: 0.9542 - recall: 0.9512 - prc: 0.9871\n",
            "119/262 [============>.................] - ETA: 16:17 - loss: 0.1828 - accuracy: 0.9359 - precision: 0.9542 - recall: 0.9513 - prc: 0.9872\n",
            "120/262 [============>.................] - ETA: 16:10 - loss: 0.1822 - accuracy: 0.9359 - precision: 0.9543 - recall: 0.9513 - prc: 0.9873\n",
            "121/262 [============>.................] - ETA: 16:03 - loss: 0.1834 - accuracy: 0.9360 - precision: 0.9546 - recall: 0.9510 - prc: 0.9872\n",
            "122/262 [============>.................] - ETA: 15:56 - loss: 0.1830 - accuracy: 0.9357 - precision: 0.9546 - recall: 0.9506 - prc: 0.9873\n",
            "123/262 [=============>................] - ETA: 15:49 - loss: 0.1823 - accuracy: 0.9360 - precision: 0.9545 - recall: 0.9509 - prc: 0.9873\n",
            "124/262 [=============>................] - ETA: 15:42 - loss: 0.1817 - accuracy: 0.9360 - precision: 0.9540 - recall: 0.9512 - prc: 0.9873\n",
            "125/262 [=============>................] - ETA: 15:34 - loss: 0.1816 - accuracy: 0.9360 - precision: 0.9541 - recall: 0.9512 - prc: 0.9874\n",
            "126/262 [=============>................] - ETA: 15:27 - loss: 0.1810 - accuracy: 0.9363 - precision: 0.9541 - recall: 0.9516 - prc: 0.9875\n",
            "127/262 [=============>................] - ETA: 15:27 - loss: 0.1803 - accuracy: 0.9365 - precision: 0.9545 - recall: 0.9517 - prc: 0.9876\n",
            "128/262 [=============>................] - ETA: 15:26 - loss: 0.1798 - accuracy: 0.9368 - precision: 0.9548 - recall: 0.9517 - prc: 0.9877\n",
            "129/262 [=============>................] - ETA: 15:24 - loss: 0.1798 - accuracy: 0.9363 - precision: 0.9545 - recall: 0.9514 - prc: 0.9877\n",
            "130/262 [=============>................] - ETA: 15:23 - loss: 0.1788 - accuracy: 0.9365 - precision: 0.9545 - recall: 0.9518 - prc: 0.9878\n",
            "131/262 [==============>...............] - ETA: 15:14 - loss: 0.1785 - accuracy: 0.9367 - precision: 0.9547 - recall: 0.9520 - prc: 0.9878\n",
            "132/262 [==============>...............] - ETA: 15:13 - loss: 0.1775 - accuracy: 0.9370 - precision: 0.9550 - recall: 0.9520 - prc: 0.9879\n",
            "133/262 [==============>...............] - ETA: 15:06 - loss: 0.1795 - accuracy: 0.9363 - precision: 0.9537 - recall: 0.9523 - prc: 0.9878\n",
            "134/262 [==============>...............] - ETA: 14:59 - loss: 0.1798 - accuracy: 0.9358 - precision: 0.9529 - recall: 0.9523 - prc: 0.9878\n",
            "135/262 [==============>...............] - ETA: 14:52 - loss: 0.1801 - accuracy: 0.9356 - precision: 0.9529 - recall: 0.9519 - prc: 0.9878\n",
            "136/262 [==============>...............] - ETA: 14:44 - loss: 0.1796 - accuracy: 0.9356 - precision: 0.9529 - recall: 0.9520 - prc: 0.9879\n",
            "137/262 [==============>...............] - ETA: 14:37 - loss: 0.1786 - accuracy: 0.9361 - precision: 0.9533 - recall: 0.9523 - prc: 0.9880\n",
            "138/262 [==============>...............] - ETA: 14:30 - loss: 0.1791 - accuracy: 0.9361 - precision: 0.9536 - recall: 0.9520 - prc: 0.9880\n",
            "139/262 [==============>...............] - ETA: 14:23 - loss: 0.1802 - accuracy: 0.9361 - precision: 0.9536 - recall: 0.9520 - prc: 0.9879\n",
            "140/262 [===============>..............] - ETA: 14:16 - loss: 0.1808 - accuracy: 0.9363 - precision: 0.9539 - recall: 0.9520 - prc: 0.9880\n",
            "141/262 [===============>..............] - ETA: 14:09 - loss: 0.1803 - accuracy: 0.9363 - precision: 0.9542 - recall: 0.9517 - prc: 0.9880\n",
            "142/262 [===============>..............] - ETA: 14:01 - loss: 0.1791 - accuracy: 0.9368 - precision: 0.9545 - recall: 0.9520 - prc: 0.9881\n",
            "143/262 [===============>..............] - ETA: 13:54 - loss: 0.1781 - accuracy: 0.9370 - precision: 0.9545 - recall: 0.9523 - prc: 0.9882\n",
            "144/262 [===============>..............] - ETA: 13:47 - loss: 0.1773 - accuracy: 0.9372 - precision: 0.9545 - recall: 0.9527 - prc: 0.9883\n",
            "145/262 [===============>..............] - ETA: 13:40 - loss: 0.1762 - accuracy: 0.9377 - precision: 0.9549 - recall: 0.9530 - prc: 0.9884\n",
            "146/262 [===============>..............] - ETA: 13:33 - loss: 0.1760 - accuracy: 0.9377 - precision: 0.9545 - recall: 0.9533 - prc: 0.9884\n",
            "147/262 [===============>..............] - ETA: 13:26 - loss: 0.1781 - accuracy: 0.9377 - precision: 0.9545 - recall: 0.9533 - prc: 0.9881\n",
            "148/262 [===============>..............] - ETA: 13:19 - loss: 0.1772 - accuracy: 0.9379 - precision: 0.9545 - recall: 0.9536 - prc: 0.9882\n",
            "149/262 [================>.............] - ETA: 13:11 - loss: 0.1773 - accuracy: 0.9379 - precision: 0.9547 - recall: 0.9533 - prc: 0.9882\n",
            "150/262 [================>.............] - ETA: 13:04 - loss: 0.1770 - accuracy: 0.9377 - precision: 0.9547 - recall: 0.9530 - prc: 0.9882\n",
            "151/262 [================>.............] - ETA: 12:57 - loss: 0.1776 - accuracy: 0.9374 - precision: 0.9547 - recall: 0.9527 - prc: 0.9882\n",
            "152/262 [================>.............] - ETA: 12:50 - loss: 0.1771 - accuracy: 0.9377 - precision: 0.9551 - recall: 0.9527 - prc: 0.9883\n",
            "153/262 [================>.............] - ETA: 12:43 - loss: 0.1765 - accuracy: 0.9379 - precision: 0.9551 - recall: 0.9531 - prc: 0.9884\n",
            "154/262 [================>.............] - ETA: 12:36 - loss: 0.1758 - accuracy: 0.9381 - precision: 0.9554 - recall: 0.9531 - prc: 0.9884\n",
            "155/262 [================>.............] - ETA: 12:29 - loss: 0.1755 - accuracy: 0.9381 - precision: 0.9554 - recall: 0.9531 - prc: 0.9885\n",
            "156/262 [================>.............] - ETA: 12:21 - loss: 0.1749 - accuracy: 0.9383 - precision: 0.9557 - recall: 0.9531 - prc: 0.9886\n",
            "157/262 [================>.............] - ETA: 12:14 - loss: 0.1763 - accuracy: 0.9378 - precision: 0.9551 - recall: 0.9531 - prc: 0.9885\n",
            "158/262 [=================>............] - ETA: 12:07 - loss: 0.1757 - accuracy: 0.9378 - precision: 0.9548 - recall: 0.9534 - prc: 0.9885\n",
            "159/262 [=================>............] - ETA: 12:00 - loss: 0.1761 - accuracy: 0.9378 - precision: 0.9544 - recall: 0.9536 - prc: 0.9885\n",
            "160/262 [=================>............]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 11:53 - loss: 0.1755 - accuracy: 0.9380 - precision: 0.9547 - recall: 0.9536 - prc: 0.9886\n",
            "161/262 [=================>............] - ETA: 11:46 - loss: 0.1755 - accuracy: 0.9378 - precision: 0.9550 - recall: 0.9531 - prc: 0.9886\n",
            "162/262 [=================>............] - ETA: 11:39 - loss: 0.1754 - accuracy: 0.9376 - precision: 0.9545 - recall: 0.9534 - prc: 0.9886\n",
            "163/262 [=================>............] - ETA: 11:32 - loss: 0.1758 - accuracy: 0.9376 - precision: 0.9545 - recall: 0.9534 - prc: 0.9886\n",
            "164/262 [=================>............] - ETA: 11:25 - loss: 0.1760 - accuracy: 0.9376 - precision: 0.9545 - recall: 0.9534 - prc: 0.9887\n",
            "165/262 [=================>............] - ETA: 11:18 - loss: 0.1755 - accuracy: 0.9376 - precision: 0.9542 - recall: 0.9537 - prc: 0.9887\n",
            "166/262 [==================>...........] - ETA: 11:11 - loss: 0.1752 - accuracy: 0.9376 - precision: 0.9543 - recall: 0.9537 - prc: 0.9888\n",
            "167/262 [==================>...........] - ETA: 11:04 - loss: 0.1751 - accuracy: 0.9376 - precision: 0.9542 - recall: 0.9536 - prc: 0.9888\n",
            "168/262 [==================>...........] - ETA: 10:57 - loss: 0.1743 - accuracy: 0.9380 - precision: 0.9544 - recall: 0.9539 - prc: 0.9888\n",
            "169/262 [==================>...........] - ETA: 10:50 - loss: 0.1738 - accuracy: 0.9382 - precision: 0.9546 - recall: 0.9538 - prc: 0.9889\n",
            "170/262 [==================>...........] - ETA: 10:43 - loss: 0.1761 - accuracy: 0.9380 - precision: 0.9549 - recall: 0.9533 - prc: 0.9888\n",
            "171/262 [==================>...........] - ETA: 10:36 - loss: 0.1795 - accuracy: 0.9371 - precision: 0.9540 - recall: 0.9527 - prc: 0.9880\n",
            "172/262 [==================>...........] - ETA: 10:29 - loss: 0.1789 - accuracy: 0.9373 - precision: 0.9541 - recall: 0.9530 - prc: 0.9881\n",
            "173/262 [==================>...........] - ETA: 10:22 - loss: 0.1783 - accuracy: 0.9375 - precision: 0.9541 - recall: 0.9533 - prc: 0.9882\n",
            "174/262 [==================>...........] - ETA: 10:15 - loss: 0.1773 - accuracy: 0.9378 - precision: 0.9543 - recall: 0.9535 - prc: 0.9883\n",
            "175/262 [===================>..........] - ETA: 10:08 - loss: 0.1789 - accuracy: 0.9375 - precision: 0.9540 - recall: 0.9532 - prc: 0.9879\n",
            "176/262 [===================>..........] - ETA: 10:01 - loss: 0.1796 - accuracy: 0.9371 - precision: 0.9540 - recall: 0.9527 - prc: 0.9879\n",
            "177/262 [===================>..........] - ETA: 9:54 - loss: 0.1792 - accuracy: 0.9369 - precision: 0.9542 - recall: 0.9522 - prc: 0.9880 \n",
            "178/262 [===================>..........] - ETA: 9:47 - loss: 0.1790 - accuracy: 0.9371 - precision: 0.9542 - recall: 0.9525 - prc: 0.9880\n",
            "179/262 [===================>..........] - ETA: 9:40 - loss: 0.1797 - accuracy: 0.9369 - precision: 0.9540 - recall: 0.9525 - prc: 0.9879\n",
            "180/262 [===================>..........] - ETA: 9:33 - loss: 0.1789 - accuracy: 0.9373 - precision: 0.9542 - recall: 0.9527 - prc: 0.9880\n",
            "181/262 [===================>..........] - ETA: 9:26 - loss: 0.1782 - accuracy: 0.9375 - precision: 0.9544 - recall: 0.9527 - prc: 0.9881\n",
            "182/262 [===================>..........] - ETA: 9:19 - loss: 0.1777 - accuracy: 0.9376 - precision: 0.9547 - recall: 0.9527 - prc: 0.9881\n",
            "183/262 [===================>..........] - ETA: 9:12 - loss: 0.1794 - accuracy: 0.9375 - precision: 0.9544 - recall: 0.9527 - prc: 0.9875\n",
            "184/262 [====================>.........] - ETA: 9:05 - loss: 0.1785 - accuracy: 0.9378 - precision: 0.9546 - recall: 0.9529 - prc: 0.9876\n",
            "185/262 [====================>.........] - ETA: 8:58 - loss: 0.1781 - accuracy: 0.9380 - precision: 0.9549 - recall: 0.9529 - prc: 0.9877\n",
            "186/262 [====================>.........] - ETA: 8:51 - loss: 0.1785 - accuracy: 0.9378 - precision: 0.9551 - recall: 0.9525 - prc: 0.9877\n",
            "187/262 [====================>.........] - ETA: 8:44 - loss: 0.1779 - accuracy: 0.9380 - precision: 0.9554 - recall: 0.9525 - prc: 0.9877\n",
            "188/262 [====================>.........] - ETA: 8:37 - loss: 0.1776 - accuracy: 0.9380 - precision: 0.9553 - recall: 0.9525 - prc: 0.9878\n",
            "189/262 [====================>.........] - ETA: 8:30 - loss: 0.1772 - accuracy: 0.9381 - precision: 0.9554 - recall: 0.9528 - prc: 0.9878\n",
            "190/262 [====================>.........] - ETA: 8:23 - loss: 0.1765 - accuracy: 0.9384 - precision: 0.9556 - recall: 0.9530 - prc: 0.9879\n",
            "191/262 [====================>.........] - ETA: 8:16 - loss: 0.1791 - accuracy: 0.9381 - precision: 0.9556 - recall: 0.9526 - prc: 0.9878\n",
            "192/262 [====================>.........] - ETA: 8:09 - loss: 0.1785 - accuracy: 0.9383 - precision: 0.9556 - recall: 0.9528 - prc: 0.9879\n",
            "193/262 [=====================>........] - ETA: 8:02 - loss: 0.1779 - accuracy: 0.9384 - precision: 0.9558 - recall: 0.9528 - prc: 0.9879\n",
            "194/262 [=====================>........] - ETA: 7:55 - loss: 0.1785 - accuracy: 0.9384 - precision: 0.9558 - recall: 0.9528 - prc: 0.9879\n",
            "195/262 [=====================>........] - ETA: 7:48 - loss: 0.1786 - accuracy: 0.9383 - precision: 0.9558 - recall: 0.9526 - prc: 0.9879\n",
            "196/262 [=====================>........] - ETA: 7:41 - loss: 0.1781 - accuracy: 0.9383 - precision: 0.9555 - recall: 0.9528 - prc: 0.9880\n",
            "197/262 [=====================>........] - ETA: 7:34 - loss: 0.1787 - accuracy: 0.9379 - precision: 0.9553 - recall: 0.9526 - prc: 0.9880\n",
            "198/262 [=====================>........] - ETA: 7:27 - loss: 0.1780 - accuracy: 0.9381 - precision: 0.9556 - recall: 0.9526 - prc: 0.9880\n",
            "199/262 [=====================>........] - ETA: 7:20 - loss: 0.1777 - accuracy: 0.9381 - precision: 0.9555 - recall: 0.9526 - prc: 0.9881\n",
            "200/262 [=====================>........] - ETA: 7:13 - loss: 0.1771 - accuracy: 0.9382 - precision: 0.9555 - recall: 0.9528 - prc: 0.9881\n",
            "201/262 [======================>.......] - ETA: 7:06 - loss: 0.1763 - accuracy: 0.9386 - precision: 0.9557 - recall: 0.9531 - prc: 0.9882\n",
            "202/262 [======================>.......] - ETA: 7:00 - loss: 0.1759 - accuracy: 0.9385 - precision: 0.9560 - recall: 0.9529 - prc: 0.9883\n",
            "203/262 [======================>.......] - ETA: 6:53 - loss: 0.1759 - accuracy: 0.9385 - precision: 0.9562 - recall: 0.9527 - prc: 0.9883\n",
            "204/262 [======================>.......] - ETA: 6:46 - loss: 0.1765 - accuracy: 0.9385 - precision: 0.9564 - recall: 0.9525 - prc: 0.9883\n",
            "205/262 [======================>.......] - ETA: 6:39 - loss: 0.1760 - accuracy: 0.9387 - precision: 0.9564 - recall: 0.9527 - prc: 0.9884\n",
            "206/262 [======================>.......] - ETA: 6:32 - loss: 0.1762 - accuracy: 0.9387 - precision: 0.9561 - recall: 0.9529 - prc: 0.9883\n",
            "207/262 [======================>.......] - ETA: 6:25 - loss: 0.1759 - accuracy: 0.9387 - precision: 0.9559 - recall: 0.9531 - prc: 0.9884\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 208/262 [======================>.......] - ETA: 6:18 - loss: 0.1763 - accuracy: 0.9388 - precision: 0.9561 - recall: 0.9531 - prc: 0.9884\n",
            "209/262 [======================>.......] - ETA: 6:11 - loss: 0.1758 - accuracy: 0.9391 - precision: 0.9563 - recall: 0.9533 - prc: 0.9884\n",
            "210/262 [=======================>......] - ETA: 6:03 - loss: 0.1751 - accuracy: 0.9394 - precision: 0.9565 - recall: 0.9535 - prc: 0.9885\n",
            "211/262 [=======================>......] - ETA: 5:56 - loss: 0.1759 - accuracy: 0.9391 - precision: 0.9563 - recall: 0.9533 - prc: 0.9882\n",
            "212/262 [=======================>......] - ETA: 5:49 - loss: 0.1761 - accuracy: 0.9389 - precision: 0.9563 - recall: 0.9531 - prc: 0.9883\n",
            "213/262 [=======================>......] - ETA: 5:42 - loss: 0.1755 - accuracy: 0.9391 - precision: 0.9563 - recall: 0.9534 - prc: 0.9883\n",
            "214/262 [=======================>......] - ETA: 5:35 - loss: 0.1760 - accuracy: 0.9391 - precision: 0.9560 - recall: 0.9535 - prc: 0.9883\n",
            "215/262 [=======================>......] - ETA: 5:28 - loss: 0.1771 - accuracy: 0.9388 - precision: 0.9558 - recall: 0.9533 - prc: 0.9882\n",
            "216/262 [=======================>......] - ETA: 5:21 - loss: 0.1791 - accuracy: 0.9385 - precision: 0.9553 - recall: 0.9533 - prc: 0.9879\n",
            "217/262 [=======================>......] - ETA: 5:14 - loss: 0.1786 - accuracy: 0.9386 - precision: 0.9553 - recall: 0.9534 - prc: 0.9879\n",
            "218/262 [=======================>......] - ETA: 5:07 - loss: 0.1781 - accuracy: 0.9388 - precision: 0.9555 - recall: 0.9534 - prc: 0.9880\n",
            "219/262 [========================>.....] - ETA: 5:00 - loss: 0.1784 - accuracy: 0.9383 - precision: 0.9551 - recall: 0.9532 - prc: 0.9880\n",
            "220/262 [========================>.....] - ETA: 4:53 - loss: 0.1779 - accuracy: 0.9385 - precision: 0.9552 - recall: 0.9532 - prc: 0.9880\n",
            "221/262 [========================>.....] - ETA: 4:46 - loss: 0.1774 - accuracy: 0.9385 - precision: 0.9553 - recall: 0.9533 - prc: 0.9881\n",
            "222/262 [========================>.....] - ETA: 4:39 - loss: 0.1768 - accuracy: 0.9387 - precision: 0.9555 - recall: 0.9535 - prc: 0.9882\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 223/262 [========================>.....]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 4:32 - loss: 0.1771 - accuracy: 0.9382 - precision: 0.9555 - recall: 0.9527 - prc: 0.9882\n",
            "224/262 [========================>.....] - ETA: 4:25 - loss: 0.1764 - accuracy: 0.9384 - precision: 0.9557 - recall: 0.9529 - prc: 0.9882\n",
            "225/262 [========================>.....] - ETA: 4:18 - loss: 0.1766 - accuracy: 0.9383 - precision: 0.9559 - recall: 0.9525 - prc: 0.9882\n",
            "226/262 [========================>.....] - ETA: 4:11 - loss: 0.1769 - accuracy: 0.9383 - precision: 0.9559 - recall: 0.9525 - prc: 0.9883\n",
            "227/262 [========================>.....] - ETA: 4:04 - loss: 0.1772 - accuracy: 0.9382 - precision: 0.9557 - recall: 0.9526 - prc: 0.9882\n",
            "228/262 [=========================>....] - ETA: 3:57 - loss: 0.1768 - accuracy: 0.9383 - precision: 0.9559 - recall: 0.9526 - prc: 0.9883\n",
            "229/262 [=========================>....] - ETA: 3:50 - loss: 0.1770 - accuracy: 0.9384 - precision: 0.9559 - recall: 0.9528 - prc: 0.9881\n",
            "230/262 [=========================>....] - ETA: 3:43 - loss: 0.1765 - accuracy: 0.9386 - precision: 0.9561 - recall: 0.9528 - prc: 0.9882\n",
            "231/262 [=========================>....] - ETA: 3:36 - loss: 0.1770 - accuracy: 0.9386 - precision: 0.9561 - recall: 0.9529 - prc: 0.9882\n",
            "232/262 [=========================>....] - ETA: 3:29 - loss: 0.1764 - accuracy: 0.9388 - precision: 0.9563 - recall: 0.9531 - prc: 0.9882\n",
            "233/262 [=========================>....] - ETA: 3:22 - loss: 0.1762 - accuracy: 0.9387 - precision: 0.9559 - recall: 0.9533 - prc: 0.9882\n",
            "234/262 [=========================>....] - ETA: 3:15 - loss: 0.1769 - accuracy: 0.9387 - precision: 0.9559 - recall: 0.9533 - prc: 0.9882\n",
            "235/262 [=========================>....] - ETA: 3:08 - loss: 0.1769 - accuracy: 0.9387 - precision: 0.9559 - recall: 0.9533 - prc: 0.9882\n",
            "236/262 [==========================>...] - ETA: 3:01 - loss: 0.1773 - accuracy: 0.9384 - precision: 0.9561 - recall: 0.9527 - prc: 0.9882\n",
            "237/262 [==========================>...] - ETA: 2:54 - loss: 0.1769 - accuracy: 0.9385 - precision: 0.9561 - recall: 0.9529 - prc: 0.9883\n",
            "238/262 [==========================>...] - ETA: 2:47 - loss: 0.1765 - accuracy: 0.9387 - precision: 0.9563 - recall: 0.9529 - prc: 0.9883\n",
            "239/262 [==========================>...] - ETA: 2:40 - loss: 0.1758 - accuracy: 0.9389 - precision: 0.9564 - recall: 0.9531 - prc: 0.9884\n",
            "240/262 [==========================>...] - ETA: 2:33 - loss: 0.1760 - accuracy: 0.9386 - precision: 0.9563 - recall: 0.9530 - prc: 0.9884\n",
            "241/262 [==========================>...] - ETA: 2:26 - loss: 0.1763 - accuracy: 0.9385 - precision: 0.9565 - recall: 0.9526 - prc: 0.9884\n",
            "242/262 [==========================>...] - ETA: 2:19 - loss: 0.1763 - accuracy: 0.9382 - precision: 0.9563 - recall: 0.9524 - prc: 0.9884\n",
            "243/262 [==========================>...] - ETA: 2:12 - loss: 0.1758 - accuracy: 0.9384 - precision: 0.9563 - recall: 0.9526 - prc: 0.9885\n",
            "244/262 [==========================>...] - ETA: 2:05 - loss: 0.1763 - accuracy: 0.9382 - precision: 0.9563 - recall: 0.9525 - prc: 0.9883\n",
            "245/262 [===========================>..] - ETA: 1:58 - loss: 0.1762 - accuracy: 0.9382 - precision: 0.9563 - recall: 0.9525 - prc: 0.9883\n",
            "246/262 [===========================>..] - ETA: 1:51 - loss: 0.1759 - accuracy: 0.9384 - precision: 0.9565 - recall: 0.9525 - prc: 0.9883\n",
            "247/262 [===========================>..] - ETA: 1:44 - loss: 0.1754 - accuracy: 0.9386 - precision: 0.9566 - recall: 0.9527 - prc: 0.9884\n",
            "248/262 [===========================>..] - ETA: 1:37 - loss: 0.1751 - accuracy: 0.9386 - precision: 0.9564 - recall: 0.9529 - prc: 0.9884\n",
            "249/262 [===========================>..] - ETA: 1:30 - loss: 0.1749 - accuracy: 0.9385 - precision: 0.9566 - recall: 0.9525 - prc: 0.9884\n",
            "250/262 [===========================>..] - ETA: 1:23 - loss: 0.1748 - accuracy: 0.9385 - precision: 0.9568 - recall: 0.9524 - prc: 0.9885\n",
            "251/262 [===========================>..] - ETA: 1:16 - loss: 0.1745 - accuracy: 0.9385 - precision: 0.9568 - recall: 0.9524 - prc: 0.9885\n",
            "252/262 [===========================>..] - ETA: 1:09 - loss: 0.1741 - accuracy: 0.9387 - precision: 0.9570 - recall: 0.9526 - prc: 0.9885\n",
            "253/262 [===========================>..] - ETA: 1:02 - loss: 0.1742 - accuracy: 0.9387 - precision: 0.9570 - recall: 0.9526 - prc: 0.9884\n",
            "254/262 [============================>.] - ETA: 55s - loss: 0.1746 - accuracy: 0.9385 - precision: 0.9566 - recall: 0.9526 - prc: 0.9884 \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 255/262 [============================>.] - ETA: 48s - loss: 0.1742 - accuracy: 0.9386 - precision: 0.9566 - recall: 0.9528 - prc: 0.9884\n",
            "256/262 [============================>.] - ETA: 41s - loss: 0.1735 - accuracy: 0.9388 - precision: 0.9567 - recall: 0.9529 - prc: 0.9885\n",
            "257/262 [============================>.] - ETA: 34s - loss: 0.1736 - accuracy: 0.9387 - precision: 0.9567 - recall: 0.9528 - prc: 0.9885\n",
            "258/262 [============================>.] - ETA: 27s - loss: 0.1737 - accuracy: 0.9388 - precision: 0.9569 - recall: 0.9527 - prc: 0.9885\n",
            "259/262 [============================>.] - ETA: 20s - loss: 0.1734 - accuracy: 0.9388 - precision: 0.9570 - recall: 0.9526 - prc: 0.9885\n",
            "260/262 [============================>.] - ETA: 13s - loss: 0.1729 - accuracy: 0.9390 - precision: 0.9572 - recall: 0.9528 - prc: 0.9886\n",
            "261/262 [============================>.] - ETA: 6s - loss: 0.1733 - accuracy: 0.9388 - precision: 0.9567 - recall: 0.9529 - prc: 0.9885 \n",
            "262/262 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9388 - precision: 0.9567 - recall: 0.9528 - prc: 0.9886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 15:39:33.881551: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r262/262 [==============================] - 2088s 8s/step - loss: 0.1732 - accuracy: 0.9388 - precision: 0.9567 - recall: 0.9528 - prc: 0.9886 - val_loss: 0.0970 - val_accuracy: 0.9636 - val_precision: 1.0000 - val_recall: 0.9636 - val_prc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 15:43:33.213845: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Epoch 4/5\n",
            "  1/262 [..............................] - ETA: 3:04:02 - loss: 0.1269 - accuracy: 0.9375 - precision: 1.0000 - recall: 0.9259 - prc: 1.0000\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   2/262 [..............................] - ETA: 27:18 - loss: 0.0984 - accuracy: 0.9531 - precision: 1.0000 - recall: 0.9434 - prc: 0.9993  \n",
            "  3/262 [..............................] - ETA: 27:58 - loss: 0.1518 - accuracy: 0.9271 - precision: 0.9706 - recall: 0.9296 - prc: 0.9949\n",
            "  4/262 [..............................] - ETA: 28:11 - loss: 0.1690 - accuracy: 0.9375 - precision: 0.9775 - recall: 0.9355 - prc: 0.9944\n",
            "  5/262 [..............................] - ETA: 28:14 - loss: 0.1686 - accuracy: 0.9375 - precision: 0.9630 - recall: 0.9455 - prc: 0.9935\n",
            "  6/262 [..............................] - ETA: 28:15 - loss: 0.1641 - accuracy: 0.9323 - precision: 0.9542 - recall: 0.9470 - prc: 0.9938\n",
            "  7/262 [..............................] - ETA: 28:14 - loss: 0.1574 - accuracy: 0.9375 - precision: 0.9548 - recall: 0.9548 - prc: 0.9942\n",
            "  8/262 [..............................] - ETA: 28:07 - loss: 0.1405 - accuracy: 0.9453 - precision: 0.9600 - recall: 0.9600 - prc: 0.9950\n",
            "  9/262 [>.............................] - ETA: 28:00 - loss: 0.1673 - accuracy: 0.9340 - precision: 0.9497 - recall: 0.9545 - prc: 0.9936\n",
            " 10/262 [>.............................] - ETA: 27:56 - loss: 0.1654 - accuracy: 0.9344 - precision: 0.9457 - recall: 0.9587 - prc: 0.9938\n",
            " 11/262 [>.............................] - ETA: 27:49 - loss: 0.1547 - accuracy: 0.9403 - precision: 0.9514 - recall: 0.9631 - prc: 0.9942\n",
            " 12/262 [>.............................] - ETA: 27:42 - loss: 0.1443 - accuracy: 0.9453 - precision: 0.9549 - recall: 0.9658 - prc: 0.9947\n",
            " 13/262 [>.............................] - ETA: 27:37 - loss: 0.1449 - accuracy: 0.9471 - precision: 0.9582 - recall: 0.9649 - prc: 0.9947\n",
            " 14/262 [>.............................] - ETA: 27:32 - loss: 0.1477 - accuracy: 0.9487 - precision: 0.9577 - recall: 0.9671 - prc: 0.9911\n",
            " 15/262 [>.............................] - ETA: 27:27 - loss: 0.1425 - accuracy: 0.9521 - precision: 0.9611 - recall: 0.9698 - prc: 0.9916\n",
            " 16/262 [>.............................] - ETA: 27:19 - loss: 0.1347 - accuracy: 0.9551 - precision: 0.9638 - recall: 0.9719 - prc: 0.9923\n",
            " 17/262 [>.............................] - ETA: 27:13 - loss: 0.1279 - accuracy: 0.9577 - precision: 0.9659 - recall: 0.9735 - prc: 0.9928\n",
            " 18/262 [=>............................] - ETA: 27:08 - loss: 0.1378 - accuracy: 0.9531 - precision: 0.9581 - recall: 0.9749 - prc: 0.9923\n",
            " 19/262 [=>............................] - ETA: 27:01 - loss: 0.1344 - accuracy: 0.9539 - precision: 0.9578 - recall: 0.9761 - prc: 0.9927\n",
            " 20/262 [=>............................] - ETA: 26:55 - loss: 0.1312 - accuracy: 0.9547 - precision: 0.9577 - recall: 0.9773 - prc: 0.9929\n",
            " 21/262 [=>............................] - ETA: 26:59 - loss: 0.1322 - accuracy: 0.9539 - precision: 0.9576 - recall: 0.9762 - prc: 0.9930\n",
            " 22/262 [=>............................] - ETA: 27:26 - loss: 0.1317 - accuracy: 0.9517 - precision: 0.9558 - recall: 0.9754 - prc: 0.9932\n",
            " 23/262 [=>............................] - ETA: 27:34 - loss: 0.1261 - accuracy: 0.9538 - precision: 0.9576 - recall: 0.9764 - prc: 0.9937\n",
            " 24/262 [=>............................] - ETA: 27:25 - loss: 0.1324 - accuracy: 0.9505 - precision: 0.9535 - recall: 0.9753 - prc: 0.9932\n",
            " 25/262 [=>............................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 27:16 - loss: 0.1335 - accuracy: 0.9500 - precision: 0.9537 - recall: 0.9745 - prc: 0.9932\n",
            " 26/262 [=>............................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 27:08 - loss: 0.1320 - accuracy: 0.9507 - precision: 0.9536 - recall: 0.9754 - prc: 0.9934\n",
            " 27/262 [==>...........................] - ETA: 27:00 - loss: 0.1281 - accuracy: 0.9525 - precision: 0.9553 - recall: 0.9763 - prc: 0.9937\n",
            " 28/262 [==>...........................] - ETA: 26:51 - loss: 0.1315 - accuracy: 0.9520 - precision: 0.9569 - recall: 0.9740 - prc: 0.9936\n",
            " 29/262 [==>...........................] - ETA: 26:42 - loss: 0.1312 - accuracy: 0.9515 - precision: 0.9551 - recall: 0.9747 - prc: 0.9936\n",
            " 30/262 [==>...........................] - ETA: 26:33 - loss: 0.1342 - accuracy: 0.9510 - precision: 0.9564 - recall: 0.9725 - prc: 0.9935\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  31/262 [==>...........................] - ETA: 26:25 - loss: 0.1342 - accuracy: 0.9516 - precision: 0.9577 - recall: 0.9719 - prc: 0.9935\n",
            " 32/262 [==>...........................] - ETA: 26:17 - loss: 0.1385 - accuracy: 0.9512 - precision: 0.9591 - recall: 0.9700 - prc: 0.9933\n",
            " 33/262 [==>...........................] - ETA: 26:11 - loss: 0.1378 - accuracy: 0.9517 - precision: 0.9591 - recall: 0.9710 - prc: 0.9934\n",
            " 34/262 [==>...........................] - ETA: 26:13 - loss: 0.1454 - accuracy: 0.9513 - precision: 0.9575 - recall: 0.9717 - prc: 0.9915\n",
            " 35/262 [===>..........................] - ETA: 26:31 - loss: 0.1422 - accuracy: 0.9527 - precision: 0.9589 - recall: 0.9726 - prc: 0.9918\n",
            " 36/262 [===>..........................] - ETA: 26:31 - loss: 0.1415 - accuracy: 0.9523 - precision: 0.9590 - recall: 0.9723 - prc: 0.9920\n",
            " 37/262 [===>..........................] - ETA: 26:22 - loss: 0.1460 - accuracy: 0.9519 - precision: 0.9599 - recall: 0.9705 - prc: 0.9918\n",
            " 38/262 [===>..........................] - ETA: 26:13 - loss: 0.1548 - accuracy: 0.9498 - precision: 0.9586 - recall: 0.9689 - prc: 0.9914\n",
            " 39/262 [===>..........................] - ETA: 26:04 - loss: 0.1538 - accuracy: 0.9495 - precision: 0.9574 - recall: 0.9697 - prc: 0.9915\n",
            " 40/262 [===>..........................] - ETA: 25:55 - loss: 0.1537 - accuracy: 0.9492 - precision: 0.9562 - recall: 0.9704 - prc: 0.9915\n",
            " 41/262 [===>..........................] - ETA: 25:47 - loss: 0.1519 - accuracy: 0.9497 - precision: 0.9570 - recall: 0.9699 - prc: 0.9916\n",
            " 42/262 [===>..........................] - ETA: 25:39 - loss: 0.1498 - accuracy: 0.9501 - precision: 0.9569 - recall: 0.9705 - prc: 0.9917\n",
            " 43/262 [===>..........................] - ETA: 25:31 - loss: 0.1483 - accuracy: 0.9506 - precision: 0.9568 - recall: 0.9711 - prc: 0.9919\n",
            " 44/262 [====>.........................] - ETA: 25:22 - loss: 0.1461 - accuracy: 0.9510 - precision: 0.9579 - recall: 0.9709 - prc: 0.9921\n",
            " 45/262 [====>.........................] - ETA: 25:16 - loss: 0.1479 - accuracy: 0.9514 - precision: 0.9580 - recall: 0.9716 - prc: 0.9911\n",
            " 46/262 [====>.........................] - ETA: 25:06 - loss: 0.1508 - accuracy: 0.9511 - precision: 0.9588 - recall: 0.9702 - prc: 0.9910\n",
            " 47/262 [====>.........................] - ETA: 24:58 - loss: 0.1521 - accuracy: 0.9508 - precision: 0.9597 - recall: 0.9690 - prc: 0.9911\n",
            " 48/262 [====>.........................] - ETA: 24:50 - loss: 0.1557 - accuracy: 0.9499 - precision: 0.9586 - recall: 0.9686 - prc: 0.9908\n",
            " 49/262 [====>.........................] - ETA: 24:42 - loss: 0.1536 - accuracy: 0.9503 - precision: 0.9594 - recall: 0.9683 - prc: 0.9910\n",
            " 50/262 [====>.........................] - ETA: 24:34 - loss: 0.1510 - accuracy: 0.9513 - precision: 0.9601 - recall: 0.9689 - prc: 0.9912\n",
            " 51/262 [====>.........................] - ETA: 24:28 - loss: 0.1509 - accuracy: 0.9510 - precision: 0.9599 - recall: 0.9685 - prc: 0.9912\n",
            " 52/262 [====>.........................] - ETA: 24:21 - loss: 0.1520 - accuracy: 0.9513 - precision: 0.9607 - recall: 0.9683 - prc: 0.9913\n",
            " 53/262 [=====>........................] - ETA: 24:13 - loss: 0.1562 - accuracy: 0.9511 - precision: 0.9596 - recall: 0.9688 - prc: 0.9903\n",
            " 54/262 [=====>........................] - ETA: 24:05 - loss: 0.1578 - accuracy: 0.9502 - precision: 0.9587 - recall: 0.9685 - prc: 0.9902\n",
            " 55/262 [=====>........................] - ETA: 23:57 - loss: 0.1552 - accuracy: 0.9511 - precision: 0.9595 - recall: 0.9691 - prc: 0.9904\n",
            " 56/262 [=====>........................] - ETA: 23:50 - loss: 0.1535 - accuracy: 0.9515 - precision: 0.9602 - recall: 0.9689 - prc: 0.9906\n",
            " 57/262 [=====>........................] - ETA: 23:42 - loss: 0.1530 - accuracy: 0.9512 - precision: 0.9601 - recall: 0.9686 - prc: 0.9907\n",
            " 58/262 [=====>........................] - ETA: 23:34 - loss: 0.1512 - accuracy: 0.9520 - precision: 0.9608 - recall: 0.9692 - prc: 0.9909\n",
            " 59/262 [=====>........................] - ETA: 23:27 - loss: 0.1537 - accuracy: 0.9513 - precision: 0.9607 - recall: 0.9682 - prc: 0.9908\n",
            " 60/262 [=====>........................] - ETA: 23:20 - loss: 0.1540 - accuracy: 0.9516 - precision: 0.9613 - recall: 0.9679 - prc: 0.9908\n",
            " 61/262 [=====>........................] - ETA: 23:13 - loss: 0.1518 - accuracy: 0.9524 - precision: 0.9621 - recall: 0.9686 - prc: 0.9910\n",
            " 62/262 [======>.......................] - ETA: 23:05 - loss: 0.1508 - accuracy: 0.9521 - precision: 0.9620 - recall: 0.9683 - prc: 0.9912\n",
            " 63/262 [======>.......................] - ETA: 22:57 - loss: 0.1501 - accuracy: 0.9519 - precision: 0.9625 - recall: 0.9674 - prc: 0.9912\n",
            " 64/262 [======>.......................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 22:50 - loss: 0.1485 - accuracy: 0.9526 - precision: 0.9631 - recall: 0.9679 - prc: 0.9914\n",
            " 65/262 [======>.......................] - ETA: 22:42 - loss: 0.1504 - accuracy: 0.9519 - precision: 0.9636 - recall: 0.9663 - prc: 0.9914\n",
            " 66/262 [======>.......................] - ETA: 22:36 - loss: 0.1493 - accuracy: 0.9522 - precision: 0.9635 - recall: 0.9669 - prc: 0.9914\n",
            " 67/262 [======>.......................] - ETA: 22:36 - loss: 0.1513 - accuracy: 0.9520 - precision: 0.9627 - recall: 0.9673 - prc: 0.9907\n",
            " 68/262 [======>.......................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 22:38 - loss: 0.1524 - accuracy: 0.9517 - precision: 0.9633 - recall: 0.9665 - prc: 0.9907\n",
            " 69/262 [======>.......................] - ETA: 22:34 - loss: 0.1509 - accuracy: 0.9524 - precision: 0.9638 - recall: 0.9670 - prc: 0.9908\n",
            " 70/262 [=======>......................] - ETA: 22:26 - loss: 0.1509 - accuracy: 0.9522 - precision: 0.9637 - recall: 0.9669 - prc: 0.9909\n",
            " 71/262 [=======>......................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 22:18 - loss: 0.1508 - accuracy: 0.9520 - precision: 0.9636 - recall: 0.9667 - prc: 0.9910\n",
            " 72/262 [=======>......................] - ETA: 22:11 - loss: 0.1497 - accuracy: 0.9523 - precision: 0.9634 - recall: 0.9671 - prc: 0.9911\n",
            " 73/262 [=======>......................] - ETA: 22:04 - loss: 0.1502 - accuracy: 0.9516 - precision: 0.9626 - recall: 0.9669 - prc: 0.9910\n",
            " 74/262 [=======>......................] - ETA: 21:57 - loss: 0.1526 - accuracy: 0.9506 - precision: 0.9624 - recall: 0.9654 - prc: 0.9909\n",
            " 75/262 [=======>......................] - ETA: 21:49 - loss: 0.1516 - accuracy: 0.9504 - precision: 0.9630 - recall: 0.9648 - prc: 0.9910\n",
            " 76/262 [=======>......................] - ETA: 21:42 - loss: 0.1498 - accuracy: 0.9511 - precision: 0.9635 - recall: 0.9652 - prc: 0.9912\n",
            " 77/262 [=======>......................] - ETA: 21:34 - loss: 0.1514 - accuracy: 0.9505 - precision: 0.9628 - recall: 0.9651 - prc: 0.9912\n",
            " 78/262 [=======>......................] - ETA: 21:27 - loss: 0.1497 - accuracy: 0.9511 - precision: 0.9633 - recall: 0.9655 - prc: 0.9913\n",
            " 79/262 [========>.....................] - ETA: 21:19 - loss: 0.1481 - accuracy: 0.9517 - precision: 0.9638 - recall: 0.9660 - prc: 0.9914\n",
            " 80/262 [========>.....................] - ETA: 21:12 - loss: 0.1491 - accuracy: 0.9512 - precision: 0.9636 - recall: 0.9653 - prc: 0.9914\n",
            " 81/262 [========>.....................] - ETA: 21:04 - loss: 0.1517 - accuracy: 0.9506 - precision: 0.9629 - recall: 0.9651 - prc: 0.9907\n",
            " 82/262 [========>.....................] - ETA: 20:57 - loss: 0.1513 - accuracy: 0.9508 - precision: 0.9628 - recall: 0.9654 - prc: 0.9907\n",
            " 83/262 [========>.....................] - ETA: 20:50 - loss: 0.1537 - accuracy: 0.9495 - precision: 0.9616 - recall: 0.9647 - prc: 0.9906\n",
            " 84/262 [========>.....................] - ETA: 20:42 - loss: 0.1530 - accuracy: 0.9498 - precision: 0.9614 - recall: 0.9651 - prc: 0.9907\n",
            " 85/262 [========>.....................] - ETA: 20:35 - loss: 0.1521 - accuracy: 0.9500 - precision: 0.9613 - recall: 0.9654 - prc: 0.9907\n",
            " 86/262 [========>.....................] - ETA: 20:28 - loss: 0.1513 - accuracy: 0.9502 - precision: 0.9612 - recall: 0.9658 - prc: 0.9908\n",
            " 87/262 [========>.....................] - ETA: 20:21 - loss: 0.1510 - accuracy: 0.9501 - precision: 0.9611 - recall: 0.9657 - prc: 0.9908\n",
            " 88/262 [=========>....................] - ETA: 20:14 - loss: 0.1520 - accuracy: 0.9499 - precision: 0.9610 - recall: 0.9656 - prc: 0.9908\n",
            " 89/262 [=========>....................] - ETA: 20:12 - loss: 0.1512 - accuracy: 0.9498 - precision: 0.9610 - recall: 0.9655 - prc: 0.9909\n",
            " 90/262 [=========>....................] - ETA: 20:11 - loss: 0.1511 - accuracy: 0.9500 - precision: 0.9615 - recall: 0.9654 - prc: 0.9909\n",
            " 91/262 [=========>....................] - ETA: 20:04 - loss: 0.1496 - accuracy: 0.9505 - precision: 0.9618 - recall: 0.9657 - prc: 0.9910\n",
            " 92/262 [=========>....................] - ETA: 19:57 - loss: 0.1512 - accuracy: 0.9494 - precision: 0.9607 - recall: 0.9650 - prc: 0.9910\n",
            " 93/262 [=========>....................] - ETA: 19:49 - loss: 0.1514 - accuracy: 0.9493 - precision: 0.9612 - recall: 0.9645 - prc: 0.9910\n",
            " 94/262 [=========>....................] - ETA: 19:43 - loss: 0.1509 - accuracy: 0.9491 - precision: 0.9616 - recall: 0.9639 - prc: 0.9911\n",
            " 95/262 [=========>....................] - ETA: 19:36 - loss: 0.1507 - accuracy: 0.9493 - precision: 0.9620 - recall: 0.9639 - prc: 0.9912\n",
            " 96/262 [=========>....................] - ETA: 19:35 - loss: 0.1506 - accuracy: 0.9489 - precision: 0.9610 - recall: 0.9642 - prc: 0.9912\n",
            " 97/262 [==========>...................] - ETA: 19:31 - loss: 0.1492 - accuracy: 0.9494 - precision: 0.9615 - recall: 0.9647 - prc: 0.9914\n",
            " 98/262 [==========>...................] - ETA: 19:27 - loss: 0.1516 - accuracy: 0.9483 - precision: 0.9609 - recall: 0.9636 - prc: 0.9908\n",
            " 99/262 [==========>...................] - ETA: 19:19 - loss: 0.1503 - accuracy: 0.9489 - precision: 0.9613 - recall: 0.9639 - prc: 0.9909\n",
            "100/262 [==========>...................] - ETA: 19:12 - loss: 0.1502 - accuracy: 0.9484 - precision: 0.9612 - recall: 0.9634 - prc: 0.9909\n",
            "101/262 [==========>...................] - ETA: 19:05 - loss: 0.1494 - accuracy: 0.9486 - precision: 0.9612 - recall: 0.9638 - prc: 0.9910\n",
            "102/262 [==========>...................] - ETA: 18:58 - loss: 0.1501 - accuracy: 0.9485 - precision: 0.9607 - recall: 0.9641 - prc: 0.9910\n",
            "103/262 [==========>...................] - ETA: 18:51 - loss: 0.1504 - accuracy: 0.9487 - precision: 0.9605 - recall: 0.9644 - prc: 0.9905\n",
            "104/262 [==========>...................] - ETA: 18:44 - loss: 0.1493 - accuracy: 0.9489 - precision: 0.9609 - recall: 0.9643 - prc: 0.9906\n",
            "105/262 [===========>..................] - ETA: 18:37 - loss: 0.1484 - accuracy: 0.9491 - precision: 0.9612 - recall: 0.9642 - prc: 0.9907\n",
            "106/262 [===========>..................] - ETA: 18:30 - loss: 0.1493 - accuracy: 0.9490 - precision: 0.9615 - recall: 0.9636 - prc: 0.9906\n",
            "107/262 [===========>..................] - ETA: 18:23 - loss: 0.1486 - accuracy: 0.9489 - precision: 0.9615 - recall: 0.9635 - prc: 0.9907\n",
            "108/262 [===========>..................] - ETA: 18:16 - loss: 0.1488 - accuracy: 0.9488 - precision: 0.9610 - recall: 0.9638 - prc: 0.9907\n",
            "109/262 [===========>..................] - ETA: 18:09 - loss: 0.1477 - accuracy: 0.9493 - precision: 0.9613 - recall: 0.9642 - prc: 0.9908\n",
            "110/262 [===========>..................] - ETA: 18:01 - loss: 0.1529 - accuracy: 0.9483 - precision: 0.9604 - recall: 0.9637 - prc: 0.9901\n",
            "111/262 [===========>..................] - ETA: 17:54 - loss: 0.1537 - accuracy: 0.9482 - precision: 0.9608 - recall: 0.9632 - prc: 0.9901\n",
            "112/262 [===========>..................] - ETA: 17:47 - loss: 0.1535 - accuracy: 0.9481 - precision: 0.9611 - recall: 0.9627 - prc: 0.9902\n",
            "113/262 [===========>..................] - ETA: 17:40 - loss: 0.1543 - accuracy: 0.9477 - precision: 0.9611 - recall: 0.9622 - prc: 0.9902\n",
            "114/262 [============>.................] - ETA: 17:33 - loss: 0.1550 - accuracy: 0.9479 - precision: 0.9614 - recall: 0.9622 - prc: 0.9902\n",
            "115/262 [============>.................] - ETA: 17:25 - loss: 0.1544 - accuracy: 0.9481 - precision: 0.9617 - recall: 0.9621 - prc: 0.9902\n",
            "116/262 [============>.................] - ETA: 17:18 - loss: 0.1552 - accuracy: 0.9472 - precision: 0.9601 - recall: 0.9624 - prc: 0.9902\n",
            "117/262 [============>.................] - ETA: 17:11 - loss: 0.1541 - accuracy: 0.9476 - precision: 0.9604 - recall: 0.9626 - prc: 0.9903\n",
            "118/262 [============>.................] - ETA: 17:07 - loss: 0.1536 - accuracy: 0.9478 - precision: 0.9607 - recall: 0.9626 - prc: 0.9904\n",
            "119/262 [============>.................] - ETA: 17:03 - loss: 0.1534 - accuracy: 0.9477 - precision: 0.9607 - recall: 0.9626 - prc: 0.9904\n",
            "120/262 [============>.................] - ETA: 16:55 - loss: 0.1548 - accuracy: 0.9477 - precision: 0.9610 - recall: 0.9621 - prc: 0.9904\n",
            "121/262 [============>.................] - ETA: 16:48 - loss: 0.1553 - accuracy: 0.9478 - precision: 0.9613 - recall: 0.9620 - prc: 0.9904\n",
            "122/262 [============>.................] - ETA: 16:40 - loss: 0.1562 - accuracy: 0.9475 - precision: 0.9613 - recall: 0.9616 - prc: 0.9904\n",
            "123/262 [=============>................] - ETA: 16:33 - loss: 0.1560 - accuracy: 0.9474 - precision: 0.9612 - recall: 0.9616 - prc: 0.9904\n",
            "124/262 [=============>................] - ETA: 16:25 - loss: 0.1550 - accuracy: 0.9478 - precision: 0.9615 - recall: 0.9618 - prc: 0.9905\n",
            "125/262 [=============>................] - ETA: 16:17 - loss: 0.1538 - accuracy: 0.9482 - precision: 0.9618 - recall: 0.9621 - prc: 0.9906\n",
            "126/262 [=============>................] - ETA: 16:10 - loss: 0.1534 - accuracy: 0.9484 - precision: 0.9621 - recall: 0.9621 - prc: 0.9907\n",
            "127/262 [=============>................] - ETA: 16:09 - loss: 0.1532 - accuracy: 0.9483 - precision: 0.9616 - recall: 0.9623 - prc: 0.9907\n",
            "128/262 [=============>................] - ETA: 16:07 - loss: 0.1540 - accuracy: 0.9480 - precision: 0.9616 - recall: 0.9619 - prc: 0.9907\n",
            "129/262 [=============>................] - ETA: 16:05 - loss: 0.1539 - accuracy: 0.9482 - precision: 0.9618 - recall: 0.9618 - prc: 0.9907\n",
            "130/262 [=============>................] - ETA: 16:07 - loss: 0.1537 - accuracy: 0.9483 - precision: 0.9617 - recall: 0.9621 - prc: 0.9907\n",
            "131/262 [==============>...............] - ETA: 15:58 - loss: 0.1533 - accuracy: 0.9485 - precision: 0.9619 - recall: 0.9622 - prc: 0.9908\n",
            "132/262 [==============>...............] - ETA: 15:57 - loss: 0.1522 - accuracy: 0.9489 - precision: 0.9622 - recall: 0.9625 - prc: 0.9909\n",
            "133/262 [==============>...............] - ETA: 15:49 - loss: 0.1556 - accuracy: 0.9485 - precision: 0.9618 - recall: 0.9624 - prc: 0.9904\n",
            "134/262 [==============>...............] - ETA: 15:41 - loss: 0.1576 - accuracy: 0.9480 - precision: 0.9620 - recall: 0.9614 - prc: 0.9903\n",
            "135/262 [==============>...............] - ETA: 15:33 - loss: 0.1586 - accuracy: 0.9477 - precision: 0.9620 - recall: 0.9610 - prc: 0.9903\n",
            "136/262 [==============>...............] - ETA: 15:26 - loss: 0.1583 - accuracy: 0.9476 - precision: 0.9616 - recall: 0.9613 - prc: 0.9903\n",
            "137/262 [==============>...............] - ETA: 15:18 - loss: 0.1582 - accuracy: 0.9478 - precision: 0.9615 - recall: 0.9615 - prc: 0.9904\n",
            "138/262 [==============>...............] - ETA: 15:10 - loss: 0.1576 - accuracy: 0.9479 - precision: 0.9618 - recall: 0.9614 - prc: 0.9904\n",
            "139/262 [==============>...............] - ETA: 15:03 - loss: 0.1580 - accuracy: 0.9476 - precision: 0.9617 - recall: 0.9611 - prc: 0.9904\n",
            "140/262 [===============>..............] - ETA: 14:57 - loss: 0.1599 - accuracy: 0.9471 - precision: 0.9610 - recall: 0.9610 - prc: 0.9904\n",
            "141/262 [===============>..............] - ETA: 14:52 - loss: 0.1596 - accuracy: 0.9472 - precision: 0.9613 - recall: 0.9610 - prc: 0.9904\n",
            "142/262 [===============>..............]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 14:46 - loss: 0.1594 - accuracy: 0.9472 - precision: 0.9615 - recall: 0.9606 - prc: 0.9904\n",
            "143/262 [===============>..............] - ETA: 14:38 - loss: 0.1589 - accuracy: 0.9471 - precision: 0.9611 - recall: 0.9608 - prc: 0.9905\n",
            "144/262 [===============>..............] - ETA: 14:31 - loss: 0.1579 - accuracy: 0.9475 - precision: 0.9614 - recall: 0.9611 - prc: 0.9905\n",
            "145/262 [===============>..............] - ETA: 14:23 - loss: 0.1590 - accuracy: 0.9472 - precision: 0.9611 - recall: 0.9611 - prc: 0.9902\n",
            "146/262 [===============>..............] - ETA: 14:15 - loss: 0.1585 - accuracy: 0.9473 - precision: 0.9610 - recall: 0.9613 - prc: 0.9902\n",
            "147/262 [===============>..............] - ETA: 14:07 - loss: 0.1580 - accuracy: 0.9475 - precision: 0.9613 - recall: 0.9613 - prc: 0.9903\n",
            "148/262 [===============>..............] - ETA: 14:00 - loss: 0.1572 - accuracy: 0.9476 - precision: 0.9615 - recall: 0.9612 - prc: 0.9903\n",
            "149/262 [================>.............] - ETA: 13:52 - loss: 0.1577 - accuracy: 0.9476 - precision: 0.9615 - recall: 0.9612 - prc: 0.9900\n",
            "150/262 [================>.............] - ETA: 13:44 - loss: 0.1573 - accuracy: 0.9477 - precision: 0.9614 - recall: 0.9614 - prc: 0.9901\n",
            "151/262 [================>.............] - ETA: 13:37 - loss: 0.1570 - accuracy: 0.9476 - precision: 0.9613 - recall: 0.9613 - prc: 0.9901\n",
            "152/262 [================>.............] - ETA: 13:29 - loss: 0.1571 - accuracy: 0.9474 - precision: 0.9610 - recall: 0.9613 - prc: 0.9901\n",
            "153/262 [================>.............] - ETA: 13:22 - loss: 0.1565 - accuracy: 0.9475 - precision: 0.9612 - recall: 0.9612 - prc: 0.9902\n",
            "154/262 [================>.............] - ETA: 13:14 - loss: 0.1568 - accuracy: 0.9474 - precision: 0.9612 - recall: 0.9612 - prc: 0.9902\n",
            "155/262 [================>.............] - ETA: 13:09 - loss: 0.1568 - accuracy: 0.9474 - precision: 0.9611 - recall: 0.9611 - prc: 0.9902\n",
            "156/262 [================>.............] - ETA: 13:02 - loss: 0.1570 - accuracy: 0.9469 - precision: 0.9613 - recall: 0.9602 - prc: 0.9902\n",
            "157/262 [================>.............] - ETA: 12:54 - loss: 0.1563 - accuracy: 0.9470 - precision: 0.9616 - recall: 0.9602 - prc: 0.9903\n",
            "158/262 [=================>............] - ETA: 12:47 - loss: 0.1563 - accuracy: 0.9472 - precision: 0.9618 - recall: 0.9601 - prc: 0.9903\n",
            "159/262 [=================>............] - ETA: 12:39 - loss: 0.1565 - accuracy: 0.9471 - precision: 0.9614 - recall: 0.9603 - prc: 0.9903\n",
            "160/262 [=================>............] - ETA: 12:32 - loss: 0.1568 - accuracy: 0.9467 - precision: 0.9611 - recall: 0.9600 - prc: 0.9903\n",
            "161/262 [=================>............] - ETA: 12:24 - loss: 0.1560 - accuracy: 0.9470 - precision: 0.9613 - recall: 0.9602 - prc: 0.9903\n",
            "162/262 [=================>............] - ETA: 12:17 - loss: 0.1561 - accuracy: 0.9471 - precision: 0.9613 - recall: 0.9605 - prc: 0.9904\n",
            "163/262 [=================>............] - ETA: 12:09 - loss: 0.1564 - accuracy: 0.9469 - precision: 0.9612 - recall: 0.9601 - prc: 0.9904\n",
            "164/262 [=================>............] - ETA: 12:02 - loss: 0.1569 - accuracy: 0.9468 - precision: 0.9612 - recall: 0.9601 - prc: 0.9904\n",
            "165/262 [=================>............] - ETA: 11:54 - loss: 0.1571 - accuracy: 0.9468 - precision: 0.9615 - recall: 0.9599 - prc: 0.9904\n",
            "166/262 [==================>...........] - ETA: 11:47 - loss: 0.1567 - accuracy: 0.9469 - precision: 0.9614 - recall: 0.9601 - prc: 0.9904\n",
            "167/262 [==================>...........] - ETA: 11:39 - loss: 0.1571 - accuracy: 0.9467 - precision: 0.9608 - recall: 0.9602 - prc: 0.9904\n",
            "168/262 [==================>...........] - ETA: 11:32 - loss: 0.1572 - accuracy: 0.9468 - precision: 0.9607 - recall: 0.9604 - prc: 0.9904\n",
            "169/262 [==================>...........] - ETA: 11:24 - loss: 0.1568 - accuracy: 0.9467 - precision: 0.9607 - recall: 0.9604 - prc: 0.9904\n",
            "170/262 [==================>...........] - ETA: 11:16 - loss: 0.1567 - accuracy: 0.9467 - precision: 0.9606 - recall: 0.9604 - prc: 0.9905\n",
            "171/262 [==================>...........] - ETA: 11:09 - loss: 0.1560 - accuracy: 0.9470 - precision: 0.9609 - recall: 0.9606 - prc: 0.9905\n",
            "172/262 [==================>...........] - ETA: 11:01 - loss: 0.1570 - accuracy: 0.9469 - precision: 0.9608 - recall: 0.9605 - prc: 0.9902\n",
            "173/262 [==================>...........] - ETA: 10:54 - loss: 0.1561 - accuracy: 0.9472 - precision: 0.9610 - recall: 0.9608 - prc: 0.9903\n",
            "174/262 [==================>...........] - ETA: 10:46 - loss: 0.1564 - accuracy: 0.9470 - precision: 0.9610 - recall: 0.9605 - prc: 0.9903\n",
            "175/262 [===================>..........] - ETA: 10:39 - loss: 0.1560 - accuracy: 0.9470 - precision: 0.9612 - recall: 0.9602 - prc: 0.9904\n",
            "176/262 [===================>..........] - ETA: 10:31 - loss: 0.1559 - accuracy: 0.9471 - precision: 0.9614 - recall: 0.9602 - prc: 0.9904\n",
            "177/262 [===================>..........] - ETA: 10:24 - loss: 0.1558 - accuracy: 0.9472 - precision: 0.9616 - recall: 0.9601 - prc: 0.9904\n",
            "178/262 [===================>..........] - ETA: 10:16 - loss: 0.1554 - accuracy: 0.9473 - precision: 0.9616 - recall: 0.9604 - prc: 0.9905\n",
            "179/262 [===================>..........] - ETA: 10:09 - loss: 0.1549 - accuracy: 0.9474 - precision: 0.9619 - recall: 0.9604 - prc: 0.9905\n",
            "180/262 [===================>..........] - ETA: 10:01 - loss: 0.1548 - accuracy: 0.9474 - precision: 0.9619 - recall: 0.9604 - prc: 0.9906\n",
            "181/262 [===================>..........] - ETA: 9:54 - loss: 0.1550 - accuracy: 0.9473 - precision: 0.9620 - recall: 0.9601 - prc: 0.9906 \n",
            "182/262 [===================>..........] - ETA: 9:47 - loss: 0.1544 - accuracy: 0.9476 - precision: 0.9622 - recall: 0.9603 - prc: 0.9906\n",
            "183/262 [===================>..........] - ETA: 9:40 - loss: 0.1579 - accuracy: 0.9465 - precision: 0.9617 - recall: 0.9592 - prc: 0.9902\n",
            "184/262 [====================>.........] - ETA: 9:34 - loss: 0.1577 - accuracy: 0.9467 - precision: 0.9617 - recall: 0.9595 - prc: 0.9902\n",
            "185/262 [====================>.........] - ETA: 9:28 - loss: 0.1571 - accuracy: 0.9468 - precision: 0.9616 - recall: 0.9597 - prc: 0.9903\n",
            "186/262 [====================>.........] - ETA: 9:21 - loss: 0.1577 - accuracy: 0.9466 - precision: 0.9611 - recall: 0.9599 - prc: 0.9903\n",
            "187/262 [====================>.........] - ETA: 9:13 - loss: 0.1582 - accuracy: 0.9465 - precision: 0.9613 - recall: 0.9596 - prc: 0.9903\n",
            "188/262 [====================>.........] - ETA: 9:07 - loss: 0.1588 - accuracy: 0.9463 - precision: 0.9610 - recall: 0.9596 - prc: 0.9902\n",
            "189/262 [====================>.........] - ETA: 9:00 - loss: 0.1587 - accuracy: 0.9464 - precision: 0.9612 - recall: 0.9596 - prc: 0.9903\n",
            "190/262 [====================>.........] - ETA: 8:52 - loss: 0.1602 - accuracy: 0.9462 - precision: 0.9614 - recall: 0.9591 - prc: 0.9902\n",
            "191/262 [====================>.........] - ETA: 8:44 - loss: 0.1595 - accuracy: 0.9465 - precision: 0.9616 - recall: 0.9593 - prc: 0.9903\n",
            "192/262 [====================>.........] - ETA: 8:37 - loss: 0.1596 - accuracy: 0.9466 - precision: 0.9616 - recall: 0.9595 - prc: 0.9903\n",
            "193/262 [=====================>........] - ETA: 8:29 - loss: 0.1606 - accuracy: 0.9464 - precision: 0.9615 - recall: 0.9592 - prc: 0.9900\n",
            "194/262 [=====================>........] - ETA: 8:22 - loss: 0.1597 - accuracy: 0.9467 - precision: 0.9617 - recall: 0.9594 - prc: 0.9901\n",
            "195/262 [=====================>........] - ETA: 8:14 - loss: 0.1603 - accuracy: 0.9466 - precision: 0.9615 - recall: 0.9596 - prc: 0.9898\n",
            "196/262 [=====================>........] - ETA: 8:07 - loss: 0.1596 - accuracy: 0.9469 - precision: 0.9617 - recall: 0.9599 - prc: 0.9899\n",
            "197/262 [=====================>........] - ETA: 7:59 - loss: 0.1596 - accuracy: 0.9468 - precision: 0.9616 - recall: 0.9598 - prc: 0.9899\n",
            "198/262 [=====================>........] - ETA: 7:52 - loss: 0.1594 - accuracy: 0.9470 - precision: 0.9618 - recall: 0.9598 - prc: 0.9899\n",
            "199/262 [=====================>........] - ETA: 7:44 - loss: 0.1588 - accuracy: 0.9472 - precision: 0.9620 - recall: 0.9600 - prc: 0.9900\n",
            "200/262 [=====================>........] - ETA: 7:37 - loss: 0.1587 - accuracy: 0.9473 - precision: 0.9622 - recall: 0.9600 - prc: 0.9900\n",
            "201/262 [======================>.......] - ETA: 7:29 - loss: 0.1582 - accuracy: 0.9473 - precision: 0.9624 - recall: 0.9597 - prc: 0.9901\n",
            "202/262 [======================>.......] - ETA: 7:22 - loss: 0.1585 - accuracy: 0.9472 - precision: 0.9622 - recall: 0.9600 - prc: 0.9901\n",
            "203/262 [======================>.......] - ETA: 7:14 - loss: 0.1583 - accuracy: 0.9472 - precision: 0.9621 - recall: 0.9599 - prc: 0.9901\n",
            "204/262 [======================>.......] - ETA: 7:07 - loss: 0.1579 - accuracy: 0.9473 - precision: 0.9623 - recall: 0.9599 - prc: 0.9901\n",
            "205/262 [======================>.......] - ETA: 7:00 - loss: 0.1580 - accuracy: 0.9471 - precision: 0.9618 - recall: 0.9601 - prc: 0.9901\n",
            "206/262 [======================>.......] - ETA: 6:52 - loss: 0.1578 - accuracy: 0.9472 - precision: 0.9618 - recall: 0.9603 - prc: 0.9902\n",
            "207/262 [======================>.......] - ETA: 6:45 - loss: 0.1575 - accuracy: 0.9473 - precision: 0.9618 - recall: 0.9605 - prc: 0.9902\n",
            "208/262 [======================>.......] - ETA: 6:38 - loss: 0.1568 - accuracy: 0.9476 - precision: 0.9619 - recall: 0.9606 - prc: 0.9902\n",
            "209/262 [======================>.......] - ETA: 6:31 - loss: 0.1566 - accuracy: 0.9477 - precision: 0.9621 - recall: 0.9606 - prc: 0.9903\n",
            "210/262 [=======================>......] - ETA: 6:24 - loss: 0.1568 - accuracy: 0.9475 - precision: 0.9618 - recall: 0.9606 - prc: 0.9903\n",
            "211/262 [=======================>......] - ETA: 6:17 - loss: 0.1567 - accuracy: 0.9473 - precision: 0.9620 - recall: 0.9601 - prc: 0.9903\n",
            "212/262 [=======================>......] - ETA: 6:10 - loss: 0.1562 - accuracy: 0.9475 - precision: 0.9622 - recall: 0.9603 - prc: 0.9903\n",
            "213/262 [=======================>......] - ETA: 6:03 - loss: 0.1561 - accuracy: 0.9475 - precision: 0.9621 - recall: 0.9602 - prc: 0.9903\n",
            "214/262 [=======================>......] - ETA: 5:56 - loss: 0.1561 - accuracy: 0.9474 - precision: 0.9621 - recall: 0.9602 - prc: 0.9904\n",
            "215/262 [=======================>......] - ETA: 5:49 - loss: 0.1556 - accuracy: 0.9475 - precision: 0.9622 - recall: 0.9602 - prc: 0.9904\n",
            "216/262 [=======================>......] - ETA: 5:42 - loss: 0.1562 - accuracy: 0.9472 - precision: 0.9618 - recall: 0.9601 - prc: 0.9904\n",
            "217/262 [=======================>......] - ETA: 5:35 - loss: 0.1557 - accuracy: 0.9473 - precision: 0.9618 - recall: 0.9603 - prc: 0.9905\n",
            "218/262 [=======================>......] - ETA: 5:29 - loss: 0.1551 - accuracy: 0.9475 - precision: 0.9619 - recall: 0.9605 - prc: 0.9905\n",
            "219/262 [========================>.....] - ETA: 5:21 - loss: 0.1552 - accuracy: 0.9475 - precision: 0.9619 - recall: 0.9605 - prc: 0.9905\n",
            "220/262 [========================>.....] - ETA: 5:14 - loss: 0.1551 - accuracy: 0.9474 - precision: 0.9617 - recall: 0.9606 - prc: 0.9905\n",
            "221/262 [========================>.....] - ETA: 5:06 - loss: 0.1546 - accuracy: 0.9475 - precision: 0.9616 - recall: 0.9608 - prc: 0.9906\n",
            "222/262 [========================>.....] - ETA: 4:59 - loss: 0.1542 - accuracy: 0.9476 - precision: 0.9618 - recall: 0.9608 - prc: 0.9906\n",
            "223/262 [========================>.....] - ETA: 4:51 - loss: 0.1541 - accuracy: 0.9477 - precision: 0.9620 - recall: 0.9608 - prc: 0.9906\n",
            "224/262 [========================>.....] - ETA: 4:44 - loss: 0.1541 - accuracy: 0.9478 - precision: 0.9621 - recall: 0.9607 - prc: 0.9906\n",
            "225/262 [========================>.....] - ETA: 4:36 - loss: 0.1537 - accuracy: 0.9479 - precision: 0.9623 - recall: 0.9607 - prc: 0.9907\n",
            "226/262 [========================>.....] - ETA: 4:29 - loss: 0.1547 - accuracy: 0.9477 - precision: 0.9622 - recall: 0.9605 - prc: 0.9906\n",
            "227/262 [========================>.....] - ETA: 4:21 - loss: 0.1547 - accuracy: 0.9477 - precision: 0.9624 - recall: 0.9602 - prc: 0.9906\n",
            "228/262 [=========================>....] - ETA: 4:14 - loss: 0.1541 - accuracy: 0.9479 - precision: 0.9626 - recall: 0.9604 - prc: 0.9907\n",
            "229/262 [=========================>....] - ETA: 4:06 - loss: 0.1537 - accuracy: 0.9481 - precision: 0.9628 - recall: 0.9606 - prc: 0.9907\n",
            "230/262 [=========================>....] - ETA: 3:58 - loss: 0.1537 - accuracy: 0.9482 - precision: 0.9627 - recall: 0.9608 - prc: 0.9907\n",
            "231/262 [=========================>....] - ETA: 3:51 - loss: 0.1533 - accuracy: 0.9483 - precision: 0.9629 - recall: 0.9608 - prc: 0.9908\n",
            "232/262 [=========================>....] - ETA: 3:43 - loss: 0.1530 - accuracy: 0.9484 - precision: 0.9628 - recall: 0.9609 - prc: 0.9908\n",
            "233/262 [=========================>....] - ETA: 3:36 - loss: 0.1524 - accuracy: 0.9486 - precision: 0.9630 - recall: 0.9611 - prc: 0.9908\n",
            "234/262 [=========================>....] - ETA: 3:28 - loss: 0.1534 - accuracy: 0.9483 - precision: 0.9624 - recall: 0.9612 - prc: 0.9906\n",
            "235/262 [=========================>....] - ETA: 3:21 - loss: 0.1530 - accuracy: 0.9484 - precision: 0.9623 - recall: 0.9614 - prc: 0.9906\n",
            "236/262 [==========================>...] - ETA: 3:13 - loss: 0.1527 - accuracy: 0.9485 - precision: 0.9623 - recall: 0.9616 - prc: 0.9907\n",
            "237/262 [==========================>...] - ETA: 3:06 - loss: 0.1530 - accuracy: 0.9482 - precision: 0.9619 - recall: 0.9615 - prc: 0.9907\n",
            "238/262 [==========================>...] - ETA: 2:58 - loss: 0.1527 - accuracy: 0.9483 - precision: 0.9620 - recall: 0.9615 - prc: 0.9907\n",
            "239/262 [==========================>...] - ETA: 2:51 - loss: 0.1528 - accuracy: 0.9482 - precision: 0.9622 - recall: 0.9612 - prc: 0.9907\n",
            "240/262 [==========================>...] - ETA: 2:43 - loss: 0.1534 - accuracy: 0.9483 - precision: 0.9623 - recall: 0.9612 - prc: 0.9907\n",
            "241/262 [==========================>...] - ETA: 2:36 - loss: 0.1530 - accuracy: 0.9484 - precision: 0.9625 - recall: 0.9612 - prc: 0.9907\n",
            "242/262 [==========================>...] - ETA: 2:28 - loss: 0.1525 - accuracy: 0.9486 - precision: 0.9627 - recall: 0.9614 - prc: 0.9908\n",
            "243/262 [==========================>...] - ETA: 2:21 - loss: 0.1529 - accuracy: 0.9483 - precision: 0.9624 - recall: 0.9612 - prc: 0.9908\n",
            "244/262 [==========================>...] - ETA: 2:13 - loss: 0.1526 - accuracy: 0.9484 - precision: 0.9624 - recall: 0.9613 - prc: 0.9908\n",
            "245/262 [===========================>..] - ETA: 2:06 - loss: 0.1527 - accuracy: 0.9483 - precision: 0.9626 - recall: 0.9611 - prc: 0.9908\n",
            "246/262 [===========================>..] - ETA: 1:58 - loss: 0.1523 - accuracy: 0.9484 - precision: 0.9625 - recall: 0.9613 - prc: 0.9909\n",
            "247/262 [===========================>..] - ETA: 1:51 - loss: 0.1520 - accuracy: 0.9485 - precision: 0.9627 - recall: 0.9613 - prc: 0.9909\n",
            "248/262 [===========================>..] - ETA: 1:43 - loss: 0.1514 - accuracy: 0.9487 - precision: 0.9629 - recall: 0.9615 - prc: 0.9910\n",
            "249/262 [===========================>..] - ETA: 1:36 - loss: 0.1516 - accuracy: 0.9485 - precision: 0.9629 - recall: 0.9613 - prc: 0.9910\n",
            "250/262 [===========================>..] - ETA: 1:29 - loss: 0.1512 - accuracy: 0.9486 - precision: 0.9628 - recall: 0.9614 - prc: 0.9910\n",
            "251/262 [===========================>..] - ETA: 1:21 - loss: 0.1508 - accuracy: 0.9487 - precision: 0.9628 - recall: 0.9615 - prc: 0.9910\n",
            "252/262 [===========================>..] - ETA: 1:14 - loss: 0.1503 - accuracy: 0.9489 - precision: 0.9629 - recall: 0.9617 - prc: 0.9911\n",
            "253/262 [===========================>..] - ETA: 1:06 - loss: 0.1504 - accuracy: 0.9490 - precision: 0.9629 - recall: 0.9618 - prc: 0.9911\n",
            "254/262 [============================>.] - ETA: 59s - loss: 0.1503 - accuracy: 0.9489 - precision: 0.9629 - recall: 0.9618 - prc: 0.9911 \n",
            "255/262 [============================>.] - ETA: 51s - loss: 0.1510 - accuracy: 0.9488 - precision: 0.9627 - recall: 0.9618 - prc: 0.9909\n",
            "256/262 [============================>.] - ETA: 44s - loss: 0.1511 - accuracy: 0.9486 - precision: 0.9624 - recall: 0.9618 - prc: 0.9909\n",
            "257/262 [============================>.] - ETA: 37s - loss: 0.1507 - accuracy: 0.9487 - precision: 0.9624 - recall: 0.9619 - prc: 0.9909\n",
            "258/262 [============================>.] - ETA: 29s - loss: 0.1509 - accuracy: 0.9488 - precision: 0.9624 - recall: 0.9621 - prc: 0.9908\n",
            "259/262 [============================>.] - ETA: 22s - loss: 0.1509 - accuracy: 0.9487 - precision: 0.9624 - recall: 0.9621 - prc: 0.9908\n",
            "260/262 [============================>.] - ETA: 14s - loss: 0.1505 - accuracy: 0.9489 - precision: 0.9625 - recall: 0.9622 - prc: 0.9908\n",
            "261/262 [============================>.] - ETA: 7s - loss: 0.1523 - accuracy: 0.9485 - precision: 0.9623 - recall: 0.9618 - prc: 0.9906 \n",
            "262/262 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9486 - precision: 0.9624 - recall: 0.9619 - prc: 0.9906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 16:17:57.242957: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r262/262 [==============================] - 2244s 8s/step - loss: 0.1521 - accuracy: 0.9486 - precision: 0.9624 - recall: 0.9619 - prc: 0.9906 - val_loss: 0.0819 - val_accuracy: 0.9741 - val_precision: 1.0000 - val_recall: 0.9741 - val_prc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 16:22:30.805540: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Epoch 5/5\n",
            "  1/262 [..............................] - ETA: 3:03:37 - loss: 0.0138 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - prc: 1.0000\n",
            "  2/262 [..............................] - ETA: 28:30 - loss: 0.0295 - accuracy: 0.9844 - precision: 1.0000 - recall: 0.9778 - prc: 1.0000  \n",
            "  3/262 [..............................] - ETA: 29:04 - loss: 0.0366 - accuracy: 0.9792 - precision: 1.0000 - recall: 0.9692 - prc: 1.0000\n",
            "  4/262 [..............................] - ETA: 29:12 - loss: 0.0388 - accuracy: 0.9844 - precision: 1.0000 - recall: 0.9783 - prc: 1.0000\n",
            "  5/262 [..............................] - ETA: 29:05 - loss: 0.0514 - accuracy: 0.9812 - precision: 0.9913 - recall: 0.9828 - prc: 0.9991\n",
            "  6/262 [..............................] - ETA: 29:02 - loss: 0.1026 - accuracy: 0.9740 - precision: 0.9853 - recall: 0.9781 - prc: 0.9894\n",
            "  7/262 [..............................] - ETA: 29:50 - loss: 0.1246 - accuracy: 0.9643 - precision: 0.9806 - recall: 0.9682 - prc: 0.9891\n",
            "  8/262 [..............................] - ETA: 31:26 - loss: 0.1118 - accuracy: 0.9688 - precision: 0.9830 - recall: 0.9719 - prc: 0.9905\n",
            "  9/262 [>.............................] - ETA: 32:36 - loss: 0.1109 - accuracy: 0.9653 - precision: 0.9800 - recall: 0.9703 - prc: 0.9914\n",
            " 10/262 [>.............................] - ETA: 33:24 - loss: 0.1043 - accuracy: 0.9688 - precision: 0.9816 - recall: 0.9726 - prc: 0.9920\n",
            " 11/262 [>.............................] - ETA: 32:53 - loss: 0.1102 - accuracy: 0.9631 - precision: 0.9790 - recall: 0.9668 - prc: 0.9923\n",
            " 12/262 [>.............................] - ETA: 32:29 - loss: 0.1171 - accuracy: 0.9609 - precision: 0.9767 - recall: 0.9655 - prc: 0.9922\n",
            " 13/262 [>.............................] - ETA: 32:06 - loss: 0.1340 - accuracy: 0.9567 - precision: 0.9715 - recall: 0.9647 - prc: 0.9881\n",
            " 14/262 [>.............................] - ETA: 31:46 - loss: 0.1263 - accuracy: 0.9598 - precision: 0.9733 - recall: 0.9669 - prc: 0.9890\n",
            " 15/262 [>.............................] - ETA: 31:24 - loss: 0.1187 - accuracy: 0.9625 - precision: 0.9755 - recall: 0.9696 - prc: 0.9900\n",
            " 16/262 [>.............................] - ETA: 31:04 - loss: 0.1143 - accuracy: 0.9629 - precision: 0.9771 - recall: 0.9688 - prc: 0.9907\n",
            " 17/262 [>.............................] - ETA: 30:47 - loss: 0.1180 - accuracy: 0.9632 - precision: 0.9784 - recall: 0.9679 - prc: 0.9909\n",
            " 18/262 [=>............................] - ETA: 30:45 - loss: 0.1208 - accuracy: 0.9618 - precision: 0.9797 - recall: 0.9650 - prc: 0.9913\n",
            " 19/262 [=>............................] - ETA: 30:30 - loss: 0.1165 - accuracy: 0.9638 - precision: 0.9807 - recall: 0.9667 - prc: 0.9917\n",
            " 20/262 [=>............................] - ETA: 30:15 - loss: 0.1150 - accuracy: 0.9641 - precision: 0.9795 - recall: 0.9684 - prc: 0.9920\n",
            " 21/262 [=>............................] - ETA: 30:01 - loss: 0.1103 - accuracy: 0.9658 - precision: 0.9803 - recall: 0.9697 - prc: 0.9923\n",
            " 22/262 [=>............................] - ETA: 29:48 - loss: 0.1158 - accuracy: 0.9645 - precision: 0.9791 - recall: 0.9690 - prc: 0.9923\n",
            " 23/262 [=>............................] - ETA: 29:34 - loss: 0.1137 - accuracy: 0.9660 - precision: 0.9799 - recall: 0.9702 - prc: 0.9925\n",
            " 24/262 [=>............................] - ETA: 29:21 - loss: 0.1108 - accuracy: 0.9661 - precision: 0.9806 - recall: 0.9693 - prc: 0.9927\n",
            " 25/262 [=>............................] - ETA: 29:08 - loss: 0.1109 - accuracy: 0.9663 - precision: 0.9795 - recall: 0.9704 - prc: 0.9928\n",
            " 26/262 [=>............................] - ETA: 28:56 - loss: 0.1213 - accuracy: 0.9663 - precision: 0.9805 - recall: 0.9701 - prc: 0.9927\n",
            " 27/262 [==>...........................] - ETA: 28:45 - loss: 0.1177 - accuracy: 0.9676 - precision: 0.9811 - recall: 0.9711 - prc: 0.9930\n",
            " 28/262 [==>...........................] - ETA: 28:33 - loss: 0.1280 - accuracy: 0.9665 - precision: 0.9785 - recall: 0.9721 - prc: 0.9912\n",
            " 29/262 [==>...........................] - ETA: 28:20 - loss: 0.1265 - accuracy: 0.9666 - precision: 0.9792 - recall: 0.9714 - prc: 0.9914\n",
            " 30/262 [==>...........................] - ETA: 28:08 - loss: 0.1280 - accuracy: 0.9656 - precision: 0.9783 - recall: 0.9708 - prc: 0.9915\n",
            " 31/262 [==>...........................] - ETA: 27:56 - loss: 0.1281 - accuracy: 0.9647 - precision: 0.9760 - recall: 0.9717 - prc: 0.9916\n",
            " 32/262 [==>...........................] - ETA: 27:44 - loss: 0.1256 - accuracy: 0.9648 - precision: 0.9767 - recall: 0.9711 - prc: 0.9918\n",
            " 33/262 [==>...........................] - ETA: 27:33 - loss: 0.1269 - accuracy: 0.9631 - precision: 0.9761 - recall: 0.9693 - prc: 0.9919\n",
            " 34/262 [==>...........................] - ETA: 27:22 - loss: 0.1241 - accuracy: 0.9632 - precision: 0.9754 - recall: 0.9701 - prc: 0.9921\n",
            " 35/262 [===>..........................] - ETA: 27:13 - loss: 0.1287 - accuracy: 0.9616 - precision: 0.9748 - recall: 0.9683 - prc: 0.9920\n",
            " 36/262 [===>..........................] - ETA: 27:03 - loss: 0.1283 - accuracy: 0.9609 - precision: 0.9743 - recall: 0.9680 - prc: 0.9922\n",
            " 37/262 [===>..........................] - ETA: 26:53 - loss: 0.1277 - accuracy: 0.9603 - precision: 0.9749 - recall: 0.9665 - prc: 0.9923\n",
            " 38/262 [===>..........................] - ETA: 26:44 - loss: 0.1247 - accuracy: 0.9613 - precision: 0.9756 - recall: 0.9674 - prc: 0.9926\n",
            " 39/262 [===>..........................] - ETA: 26:46 - loss: 0.1333 - accuracy: 0.9591 - precision: 0.9738 - recall: 0.9658 - prc: 0.9922\n",
            " 40/262 [===>..........................] - ETA: 26:56 - loss: 0.1317 - accuracy: 0.9586 - precision: 0.9733 - recall: 0.9655 - prc: 0.9924\n",
            " 41/262 [===>..........................] - ETA: 26:52 - loss: 0.1322 - accuracy: 0.9581 - precision: 0.9741 - recall: 0.9643 - prc: 0.9925\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  42/262 [===>..........................] - ETA: 26:44 - loss: 0.1307 - accuracy: 0.9583 - precision: 0.9748 - recall: 0.9642 - prc: 0.9927\n",
            " 43/262 [===>..........................] - ETA: 26:37 - loss: 0.1284 - accuracy: 0.9593 - precision: 0.9752 - recall: 0.9648 - prc: 0.9928\n",
            " 44/262 [====>.........................] - ETA: 26:44 - loss: 0.1274 - accuracy: 0.9595 - precision: 0.9758 - recall: 0.9647 - prc: 0.9929\n",
            " 45/262 [====>.........................] - ETA: 26:43 - loss: 0.1264 - accuracy: 0.9597 - precision: 0.9762 - recall: 0.9643 - prc: 0.9930\n",
            " 46/262 [====>.........................] - ETA: 26:33 - loss: 0.1264 - accuracy: 0.9599 - precision: 0.9767 - recall: 0.9641 - prc: 0.9931\n",
            " 47/262 [====>.........................] - ETA: 26:24 - loss: 0.1246 - accuracy: 0.9608 - precision: 0.9773 - recall: 0.9649 - prc: 0.9932\n",
            " 48/262 [====>.........................] - ETA: 26:16 - loss: 0.1280 - accuracy: 0.9596 - precision: 0.9758 - recall: 0.9646 - prc: 0.9931\n",
            " 49/262 [====>.........................] - ETA: 26:06 - loss: 0.1286 - accuracy: 0.9592 - precision: 0.9763 - recall: 0.9635 - prc: 0.9931\n",
            " 50/262 [====>.........................]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 26:08 - loss: 0.1289 - accuracy: 0.9588 - precision: 0.9750 - recall: 0.9643 - prc: 0.9932\n",
            " 51/262 [====>.........................] - ETA: 25:59 - loss: 0.1273 - accuracy: 0.9596 - precision: 0.9755 - recall: 0.9650 - prc: 0.9933\n",
            " 52/262 [====>.........................] - ETA: 25:50 - loss: 0.1269 - accuracy: 0.9597 - precision: 0.9760 - recall: 0.9649 - prc: 0.9933\n",
            " 53/262 [=====>........................] - ETA: 25:40 - loss: 0.1260 - accuracy: 0.9593 - precision: 0.9765 - recall: 0.9639 - prc: 0.9934\n",
            " 54/262 [=====>........................] - ETA: 25:30 - loss: 0.1237 - accuracy: 0.9601 - precision: 0.9770 - recall: 0.9646 - prc: 0.9936\n",
            " 55/262 [=====>........................] - ETA: 25:20 - loss: 0.1248 - accuracy: 0.9597 - precision: 0.9775 - recall: 0.9638 - prc: 0.9937\n",
            " 56/262 [=====>........................] - ETA: 25:10 - loss: 0.1286 - accuracy: 0.9587 - precision: 0.9753 - recall: 0.9642 - prc: 0.9926\n",
            " 57/262 [=====>........................] - ETA: 25:00 - loss: 0.1306 - accuracy: 0.9578 - precision: 0.9742 - recall: 0.9641 - prc: 0.9926\n",
            " 58/262 [=====>........................] - ETA: 24:51 - loss: 0.1323 - accuracy: 0.9574 - precision: 0.9738 - recall: 0.9638 - prc: 0.9926\n",
            " 59/262 [=====>........................] - ETA: 24:41 - loss: 0.1348 - accuracy: 0.9566 - precision: 0.9735 - recall: 0.9630 - prc: 0.9925\n",
            " 60/262 [=====>........................] - ETA: 24:32 - loss: 0.1371 - accuracy: 0.9563 - precision: 0.9723 - recall: 0.9634 - prc: 0.9923\n",
            " 61/262 [=====>........................] - ETA: 24:23 - loss: 0.1371 - accuracy: 0.9565 - precision: 0.9727 - recall: 0.9632 - prc: 0.9924\n",
            " 62/262 [======>.......................] - ETA: 24:15 - loss: 0.1355 - accuracy: 0.9567 - precision: 0.9724 - recall: 0.9638 - prc: 0.9925\n",
            " 63/262 [======>.......................] - ETA: 24:06 - loss: 0.1347 - accuracy: 0.9568 - precision: 0.9720 - recall: 0.9642 - prc: 0.9926\n",
            " 64/262 [======>.......................] - ETA: 23:57 - loss: 0.1329 - accuracy: 0.9575 - precision: 0.9725 - recall: 0.9649 - prc: 0.9927\n",
            " 65/262 [======>.......................] - ETA: 23:48 - loss: 0.1310 - accuracy: 0.9582 - precision: 0.9729 - recall: 0.9654 - prc: 0.9928\n",
            " 66/262 [======>.......................] - ETA: 23:39 - loss: 0.1322 - accuracy: 0.9579 - precision: 0.9719 - recall: 0.9658 - prc: 0.9927\n",
            " 67/262 [======>.......................] - ETA: 23:30 - loss: 0.1348 - accuracy: 0.9571 - precision: 0.9723 - recall: 0.9643 - prc: 0.9926\n",
            " 68/262 [======>.......................] - ETA: 23:21 - loss: 0.1334 - accuracy: 0.9577 - precision: 0.9727 - recall: 0.9648 - prc: 0.9927\n",
            " 69/262 [======>.......................] - ETA: 23:13 - loss: 0.1319 - accuracy: 0.9583 - precision: 0.9731 - recall: 0.9654 - prc: 0.9928\n",
            " 70/262 [=======>......................] - ETA: 23:05 - loss: 0.1330 - accuracy: 0.9580 - precision: 0.9722 - recall: 0.9658 - prc: 0.9928\n",
            " 71/262 [=======>......................] - ETA: 23:01 - loss: 0.1351 - accuracy: 0.9573 - precision: 0.9726 - recall: 0.9644 - prc: 0.9928\n",
            " 72/262 [=======>......................] - ETA: 22:53 - loss: 0.1337 - accuracy: 0.9579 - precision: 0.9730 - recall: 0.9649 - prc: 0.9929\n",
            " 73/262 [=======>......................] - ETA: 22:50 - loss: 0.1320 - accuracy: 0.9585 - precision: 0.9734 - recall: 0.9654 - prc: 0.9930\n",
            " 74/262 [=======>......................] - ETA: 22:53 - loss: 0.1326 - accuracy: 0.9573 - precision: 0.9731 - recall: 0.9641 - prc: 0.9931\n",
            " 75/262 [=======>......................] - ETA: 22:51 - loss: 0.1323 - accuracy: 0.9575 - precision: 0.9735 - recall: 0.9640 - prc: 0.9931\n",
            " 76/262 [=======>......................] - ETA: 22:42 - loss: 0.1315 - accuracy: 0.9576 - precision: 0.9733 - recall: 0.9645 - prc: 0.9932\n",
            " 77/262 [=======>......................] - ETA: 22:33 - loss: 0.1307 - accuracy: 0.9578 - precision: 0.9737 - recall: 0.9644 - prc: 0.9933\n",
            " 78/262 [=======>......................] - ETA: 22:24 - loss: 0.1302 - accuracy: 0.9579 - precision: 0.9740 - recall: 0.9643 - prc: 0.9934\n",
            " 79/262 [========>.....................] - ETA: 22:16 - loss: 0.1289 - accuracy: 0.9585 - precision: 0.9743 - recall: 0.9647 - prc: 0.9935\n",
            " 80/262 [========>.....................] - ETA: 22:15 - loss: 0.1276 - accuracy: 0.9590 - precision: 0.9746 - recall: 0.9651 - prc: 0.9935\n",
            " 81/262 [========>.....................] - ETA: 22:10 - loss: 0.1265 - accuracy: 0.9595 - precision: 0.9749 - recall: 0.9655 - prc: 0.9936\n",
            " 82/262 [========>.....................] - ETA: 22:07 - loss: 0.1256 - accuracy: 0.9596 - precision: 0.9746 - recall: 0.9659 - prc: 0.9937\n",
            " 83/262 [========>.....................] - ETA: 22:10 - loss: 0.1274 - accuracy: 0.9593 - precision: 0.9744 - recall: 0.9657 - prc: 0.9936\n",
            " 84/262 [========>.....................] - ETA: 22:08 - loss: 0.1267 - accuracy: 0.9591 - precision: 0.9746 - recall: 0.9651 - prc: 0.9936\n",
            " 85/262 [========>.....................] - ETA: 22:06 - loss: 0.1272 - accuracy: 0.9588 - precision: 0.9739 - recall: 0.9655 - prc: 0.9937\n",
            " 86/262 [========>.....................] - ETA: 22:03 - loss: 0.1288 - accuracy: 0.9582 - precision: 0.9726 - recall: 0.9658 - prc: 0.9936\n",
            " 87/262 [========>.....................] - ETA: 21:59 - loss: 0.1286 - accuracy: 0.9580 - precision: 0.9729 - recall: 0.9652 - prc: 0.9936\n",
            " 88/262 [=========>....................] - ETA: 21:56 - loss: 0.1300 - accuracy: 0.9581 - precision: 0.9732 - recall: 0.9651 - prc: 0.9936\n",
            " 89/262 [=========>....................] - ETA: 21:48 - loss: 0.1298 - accuracy: 0.9582 - precision: 0.9730 - recall: 0.9655 - prc: 0.9936\n",
            " 90/262 [=========>....................] - ETA: 21:45 - loss: 0.1290 - accuracy: 0.9583 - precision: 0.9728 - recall: 0.9659 - prc: 0.9936\n",
            " 91/262 [=========>....................] - ETA: 21:40 - loss: 0.1296 - accuracy: 0.9578 - precision: 0.9726 - recall: 0.9653 - prc: 0.9937\n",
            " 92/262 [=========>....................] - ETA: 21:30 - loss: 0.1293 - accuracy: 0.9579 - precision: 0.9724 - recall: 0.9656 - prc: 0.9937\n",
            " 93/262 [=========>....................] - ETA: 21:21 - loss: 0.1286 - accuracy: 0.9580 - precision: 0.9722 - recall: 0.9659 - prc: 0.9937\n",
            " 94/262 [=========>....................] - ETA: 21:12 - loss: 0.1295 - accuracy: 0.9574 - precision: 0.9719 - recall: 0.9653 - prc: 0.9937\n",
            " 95/262 [=========>....................] - ETA: 21:06 - loss: 0.1301 - accuracy: 0.9572 - precision: 0.9718 - recall: 0.9652 - prc: 0.9937\n",
            " 96/262 [=========>....................] - ETA: 21:01 - loss: 0.1309 - accuracy: 0.9561 - precision: 0.9701 - recall: 0.9650 - prc: 0.9936\n",
            " 97/262 [==========>...................] - ETA: 20:52 - loss: 0.1307 - accuracy: 0.9562 - precision: 0.9700 - recall: 0.9654 - prc: 0.9937\n",
            " 98/262 [==========>...................] - ETA: 20:55 - loss: 0.1307 - accuracy: 0.9560 - precision: 0.9698 - recall: 0.9652 - prc: 0.9937\n",
            " 99/262 [==========>...................] - ETA: 20:51 - loss: 0.1303 - accuracy: 0.9561 - precision: 0.9701 - recall: 0.9651 - prc: 0.9937\n",
            "100/262 [==========>...................] - ETA: 20:50 - loss: 0.1296 - accuracy: 0.9563 - precision: 0.9699 - recall: 0.9654 - prc: 0.9938\n",
            "101/262 [==========>...................] - ETA: 20:50 - loss: 0.1286 - accuracy: 0.9567 - precision: 0.9702 - recall: 0.9658 - prc: 0.9938\n",
            "102/262 [==========>...................] - ETA: 20:45 - loss: 0.1277 - accuracy: 0.9571 - precision: 0.9705 - recall: 0.9662 - prc: 0.9939\n",
            "103/262 [==========>...................] - ETA: 20:40 - loss: 0.1269 - accuracy: 0.9572 - precision: 0.9704 - recall: 0.9665 - prc: 0.9940\n",
            "104/262 [==========>...................] - ETA: 20:35 - loss: 0.1278 - accuracy: 0.9570 - precision: 0.9703 - recall: 0.9664 - prc: 0.9940\n",
            "105/262 [===========>..................] - ETA: 20:30 - loss: 0.1277 - accuracy: 0.9568 - precision: 0.9697 - recall: 0.9667 - prc: 0.9940\n",
            "106/262 [===========>..................] - ETA: 20:24 - loss: 0.1283 - accuracy: 0.9567 - precision: 0.9691 - recall: 0.9670 - prc: 0.9940\n",
            "107/262 [===========>..................] - ETA: 20:19 - loss: 0.1279 - accuracy: 0.9568 - precision: 0.9695 - recall: 0.9670 - prc: 0.9940\n",
            "108/262 [===========>..................] - ETA: 20:13 - loss: 0.1269 - accuracy: 0.9572 - precision: 0.9697 - recall: 0.9673 - prc: 0.9941\n",
            "109/262 [===========>..................] - ETA: 20:09 - loss: 0.1264 - accuracy: 0.9570 - precision: 0.9700 - recall: 0.9668 - prc: 0.9941\n",
            "110/262 [===========>..................] - ETA: 20:03 - loss: 0.1255 - accuracy: 0.9574 - precision: 0.9703 - recall: 0.9671 - prc: 0.9942\n",
            "111/262 [===========>..................] - ETA: 19:58 - loss: 0.1258 - accuracy: 0.9572 - precision: 0.9701 - recall: 0.9669 - prc: 0.9942\n",
            "112/262 [===========>..................] - ETA: 19:52 - loss: 0.1264 - accuracy: 0.9573 - precision: 0.9704 - recall: 0.9668 - prc: 0.9941\n",
            "113/262 [===========>..................] - ETA: 19:46 - loss: 0.1261 - accuracy: 0.9571 - precision: 0.9698 - recall: 0.9670 - prc: 0.9941\n",
            "114/262 [============>.................] - ETA: 19:40 - loss: 0.1278 - accuracy: 0.9570 - precision: 0.9701 - recall: 0.9666 - prc: 0.9941\n",
            "115/262 [============>.................] - ETA: 19:34 - loss: 0.1292 - accuracy: 0.9568 - precision: 0.9704 - recall: 0.9661 - prc: 0.9940\n",
            "116/262 [============>.................] - ETA: 19:28 - loss: 0.1285 - accuracy: 0.9572 - precision: 0.9706 - recall: 0.9664 - prc: 0.9941\n",
            "117/262 [============>.................] - ETA: 19:22 - loss: 0.1277 - accuracy: 0.9575 - precision: 0.9708 - recall: 0.9667 - prc: 0.9941\n",
            "118/262 [============>.................] - ETA: 19:17 - loss: 0.1275 - accuracy: 0.9574 - precision: 0.9707 - recall: 0.9665 - prc: 0.9941\n",
            "119/262 [============>.................] - ETA: 19:08 - loss: 0.1279 - accuracy: 0.9575 - precision: 0.9705 - recall: 0.9667 - prc: 0.9941\n",
            "120/262 [============>.................] - ETA: 18:59 - loss: 0.1285 - accuracy: 0.9570 - precision: 0.9700 - recall: 0.9666 - prc: 0.9940\n",
            "121/262 [============>.................] - ETA: 18:49 - loss: 0.1288 - accuracy: 0.9569 - precision: 0.9698 - recall: 0.9665 - prc: 0.9940\n",
            "122/262 [============>.................] - ETA: 18:40 - loss: 0.1286 - accuracy: 0.9570 - precision: 0.9701 - recall: 0.9664 - prc: 0.9941\n",
            "123/262 [=============>................] - ETA: 18:30 - loss: 0.1280 - accuracy: 0.9573 - precision: 0.9703 - recall: 0.9667 - prc: 0.9941\n",
            "124/262 [=============>................] - ETA: 18:20 - loss: 0.1289 - accuracy: 0.9572 - precision: 0.9698 - recall: 0.9669 - prc: 0.9937\n",
            "125/262 [=============>................] - ETA: 18:11 - loss: 0.1289 - accuracy: 0.9570 - precision: 0.9700 - recall: 0.9664 - prc: 0.9937\n",
            "126/262 [=============>................] - ETA: 18:07 - loss: 0.1281 - accuracy: 0.9573 - precision: 0.9702 - recall: 0.9667 - prc: 0.9937\n",
            "127/262 [=============>................] - ETA: 18:05 - loss: 0.1286 - accuracy: 0.9569 - precision: 0.9701 - recall: 0.9662 - prc: 0.9937\n",
            "128/262 [=============>................] - ETA: 18:05 - loss: 0.1305 - accuracy: 0.9570 - precision: 0.9700 - recall: 0.9665 - prc: 0.9934\n",
            "129/262 [=============>................] - ETA: 18:06 - loss: 0.1299 - accuracy: 0.9571 - precision: 0.9699 - recall: 0.9668 - prc: 0.9934\n",
            "130/262 [=============>................] - ETA: 18:03 - loss: 0.1327 - accuracy: 0.9563 - precision: 0.9694 - recall: 0.9660 - prc: 0.9933\n",
            "131/262 [==============>...............] - ETA: 17:50 - loss: 0.1343 - accuracy: 0.9559 - precision: 0.9695 - recall: 0.9654 - prc: 0.9932\n",
            "132/262 [==============>...............] - ETA: 17:41 - loss: 0.1336 - accuracy: 0.9562 - precision: 0.9697 - recall: 0.9656 - prc: 0.9933\n",
            "133/262 [==============>...............] - ETA: 17:31 - loss: 0.1332 - accuracy: 0.9563 - precision: 0.9696 - recall: 0.9659 - prc: 0.9933\n",
            "134/262 [==============>...............] - ETA: 17:22 - loss: 0.1338 - accuracy: 0.9564 - precision: 0.9698 - recall: 0.9658 - prc: 0.9933\n",
            "135/262 [==============>...............] - ETA: 17:12 - loss: 0.1330 - accuracy: 0.9567 - precision: 0.9700 - recall: 0.9660 - prc: 0.9933\n",
            "136/262 [==============>...............] - ETA: 17:03 - loss: 0.1323 - accuracy: 0.9571 - precision: 0.9702 - recall: 0.9663 - prc: 0.9934\n",
            "137/262 [==============>...............] - ETA: 16:54 - loss: 0.1319 - accuracy: 0.9571 - precision: 0.9704 - recall: 0.9662 - prc: 0.9934\n",
            "138/262 [==============>...............] - ETA: 16:46 - loss: 0.1319 - accuracy: 0.9572 - precision: 0.9707 - recall: 0.9661 - prc: 0.9934\n",
            "139/262 [==============>...............] - ETA: 16:37 - loss: 0.1310 - accuracy: 0.9575 - precision: 0.9709 - recall: 0.9664 - prc: 0.9935\n",
            "140/262 [===============>..............] - ETA: 16:29 - loss: 0.1318 - accuracy: 0.9576 - precision: 0.9708 - recall: 0.9666 - prc: 0.9932\n",
            "141/262 [===============>..............] - ETA: 16:20 - loss: 0.1312 - accuracy: 0.9577 - precision: 0.9707 - recall: 0.9669 - prc: 0.9932\n",
            "142/262 [===============>..............] - ETA: 16:11 - loss: 0.1310 - accuracy: 0.9576 - precision: 0.9709 - recall: 0.9665 - prc: 0.9933\n",
            "143/262 [===============>..............] - ETA: 16:02 - loss: 0.1321 - accuracy: 0.9572 - precision: 0.9708 - recall: 0.9661 - prc: 0.9932\n",
            "144/262 [===============>..............] - ETA: 15:53 - loss: 0.1331 - accuracy: 0.9568 - precision: 0.9710 - recall: 0.9654 - prc: 0.9932\n",
            "145/262 [===============>..............] - ETA: 15:44 - loss: 0.1332 - accuracy: 0.9567 - precision: 0.9705 - recall: 0.9656 - prc: 0.9932\n",
            "146/262 [===============>..............] - ETA: 15:34 - loss: 0.1326 - accuracy: 0.9570 - precision: 0.9708 - recall: 0.9659 - prc: 0.9933\n",
            "147/262 [===============>..............] - ETA: 15:25 - loss: 0.1323 - accuracy: 0.9569 - precision: 0.9710 - recall: 0.9655 - prc: 0.9933\n",
            "148/262 [===============>..............] - ETA: 15:16 - loss: 0.1317 - accuracy: 0.9572 - precision: 0.9712 - recall: 0.9657 - prc: 0.9933\n",
            "149/262 [================>.............] - ETA: 15:07 - loss: 0.1327 - accuracy: 0.9568 - precision: 0.9704 - recall: 0.9659 - prc: 0.9933\n",
            "150/262 [================>.............] - ETA: 15:03 - loss: 0.1321 - accuracy: 0.9569 - precision: 0.9703 - recall: 0.9661 - prc: 0.9933\n",
            "151/262 [================>.............] - ETA: 14:55 - loss: 0.1332 - accuracy: 0.9568 - precision: 0.9699 - recall: 0.9663 - prc: 0.9930\n",
            "152/262 [================>.............] - ETA: 14:46 - loss: 0.1331 - accuracy: 0.9569 - precision: 0.9697 - recall: 0.9665 - prc: 0.9930\n",
            "153/262 [================>.............] - ETA: 14:38 - loss: 0.1333 - accuracy: 0.9569 - precision: 0.9700 - recall: 0.9664 - prc: 0.9930\n",
            "154/262 [================>.............] - ETA: 14:29 - loss: 0.1325 - accuracy: 0.9572 - precision: 0.9702 - recall: 0.9667 - prc: 0.9931\n",
            "155/262 [================>.............] - ETA: 14:21 - loss: 0.1331 - accuracy: 0.9569 - precision: 0.9704 - recall: 0.9660 - prc: 0.9931\n",
            "156/262 [================>.............] - ETA: 14:12 - loss: 0.1340 - accuracy: 0.9566 - precision: 0.9699 - recall: 0.9659 - prc: 0.9930\n",
            "157/262 [================>.............]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 14:03 - loss: 0.1335 - accuracy: 0.9566 - precision: 0.9698 - recall: 0.9661 - prc: 0.9931\n",
            "158/262 [=================>............] - ETA: 13:55 - loss: 0.1327 - accuracy: 0.9569 - precision: 0.9701 - recall: 0.9664 - prc: 0.9931\n",
            "159/262 [=================>............] - ETA: 13:46 - loss: 0.1372 - accuracy: 0.9560 - precision: 0.9696 - recall: 0.9654 - prc: 0.9929\n",
            "160/262 [=================>............] - ETA: 13:38 - loss: 0.1366 - accuracy: 0.9563 - precision: 0.9699 - recall: 0.9657 - prc: 0.9930\n",
            "161/262 [=================>............] - ETA: 13:29 - loss: 0.1376 - accuracy: 0.9562 - precision: 0.9695 - recall: 0.9658 - prc: 0.9929\n",
            "162/262 [=================>............] - ETA: 13:20 - loss: 0.1391 - accuracy: 0.9555 - precision: 0.9685 - recall: 0.9657 - prc: 0.9928\n",
            "163/262 [=================>............] - ETA: 13:12 - loss: 0.1388 - accuracy: 0.9555 - precision: 0.9684 - recall: 0.9659 - prc: 0.9929\n",
            "164/262 [=================>............] - ETA: 13:03 - loss: 0.1383 - accuracy: 0.9556 - precision: 0.9683 - recall: 0.9661 - prc: 0.9929\n",
            "165/262 [=================>............] - ETA: 12:54 - loss: 0.1383 - accuracy: 0.9555 - precision: 0.9684 - recall: 0.9657 - prc: 0.9929\n",
            "166/262 [==================>...........] - ETA: 12:45 - loss: 0.1405 - accuracy: 0.9552 - precision: 0.9684 - recall: 0.9654 - prc: 0.9928\n",
            "167/262 [==================>...........] - ETA: 12:37 - loss: 0.1404 - accuracy: 0.9553 - precision: 0.9683 - recall: 0.9656 - prc: 0.9928\n",
            "168/262 [==================>...........] - ETA: 12:28 - loss: 0.1411 - accuracy: 0.9550 - precision: 0.9676 - recall: 0.9658 - prc: 0.9927\n",
            "169/262 [==================>...........] - ETA: 12:20 - loss: 0.1423 - accuracy: 0.9547 - precision: 0.9676 - recall: 0.9655 - prc: 0.9927\n",
            "170/262 [==================>...........] - ETA: 12:11 - loss: 0.1418 - accuracy: 0.9550 - precision: 0.9678 - recall: 0.9656 - prc: 0.9927\n",
            "171/262 [==================>...........] - ETA: 12:02 - loss: 0.1419 - accuracy: 0.9549 - precision: 0.9677 - recall: 0.9656 - prc: 0.9927\n",
            "172/262 [==================>...........] - ETA: 11:54 - loss: 0.1416 - accuracy: 0.9550 - precision: 0.9676 - recall: 0.9658 - prc: 0.9927\n",
            "173/262 [==================>...........] - ETA: 11:45 - loss: 0.1416 - accuracy: 0.9549 - precision: 0.9675 - recall: 0.9657 - prc: 0.9928\n",
            "174/262 [==================>...........] - ETA: 11:37 - loss: 0.1411 - accuracy: 0.9551 - precision: 0.9677 - recall: 0.9659 - prc: 0.9928\n",
            "175/262 [===================>..........] - ETA: 11:28 - loss: 0.1406 - accuracy: 0.9552 - precision: 0.9676 - recall: 0.9661 - prc: 0.9928\n",
            "176/262 [===================>..........] - ETA: 11:20 - loss: 0.1401 - accuracy: 0.9553 - precision: 0.9678 - recall: 0.9660 - prc: 0.9929\n",
            "177/262 [===================>..........] - ETA: 11:11 - loss: 0.1395 - accuracy: 0.9555 - precision: 0.9680 - recall: 0.9662 - prc: 0.9929\n",
            "178/262 [===================>..........] - ETA: 11:03 - loss: 0.1392 - accuracy: 0.9556 - precision: 0.9682 - recall: 0.9662 - prc: 0.9929\n",
            "179/262 [===================>..........] - ETA: 10:54 - loss: 0.1389 - accuracy: 0.9557 - precision: 0.9681 - recall: 0.9664 - prc: 0.9930\n",
            "180/262 [===================>..........] - ETA: 10:46 - loss: 0.1390 - accuracy: 0.9554 - precision: 0.9680 - recall: 0.9660 - prc: 0.9930\n",
            "181/262 [===================>..........]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 10:39 - loss: 0.1385 - accuracy: 0.9555 - precision: 0.9682 - recall: 0.9660 - prc: 0.9930\n",
            "182/262 [===================>..........] - ETA: 10:31 - loss: 0.1380 - accuracy: 0.9555 - precision: 0.9681 - recall: 0.9661 - prc: 0.9930\n",
            "183/262 [===================>..........] - ETA: 10:23 - loss: 0.1384 - accuracy: 0.9551 - precision: 0.9683 - recall: 0.9654 - prc: 0.9930\n",
            "184/262 [====================>.........] - ETA: 10:15 - loss: 0.1385 - accuracy: 0.9548 - precision: 0.9682 - recall: 0.9651 - prc: 0.9930\n",
            "185/262 [====================>.........] - ETA: 10:07 - loss: 0.1398 - accuracy: 0.9546 - precision: 0.9677 - recall: 0.9653 - prc: 0.9927\n",
            "186/262 [====================>.........] - ETA: 9:59 - loss: 0.1393 - accuracy: 0.9548 - precision: 0.9678 - recall: 0.9654 - prc: 0.9928 \n",
            "187/262 [====================>.........] - ETA: 9:52 - loss: 0.1388 - accuracy: 0.9551 - precision: 0.9680 - recall: 0.9656 - prc: 0.9928\n",
            "188/262 [====================>.........] - ETA: 9:44 - loss: 0.1381 - accuracy: 0.9553 - precision: 0.9682 - recall: 0.9658 - prc: 0.9929\n",
            "189/262 [====================>.........] - ETA: 9:38 - loss: 0.1383 - accuracy: 0.9550 - precision: 0.9681 - recall: 0.9655 - prc: 0.9929\n",
            "190/262 [====================>.........] - ETA: 9:31 - loss: 0.1388 - accuracy: 0.9551 - precision: 0.9681 - recall: 0.9657 - prc: 0.9926\n",
            "191/262 [====================>.........] - ETA: 9:25 - loss: 0.1393 - accuracy: 0.9550 - precision: 0.9677 - recall: 0.9659 - prc: 0.9923\n",
            "192/262 [====================>.........] - ETA: 9:19 - loss: 0.1399 - accuracy: 0.9548 - precision: 0.9672 - recall: 0.9660 - prc: 0.9923\n",
            "193/262 [=====================>........] - ETA: 9:13 - loss: 0.1403 - accuracy: 0.9547 - precision: 0.9673 - recall: 0.9657 - prc: 0.9923\n",
            "194/262 [=====================>........] - ETA: 9:06 - loss: 0.1405 - accuracy: 0.9546 - precision: 0.9673 - recall: 0.9657 - prc: 0.9923\n",
            "195/262 [=====================>........] - ETA: 8:58 - loss: 0.1405 - accuracy: 0.9547 - precision: 0.9672 - recall: 0.9658 - prc: 0.9923\n",
            "196/262 [=====================>........] - ETA: 8:50 - loss: 0.1398 - accuracy: 0.9549 - precision: 0.9674 - recall: 0.9660 - prc: 0.9924\n",
            "197/262 [=====================>........] - ETA: 8:42 - loss: 0.1393 - accuracy: 0.9550 - precision: 0.9675 - recall: 0.9659 - prc: 0.9924\n",
            "198/262 [=====================>........] - ETA: 8:34 - loss: 0.1398 - accuracy: 0.9549 - precision: 0.9674 - recall: 0.9658 - prc: 0.9924\n",
            "199/262 [=====================>........] - ETA: 8:27 - loss: 0.1393 - accuracy: 0.9551 - precision: 0.9676 - recall: 0.9660 - prc: 0.9924\n",
            "200/262 [=====================>........] - ETA: 8:20 - loss: 0.1387 - accuracy: 0.9553 - precision: 0.9678 - recall: 0.9662 - prc: 0.9925\n",
            "201/262 [======================>.......] - ETA: 8:12 - loss: 0.1382 - accuracy: 0.9554 - precision: 0.9677 - recall: 0.9664 - prc: 0.9925\n",
            "202/262 [======================>.......] - ETA: 8:05 - loss: 0.1375 - accuracy: 0.9556 - precision: 0.9679 - recall: 0.9665 - prc: 0.9926\n",
            "203/262 [======================>.......]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 7:58 - loss: 0.1385 - accuracy: 0.9554 - precision: 0.9676 - recall: 0.9665 - prc: 0.9925\n",
            "204/262 [======================>.......] - ETA: 7:50 - loss: 0.1389 - accuracy: 0.9550 - precision: 0.9669 - recall: 0.9667 - prc: 0.9925\n",
            "205/262 [======================>.......] - ETA: 7:42 - loss: 0.1383 - accuracy: 0.9552 - precision: 0.9670 - recall: 0.9668 - prc: 0.9926\n",
            "206/262 [======================>.......] - ETA: 7:34 - loss: 0.1381 - accuracy: 0.9551 - precision: 0.9667 - recall: 0.9670 - prc: 0.9926\n",
            "207/262 [======================>.......] - ETA: 7:26 - loss: 0.1375 - accuracy: 0.9553 - precision: 0.9669 - recall: 0.9671 - prc: 0.9926\n",
            "208/262 [======================>.......] - ETA: 7:18 - loss: 0.1370 - accuracy: 0.9555 - precision: 0.9670 - recall: 0.9673 - prc: 0.9927\n",
            "209/262 [======================>.......] - ETA: 7:10 - loss: 0.1366 - accuracy: 0.9556 - precision: 0.9672 - recall: 0.9672 - prc: 0.9927\n",
            "210/262 [=======================>......] - ETA: 7:02 - loss: 0.1393 - accuracy: 0.9552 - precision: 0.9669 - recall: 0.9669 - prc: 0.9924\n",
            "211/262 [=======================>......] - ETA: 6:54 - loss: 0.1390 - accuracy: 0.9553 - precision: 0.9668 - recall: 0.9670 - prc: 0.9924\n",
            "212/262 [=======================>......] - ETA: 6:46 - loss: 0.1386 - accuracy: 0.9555 - precision: 0.9670 - recall: 0.9672 - prc: 0.9925\n",
            "213/262 [=======================>......] - ETA: 6:37 - loss: 0.1385 - accuracy: 0.9556 - precision: 0.9669 - recall: 0.9673 - prc: 0.9925\n",
            "214/262 [=======================>......] - ETA: 6:30 - loss: 0.1395 - accuracy: 0.9555 - precision: 0.9669 - recall: 0.9673 - prc: 0.9922\n",
            "215/262 [=======================>......] - ETA: 6:22 - loss: 0.1394 - accuracy: 0.9555 - precision: 0.9670 - recall: 0.9672 - prc: 0.9922\n",
            "216/262 [=======================>......] - ETA: 6:15 - loss: 0.1393 - accuracy: 0.9555 - precision: 0.9672 - recall: 0.9670 - prc: 0.9923\n",
            "217/262 [=======================>......] - ETA: 6:07 - loss: 0.1397 - accuracy: 0.9554 - precision: 0.9671 - recall: 0.9669 - prc: 0.9923\n",
            "218/262 [=======================>......] - ETA: 5:59 - loss: 0.1391 - accuracy: 0.9556 - precision: 0.9673 - recall: 0.9671 - prc: 0.9923\n",
            "219/262 [========================>.....] - ETA: 5:51 - loss: 0.1390 - accuracy: 0.9554 - precision: 0.9674 - recall: 0.9666 - prc: 0.9923\n",
            "220/262 [========================>.....] - ETA: 5:43 - loss: 0.1393 - accuracy: 0.9551 - precision: 0.9674 - recall: 0.9664 - prc: 0.9923\n",
            "221/262 [========================>.....] - ETA: 5:36 - loss: 0.1391 - accuracy: 0.9552 - precision: 0.9673 - recall: 0.9665 - prc: 0.9923\n",
            "222/262 [========================>.....] - ETA: 5:28 - loss: 0.1409 - accuracy: 0.9551 - precision: 0.9674 - recall: 0.9662 - prc: 0.9923\n",
            "223/262 [========================>.....] - ETA: 5:21 - loss: 0.1405 - accuracy: 0.9552 - precision: 0.9674 - recall: 0.9664 - prc: 0.9923\n",
            "224/262 [========================>.....] - ETA: 5:13 - loss: 0.1425 - accuracy: 0.9550 - precision: 0.9673 - recall: 0.9661 - prc: 0.9920\n",
            "225/262 [========================>.....] - ETA: 5:05 - loss: 0.1420 - accuracy: 0.9550 - precision: 0.9673 - recall: 0.9663 - prc: 0.9921\n",
            "226/262 [========================>.....] - ETA: 4:57 - loss: 0.1426 - accuracy: 0.9549 - precision: 0.9670 - recall: 0.9664 - prc: 0.9921\n",
            "227/262 [========================>.....] - ETA: 4:49 - loss: 0.1423 - accuracy: 0.9550 - precision: 0.9671 - recall: 0.9664 - prc: 0.9921\n",
            "228/262 [=========================>....] - ETA: 4:42 - loss: 0.1421 - accuracy: 0.9551 - precision: 0.9673 - recall: 0.9663 - prc: 0.9921\n",
            "229/262 [=========================>....] - ETA: 4:34 - loss: 0.1423 - accuracy: 0.9550 - precision: 0.9674 - recall: 0.9660 - prc: 0.9921\n",
            "230/262 [=========================>....] - ETA: 4:26 - loss: 0.1421 - accuracy: 0.9550 - precision: 0.9676 - recall: 0.9660 - prc: 0.9921\n",
            "231/262 [=========================>....] - ETA: 4:18 - loss: 0.1415 - accuracy: 0.9552 - precision: 0.9677 - recall: 0.9661 - prc: 0.9922\n",
            "232/262 [=========================>....] - ETA: 4:10 - loss: 0.1416 - accuracy: 0.9552 - precision: 0.9675 - recall: 0.9663 - prc: 0.9922\n",
            "233/262 [=========================>....] - ETA: 4:02 - loss: 0.1411 - accuracy: 0.9554 - precision: 0.9676 - recall: 0.9665 - prc: 0.9922\n",
            "234/262 [=========================>....] - ETA: 3:53 - loss: 0.1411 - accuracy: 0.9551 - precision: 0.9676 - recall: 0.9662 - prc: 0.9922\n",
            "235/262 [=========================>....] - ETA: 3:45 - loss: 0.1410 - accuracy: 0.9552 - precision: 0.9677 - recall: 0.9662 - prc: 0.9923\n",
            "236/262 [==========================>...] - ETA: 3:36 - loss: 0.1418 - accuracy: 0.9550 - precision: 0.9675 - recall: 0.9661 - prc: 0.9922\n",
            "237/262 [==========================>...] - ETA: 3:28 - loss: 0.1413 - accuracy: 0.9552 - precision: 0.9676 - recall: 0.9663 - prc: 0.9923\n",
            "238/262 [==========================>...] - ETA: 3:20 - loss: 0.1413 - accuracy: 0.9552 - precision: 0.9678 - recall: 0.9663 - prc: 0.9923\n",
            "239/262 [==========================>...] - ETA: 3:11 - loss: 0.1409 - accuracy: 0.9554 - precision: 0.9679 - recall: 0.9664 - prc: 0.9923\n",
            "240/262 [==========================>...] - ETA: 3:03 - loss: 0.1409 - accuracy: 0.9554 - precision: 0.9680 - recall: 0.9661 - prc: 0.9923\n",
            "241/262 [==========================>...] - ETA: 2:55 - loss: 0.1420 - accuracy: 0.9549 - precision: 0.9676 - recall: 0.9659 - prc: 0.9923\n",
            "242/262 [==========================>...] - ETA: 2:47 - loss: 0.1418 - accuracy: 0.9549 - precision: 0.9677 - recall: 0.9658 - prc: 0.9923\n",
            "243/262 [==========================>...] - ETA: 2:39 - loss: 0.1423 - accuracy: 0.9549 - precision: 0.9676 - recall: 0.9658 - prc: 0.9923\n",
            "244/262 [==========================>...]\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m  - ETA: 2:31 - loss: 0.1429 - accuracy: 0.9548 - precision: 0.9674 - recall: 0.9659 - prc: 0.9920\n",
            "245/262 [===========================>..] - ETA: 2:23 - loss: 0.1426 - accuracy: 0.9549 - precision: 0.9673 - recall: 0.9660 - prc: 0.9921\n",
            "246/262 [===========================>..] - ETA: 2:15 - loss: 0.1424 - accuracy: 0.9548 - precision: 0.9673 - recall: 0.9660 - prc: 0.9921\n",
            "247/262 [===========================>..] - ETA: 2:07 - loss: 0.1421 - accuracy: 0.9548 - precision: 0.9672 - recall: 0.9661 - prc: 0.9921\n",
            "248/262 [===========================>..] - ETA: 1:58 - loss: 0.1426 - accuracy: 0.9545 - precision: 0.9666 - recall: 0.9662 - prc: 0.9921\n",
            "249/262 [===========================>..] - ETA: 1:50 - loss: 0.1425 - accuracy: 0.9543 - precision: 0.9664 - recall: 0.9662 - prc: 0.9921\n",
            "250/262 [===========================>..] - ETA: 1:42 - loss: 0.1421 - accuracy: 0.9545 - precision: 0.9665 - recall: 0.9663 - prc: 0.9921\n",
            "251/262 [===========================>..] - ETA: 1:33 - loss: 0.1420 - accuracy: 0.9544 - precision: 0.9665 - recall: 0.9663 - prc: 0.9922\n",
            "252/262 [===========================>..] - ETA: 1:25 - loss: 0.1415 - accuracy: 0.9546 - precision: 0.9666 - recall: 0.9664 - prc: 0.9922\n",
            "253/262 [===========================>..] - ETA: 1:16 - loss: 0.1417 - accuracy: 0.9546 - precision: 0.9663 - recall: 0.9665 - prc: 0.9922\n",
            "254/262 [============================>.] - ETA: 1:08 - loss: 0.1413 - accuracy: 0.9546 - precision: 0.9665 - recall: 0.9665 - prc: 0.9922\n",
            "255/262 [============================>.] - ETA: 59s - loss: 0.1409 - accuracy: 0.9547 - precision: 0.9666 - recall: 0.9664 - prc: 0.9923 \n",
            "256/262 [============================>.] - ETA: 51s - loss: 0.1413 - accuracy: 0.9545 - precision: 0.9664 - recall: 0.9664 - prc: 0.9922\n",
            "257/262 [============================>.] - ETA: 42s - loss: 0.1410 - accuracy: 0.9545 - precision: 0.9663 - recall: 0.9665 - prc: 0.9923\n",
            "258/262 [============================>.] - ETA: 34s - loss: 0.1406 - accuracy: 0.9546 - precision: 0.9665 - recall: 0.9665 - prc: 0.9923\n",
            "259/262 [============================>.] - ETA: 25s - loss: 0.1404 - accuracy: 0.9545 - precision: 0.9666 - recall: 0.9663 - prc: 0.9923\n",
            "260/262 [============================>.] - ETA: 17s - loss: 0.1400 - accuracy: 0.9547 - precision: 0.9667 - recall: 0.9664 - prc: 0.9924\n",
            "261/262 [============================>.] - ETA: 8s - loss: 0.1394 - accuracy: 0.9549 - precision: 0.9669 - recall: 0.9666 - prc: 0.9924 \n",
            "262/262 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.9548 - precision: 0.9670 - recall: 0.9664 - prc: 0.9924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 17:01:29.716759: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r262/262 [==============================] - 2664s 10s/step - loss: 0.1394 - accuracy: 0.9548 - precision: 0.9670 - recall: 0.9664 - prc: 0.9924 - val_loss: 0.1178 - val_accuracy: 0.9569 - val_precision: 1.0000 - val_recall: 0.9569 - val_prc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/tmp/Xray_InceptionV3_keras.ckpt'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_stats = est.evaluate(val_data_creator, batch_size = 32)\n",
        "val_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHIJLKdQGsdt",
        "outputId": "51ef7ed3-9061-4b42-b78e-d14c26d4f008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Found 5216 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function val_data_creator.<locals>.<lambda> at 0x7f095bba84d0> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function val_data_creator.<locals>.<lambda> at 0x7f095bba84d0>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function val_data_creator.<locals>.<lambda> at 0x7f095bba8e60> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function val_data_creator.<locals>.<lambda> at 0x7f095bba8e60>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function val_data_creator.<locals>.<lambda> at 0x7f095bba8cb0> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function val_data_creator.<locals>.<lambda> at 0x7f095bba8cb0>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 17:08:21.488999: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m op: \"TensorSliceDataset\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m input: \"Placeholder/_0\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"Toutput_types\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       type: DT_STRING\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"_cardinality\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     i: 5216\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"is_files\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     b: false\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"metadata\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     s: \"\\n\\026TensorSliceDataset:132\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"output_shapes\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       shape {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 17:08:21.582759: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1/33 [..............................] - ETA: 27:09 - loss: 0.0482 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9688 - prc: 1.0000\n",
            " 2/33 [>.............................] - ETA: 3:28 - loss: 0.0427 - accuracy: 0.9844 - precision: 1.0000 - recall: 0.9844 - prc: 1.0000 \n",
            " 3/33 [=>............................] - ETA: 3:20 - loss: 0.0364 - accuracy: 0.9896 - precision: 1.0000 - recall: 0.9896 - prc: 1.0000\n",
            " 4/33 [==>...........................] - ETA: 3:13 - loss: 0.0433 - accuracy: 0.9844 - precision: 1.0000 - recall: 0.9844 - prc: 1.0000\n",
            " 5/33 [===>..........................] - ETA: 3:06 - loss: 0.0462 - accuracy: 0.9750 - precision: 1.0000 - recall: 0.9750 - prc: 1.0000\n",
            " 6/33 [====>.........................] - ETA: 2:59 - loss: 0.0522 - accuracy: 0.9740 - precision: 1.0000 - recall: 0.9740 - prc: 1.0000\n",
            " 7/33 [=====>........................] - ETA: 2:52 - loss: 0.0614 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9688 - prc: 1.0000\n",
            " 8/33 [======>.......................] - ETA: 2:45 - loss: 0.0620 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9688 - prc: 1.0000\n",
            " 9/33 [=======>......................] - ETA: 2:38 - loss: 0.0740 - accuracy: 0.9653 - precision: 1.0000 - recall: 0.9653 - prc: 1.0000\n",
            "10/33 [========>.....................] - ETA: 2:32 - loss: 0.0773 - accuracy: 0.9625 - precision: 1.0000 - recall: 0.9625 - prc: 1.0000\n",
            "11/33 [=========>....................] - ETA: 2:25 - loss: 0.0809 - accuracy: 0.9602 - precision: 1.0000 - recall: 0.9602 - prc: 1.0000\n",
            "12/33 [=========>....................] - ETA: 2:18 - loss: 0.0800 - accuracy: 0.9609 - precision: 1.0000 - recall: 0.9609 - prc: 1.0000\n",
            "13/33 [==========>...................] - ETA: 2:12 - loss: 0.0862 - accuracy: 0.9591 - precision: 1.0000 - recall: 0.9591 - prc: 1.0000\n",
            "14/33 [===========>..................] - ETA: 2:05 - loss: 0.0823 - accuracy: 0.9621 - precision: 1.0000 - recall: 0.9621 - prc: 1.0000\n",
            "15/33 [============>.................] - ETA: 1:58 - loss: 0.0967 - accuracy: 0.9563 - precision: 1.0000 - recall: 0.9563 - prc: 1.0000\n",
            "16/33 [=============>................] - ETA: 1:52 - loss: 0.1255 - accuracy: 0.9531 - precision: 1.0000 - recall: 0.9531 - prc: 1.0000\n",
            "17/33 [==============>...............] - ETA: 1:45 - loss: 0.1204 - accuracy: 0.9559 - precision: 1.0000 - recall: 0.9559 - prc: 1.0000\n",
            "18/33 [===============>..............] - ETA: 1:38 - loss: 0.1176 - accuracy: 0.9549 - precision: 1.0000 - recall: 0.9549 - prc: 1.0000\n",
            "19/33 [================>.............] - ETA: 1:32 - loss: 0.1129 - accuracy: 0.9572 - precision: 1.0000 - recall: 0.9572 - prc: 1.0000\n",
            "20/33 [=================>............] - ETA: 1:25 - loss: 0.1093 - accuracy: 0.9594 - precision: 1.0000 - recall: 0.9594 - prc: 1.0000\n",
            "21/33 [==================>...........] - ETA: 1:19 - loss: 0.1265 - accuracy: 0.9554 - precision: 1.0000 - recall: 0.9554 - prc: 1.0000\n",
            "22/33 [===================>..........] - ETA: 1:12 - loss: 0.1240 - accuracy: 0.9560 - precision: 1.0000 - recall: 0.9560 - prc: 1.0000\n",
            "23/33 [===================>..........] - ETA: 1:06 - loss: 0.1274 - accuracy: 0.9538 - precision: 1.0000 - recall: 0.9538 - prc: 1.0000\n",
            "24/33 [====================>.........] - ETA: 59s - loss: 0.1234 - accuracy: 0.9544 - precision: 1.0000 - recall: 0.9544 - prc: 1.0000 \n",
            "25/33 [=====================>........] - ETA: 52s - loss: 0.1238 - accuracy: 0.9550 - precision: 1.0000 - recall: 0.9550 - prc: 1.0000\n",
            "26/33 [======================>.......] - ETA: 46s - loss: 0.1258 - accuracy: 0.9531 - precision: 1.0000 - recall: 0.9531 - prc: 1.0000\n",
            "27/33 [=======================>......] - ETA: 39s - loss: 0.1272 - accuracy: 0.9525 - precision: 1.0000 - recall: 0.9525 - prc: 1.0000\n",
            "28/33 [========================>.....] - ETA: 33s - loss: 0.1253 - accuracy: 0.9531 - precision: 1.0000 - recall: 0.9531 - prc: 1.0000\n",
            "29/33 [=========================>....] - ETA: 26s - loss: 0.1248 - accuracy: 0.9526 - precision: 1.0000 - recall: 0.9526 - prc: 1.0000\n",
            "30/33 [==========================>...] - ETA: 19s - loss: 0.1251 - accuracy: 0.9531 - precision: 1.0000 - recall: 0.9531 - prc: 1.0000\n",
            "31/33 [===========================>..] - ETA: 13s - loss: 0.1218 - accuracy: 0.9546 - precision: 1.0000 - recall: 0.9546 - prc: 1.0000\n",
            "32/33 [============================>.] - ETA: 6s - loss: 0.1198 - accuracy: 0.9561 - precision: 1.0000 - recall: 0.9561 - prc: 1.0000 \n",
            "33/33 [==============================] - 260s 7s/step - loss: 0.1178 - accuracy: 0.9569 - precision: 1.0000 - recall: 0.9569 - prc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'validation_accuracy': 0.9568965435028076,\n",
              " 'validation_loss': 0.11776074767112732,\n",
              " 'validation_prc': 1.0,\n",
              " 'validation_precision': 1.0,\n",
              " 'validation_recall': 0.9568965435028076}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with the test set\n",
        "test_stats = est.evaluate(test_data_creator, batch_size = 32)\n",
        "test_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0CKLSd4IQ9m",
        "outputId": "210fc00c-da19-4187-e22f-950a4bbca1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Found 624 files belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function test_data_creator.<locals>.<lambda> at 0x7f0967692440> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function test_data_creator.<locals>.<lambda> at 0x7f0967692440>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function test_data_creator.<locals>.<lambda> at 0x7f0967692e60> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function test_data_creator.<locals>.<lambda> at 0x7f0967692e60>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function test_data_creator.<locals>.<lambda> at 0x7f0967692b00> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m Cause: could not parse the source code of <function test_data_creator.<locals>.<lambda> at 0x7f0967692b00>: no matching AST found among candidates:\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 17:13:39.818687: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m op: \"TensorSliceDataset\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m input: \"Placeholder/_0\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"Toutput_types\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       type: DT_STRING\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"_cardinality\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     i: 624\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"is_files\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     b: false\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"metadata\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     s: \"\\n\\026TensorSliceDataset:162\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m attr {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   key: \"output_shapes\"\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   value {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     list {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       shape {\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m       }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m     }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m   }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m }\n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=785)\u001b[0m 2022-02-08 17:13:39.913149: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1/20 [>.............................] - ETA: 15:57 - loss: 0.6153 - accuracy: 0.8750 - precision: 0.8400 - recall: 1.0000 - prc: 0.8783\n",
            " 2/20 [==>...........................] - ETA: 1:59 - loss: 0.6790 - accuracy: 0.8438 - precision: 0.8077 - recall: 1.0000 - prc: 0.8855 \n",
            " 3/20 [===>..........................] - ETA: 1:52 - loss: 0.6086 - accuracy: 0.8438 - precision: 0.8101 - recall: 1.0000 - prc: 0.9070\n",
            " 4/20 [=====>........................] - ETA: 1:46 - loss: 0.7850 - accuracy: 0.7734 - precision: 0.7429 - recall: 0.9750 - prc: 0.8658\n",
            " 5/20 [======>.......................] - ETA: 1:39 - loss: 0.7222 - accuracy: 0.7812 - precision: 0.7462 - recall: 0.9798 - prc: 0.8867\n",
            " 6/20 [========>.....................] - ETA: 1:32 - loss: 0.6643 - accuracy: 0.7969 - precision: 0.7628 - recall: 0.9835 - prc: 0.9042\n",
            " 7/20 [=========>....................] - ETA: 1:24 - loss: 0.6458 - accuracy: 0.8080 - precision: 0.7802 - recall: 0.9793 - prc: 0.9036\n",
            " 8/20 [===========>..................] - ETA: 1:17 - loss: 0.6581 - accuracy: 0.8086 - precision: 0.7745 - recall: 0.9814 - prc: 0.9013\n",
            " 9/20 [============>.................] - ETA: 1:10 - loss: 0.6310 - accuracy: 0.8125 - precision: 0.7817 - recall: 0.9781 - prc: 0.9095\n",
            "10/20 [==============>...............] - ETA: 1:04 - loss: 0.6259 - accuracy: 0.8188 - precision: 0.7882 - recall: 0.9805 - prc: 0.9094\n",
            "11/20 [===============>..............] - ETA: 57s - loss: 0.6192 - accuracy: 0.8153 - precision: 0.7837 - recall: 0.9822 - prc: 0.9174 \n",
            "12/20 [=================>............] - ETA: 50s - loss: 0.6120 - accuracy: 0.8125 - precision: 0.7814 - recall: 0.9838 - prc: 0.9234\n",
            "13/20 [==================>...........] - ETA: 44s - loss: 0.6408 - accuracy: 0.8029 - precision: 0.7718 - recall: 0.9772 - prc: 0.9155\n",
            "14/20 [====================>.........] - ETA: 38s - loss: 0.6211 - accuracy: 0.8103 - precision: 0.7799 - recall: 0.9790 - prc: 0.9184\n",
            "15/20 [=====================>........] - ETA: 31s - loss: 0.6192 - accuracy: 0.8083 - precision: 0.7755 - recall: 0.9802 - prc: 0.9210\n",
            "16/20 [=======================>......] - ETA: 25s - loss: 0.6278 - accuracy: 0.8066 - precision: 0.7734 - recall: 0.9782 - prc: 0.9216\n",
            "17/20 [========================>.....] - ETA: 18s - loss: 0.6285 - accuracy: 0.8070 - precision: 0.7737 - recall: 0.9795 - prc: 0.9232\n",
            "18/20 [==========================>...] - ETA: 12s - loss: 0.6175 - accuracy: 0.8108 - precision: 0.7790 - recall: 0.9780 - prc: 0.9246\n",
            "19/20 [===========================>..] - ETA: 6s - loss: 0.6380 - accuracy: 0.8092 - precision: 0.7750 - recall: 0.9789 - prc: 0.9192 \n",
            "20/20 [==============================] - 166s 6s/step - loss: 0.6409 - accuracy: 0.8093 - precision: 0.7760 - recall: 0.9769 - prc: 0.9190\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'validation_accuracy': 0.8092948794364929,\n",
              " 'validation_loss': 0.6408560276031494,\n",
              " 'validation_prc': 0.9189658164978027,\n",
              " 'validation_precision': 0.7759674191474915,\n",
              " 'validation_recall': 0.9769230484962463}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get these metrics in the test set:"
      ],
      "metadata": {
        "id": "wolCpGa_YBAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for stat, value in test_stats.items():\n",
        "  print (stat, \": \", value, sep = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GXmzXWnZBcI",
        "outputId": "fc67a8d1-fc5a-4103-d229-2e9913c16444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation_loss: 0.6408560276031494\n",
            "validation_accuracy: 0.8092948794364929\n",
            "validation_precision: 0.7759674191474915\n",
            "validation_recall: 0.9769230484962463\n",
            "validation_prc: 0.9189658164978027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "They are not as high as the validation metrics, but they are still good. The precision of 77.6% means that there are some false positives, (approx. 1 false positive for each 3 true positives). According to Kaggle, there are 234 normal images and 390 pneumonia images in the test dataset. If a classifier always predicted the most common class:"
      ],
      "metadata": {
        "id": "FBdMFuRVZAsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP = 390 # true positives\n",
        "FP = 234 # false positives\n",
        "precision = TP / (TP + FP)\n",
        "precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBp4SgPIbeZT",
        "outputId": "975c9827-1fe5-48ff-f5db-f03c81b071fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our neural network has a higher precision than that (77.6% versus the 62.5% of a dummy classifier)."
      ],
      "metadata": {
        "id": "NqFjz8Flbpyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the recall, it is 97.7%, so almost every pneumonia image gets classified as such."
      ],
      "metadata": {
        "id": "DcuBfNjhb2t0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, our neural network has an accuracy of 80.9%."
      ],
      "metadata": {
        "id": "ug69uyy7cB87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we will save the model and its weights:\n",
        "model_path = \"/content/drive/MyDrive/BigData/Model/\"\n",
        "est.save(os.path.join(model_path, \"final_model_big_data\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AQCxsXAtNcC5",
        "outputId": "98614c1e-8968-4553-9537-e77acdf67354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/BigData/Model/final_model_big_data'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Possible improvements**"
      ],
      "metadata": {
        "id": "pmEA7_-acO9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There has been some overfitting (because the training metrics are better than the test metrics). One possible improvement of this is to increase the regularization of the neural network (for example, increasing the dropout rate of the Dropout layer).\n",
        "\n",
        "Another possible improvement is to use a better pretrained model, such as InceptionV3 successors (InceptionV4, Inception-ResNet, https://arxiv.org/abs/1602.07261), or others.\n",
        "\n",
        "Also, an ensemble of various neural networks with different architectures could be made, to integrate their predictions and possibly have a more robust model overall."
      ],
      "metadata": {
        "id": "OlgsnMl0cRda"
      }
    }
  ]
}
